{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalytskyiJaroslaw/PathogensRamanOpenSet/blob/main/Naive_K_p_1_p_2_submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6MZtHw8cizd"
      },
      "source": [
        "# Naive thresholding, $\\mathcal{K} = p_1+p_2$, $\\mathcal{I}$ = $âˆ…$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlQ76qXvX-xJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import ExcelWriter\n",
        "from pandas import ExcelFile\n",
        "from pandas import read_csv\n",
        "\n",
        "from keras.layers import Lambda, Multiply\n",
        "import csv\n",
        "import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "import keras\n",
        "#from keras.utils import to_categorical\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "#from keras.optimizers import SGD\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras.optimizers import SGD\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from pprint import pprint\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "#from keras.utils import to_categorical\n",
        "\n",
        "from time import time\n",
        "t00 = time()\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjMmDBRYc2nm"
      },
      "source": [
        "# Initializing TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEBeC3Gfc8Jg"
      },
      "outputs": [],
      "source": [
        "# Initialize the TPU and spread the computations across the 8 cores\n",
        "import tensorflow as tf\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "tf.config.list_logical_devices('TPU')\n",
        "#print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoXfC8vkdHIQ"
      },
      "source": [
        "# Uploading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEFKstcxdGnO",
        "outputId": "9ba826d1-dc92-4a24-f14f-adb228ac9a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyFU_ZXCgHGG"
      },
      "source": [
        "# Reference dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2X6lpq-dOSV"
      },
      "outputs": [],
      "source": [
        "# Reference data\n",
        "data_X_reference = np.load(\"/content/gdrive/MyDrive/Stanford_data/X_reference.npy\")\n",
        "# Test data\n",
        "data_X_test = np.load(\"/content/gdrive/MyDrive/Stanford_data/X_test.npy\")\n",
        "\n",
        "data_y_reference = np.load(\"/content/gdrive/MyDrive/Stanford_data/y_reference.npy\")\n",
        "# Test labels\n",
        "data_y_test = np.load(\"/content/gdrive/MyDrive/Stanford_data/y_test.npy\")\n",
        "\n",
        "data_y_reference_int = []\n",
        "\n",
        "for i in range(data_y_reference.shape[0]):\n",
        "  data_y_reference_int.append(int(data_y_reference[i]))\n",
        "\n",
        "data_y_test_int = []\n",
        "\n",
        "for i in range(data_y_test.shape[0]):\n",
        "  data_y_test_int.append(int(data_y_test[i]))\n",
        "\n",
        "train_label = tf.keras.utils.to_categorical(data_y_reference_int)\n",
        "test_label = tf.keras.utils.to_categorical(data_y_test_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpn4ERtzdVkk"
      },
      "source": [
        "# The case of $\\mathcal{K} = p_1+p_2$, corresponds to the known training reference indices: 18000 - 26000, 28000 - 38000, 44000 - 50000,52000 - 58000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ76u-x7di-7",
        "outputId": "2ee3940b-df4e-4d65-901e-13a5ed31d5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the known data, reference: (30000, 1000)\n",
            "Shape of the known labels, reference: (30000, 15)\n"
          ]
        }
      ],
      "source": [
        "indices = [slice(18000, 26000),slice(28000, 38000),slice(44000, 50000),slice(52000, 58000)]\n",
        "\n",
        "Known_data_X_reference = np.concatenate([data_X_reference[idx, :] for idx in indices], axis=0)\n",
        "\n",
        "Known_data_X_train_label_int = []\n",
        "\n",
        "for i in range(2000*15):\n",
        "  Known_data_X_train_label_int.append(int(data_y_reference[i]))\n",
        "\n",
        "Known_data_X_train_label = tf.keras.utils.to_categorical(Known_data_X_train_label_int)\n",
        "print(\"Shape of the known data, reference:\", Known_data_X_reference.shape)\n",
        "print(\"Shape of the known labels, reference:\", Known_data_X_train_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZzs-rpIfJ9f"
      },
      "source": [
        "# Known data indices, testing dataset. Indices = 900 - 1300, 1400 - 1900, 2200 - 2500, 2600 - 2900.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1a48lk8djF5",
        "outputId": "4d6fb80d-602f-406d-eaeb-e78879397ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the known data, test: (1500, 1000)\n",
            "Shape of the known labels, test: (1500, 15)\n"
          ]
        }
      ],
      "source": [
        "indices = [slice(900, 1300),slice(1400, 1900),slice(2200, 2500),slice(2600, 2900)]\n",
        "Known_data_X_test = np.concatenate([data_X_test[idx, :] for idx in indices], axis=0)\n",
        "\n",
        "Known_data_X_test_label_int = []\n",
        "\n",
        "for i in range(100*15):\n",
        "  Known_data_X_test_label_int.append(int(data_y_test[i]))\n",
        "\n",
        "Known_data_X_test_label = tf.keras.utils.to_categorical(Known_data_X_test_label_int)\n",
        "print(\"Shape of the known data, test:\", Known_data_X_test.shape)\n",
        "print(\"Shape of the known labels, test:\", Known_data_X_test_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TM7Jf_ugOVY"
      },
      "source": [
        "# Finetuning dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33-QtJx0djIS"
      },
      "outputs": [],
      "source": [
        "# Finetuning\n",
        "data_X_finetune = np.load(\"/content/gdrive/MyDrive/Stanford_data/X_finetune.npy\")\n",
        "# Test data\n",
        "data_X_test = np.load(\"/content/gdrive/MyDrive/Stanford_data/X_test.npy\")\n",
        "\n",
        "data_y_finetune = np.load(\"/content/gdrive/MyDrive/Stanford_data/y_finetune.npy\")\n",
        "# Test labels\n",
        "data_y_test = np.load(\"/content/gdrive/MyDrive/Stanford_data/y_test.npy\")\n",
        "\n",
        "data_y_finetune_int = []\n",
        "\n",
        "for i in range(data_y_finetune.shape[0]):\n",
        "  data_y_finetune_int.append(int(data_y_finetune[i]))\n",
        "\n",
        "data_y_test_int = []\n",
        "\n",
        "for i in range(data_y_test.shape[0]):\n",
        "  data_y_test_int.append(int(data_y_test[i]))\n",
        "\n",
        "train_label = tf.keras.utils.to_categorical(data_y_finetune_int)\n",
        "test_label = tf.keras.utils.to_categorical(data_y_test_int)\n",
        "\n",
        "#data_X_finetune, train_label = shuffle(data_X_finetune, train_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQhOuowIgkWf"
      },
      "source": [
        "# Known data for finetuning. Finetuning indices = 900 - 1300, 1400 - 1900, 2200 - 2500, 2600 - 2900.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDHDu8argdnn",
        "outputId": "e54fddd0-1d62-4685-d17f-d8ab6d1db005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the known data, finetune: (1500, 1000)\n",
            "Shape of the known labels, finetune: (1500, 15)\n"
          ]
        }
      ],
      "source": [
        "indices = [slice(900, 1300),slice(1400, 1900),slice(2200, 2500),slice(2600, 2900)]\n",
        "\n",
        "Known_data_X_finetune = np.concatenate([data_X_finetune[idx, :] for idx in indices], axis=0)\n",
        "\n",
        "Known_data_X_finetune_label_int = []\n",
        "\n",
        "for i in range(100*15):\n",
        "  Known_data_X_finetune_label_int.append(int(data_y_finetune[i]))\n",
        "\n",
        "Known_data_X_finetune_label = tf.keras.utils.to_categorical(Known_data_X_finetune_label_int)\n",
        "\n",
        "print(\"Shape of the known data, finetune:\", Known_data_X_finetune.shape)\n",
        "print(\"Shape of the known labels, finetune:\", Known_data_X_finetune_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmd9kT5hnsuw"
      },
      "source": [
        "# Never seen before, $\\mathcal{N} = p_4$, test: 0 - 500, 700 - 900, 1900 - 2100, 2500 - 2600.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZalnP5OKoA8l",
        "outputId": "7fb25a76-54f9-4d64-b275-914b392d15f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the Neverseen data, test: (1000, 1000)\n",
            "Shape of the Neverseen labels, test: (1000, 10)\n"
          ]
        }
      ],
      "source": [
        "indices = [slice(0, 500),slice(700, 900),slice(1900, 2100),slice(2500, 2600)]\n",
        "NeverSeen_data_X_test = np.concatenate([data_X_test[idx, :] for idx in indices], axis=0)\n",
        "\n",
        "NeverSeen_data_X_test_label_int = []\n",
        "\n",
        "for i in range(100*10):\n",
        "  NeverSeen_data_X_test_label_int.append(int(data_y_test[i]))\n",
        "\n",
        "NeverSeen_data_X_test_label = tf.keras.utils.to_categorical(NeverSeen_data_X_test_label_int)\n",
        "print(\"Shape of the Neverseen data, test:\", NeverSeen_data_X_test.shape)\n",
        "print(\"Shape of the Neverseen labels, test:\", NeverSeen_data_X_test_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkB5irz-g4nT"
      },
      "source": [
        "# Initializing the our NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va3BhxK3gdh9"
      },
      "outputs": [],
      "source": [
        "# Create a checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from numpy import interp\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "#from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve, plot_roc_curve\n",
        "from sklearn.metrics import confusion_matrix, classification_report, auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D, concatenate, BatchNormalization, Activation, add\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, Flatten, Dense, GlobalAveragePooling1D, GlobalMaxPooling1D, Multiply, Conv1DTranspose, LeakyReLU, Dropout\n",
        "from keras.models import Model, model_from_json\n",
        "#from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFBxaPt1hDQQ",
        "outputId": "98cc11c7-fb6c-4aca-bf85-ebc900fcd88c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet29\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1000, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " zero_padding1d (ZeroPaddin  (None, 1006, 1)              0         ['input_1[0][0]']             \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " Convolution1 (Conv1D)       (None, 500, 64)              512       ['zero_padding1d[0][0]']      \n",
            "                                                                                                  \n",
            " BatchNormStage1 (BatchNorm  (None, 500, 64)              256       ['Convolution1[0][0]']        \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 500, 64)              0         ['BatchNormStage1[0][0]']     \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, 249, 64)              0         ['activation[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 249, 64)              12352     ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 249, 64)              256       ['conv1d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 249, 64)              0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 249, 64)              12352     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 249, 64)              256       ['conv1d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 249, 64)              0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " res2a_branch (Conv1D)       (None, 249, 256)             49408     ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 249, 256)             16640     ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 249, 256)             1024      ['res2a_branch[0][0]']        \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 249, 256)             1024      ['conv1d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 249, 256)             0         ['batch_normalization_2[0][0]'\n",
            "                                                                    , 'batch_normalization_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 249, 256)             0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 249, 64)              16448     ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 249, 64)              256       ['conv1d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 249, 64)              0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 249, 64)              12352     ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 249, 64)              256       ['conv1d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 249, 64)              0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 249, 256)             16640     ['activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 249, 256)             1024      ['conv1d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 249, 256)             0         ['batch_normalization_6[0][0]'\n",
            "                                                                    , 'activation_3[0][0]']       \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 249, 256)             0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 125, 128)             98432     ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 125, 128)             512       ['conv1d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 125, 128)             0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 125, 128)             49280     ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 125, 128)             512       ['conv1d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 125, 128)             0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " res3a_branch (Conv1D)       (None, 125, 512)             197120    ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, 125, 512)             131584    ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 125, 512)             2048      ['res3a_branch[0][0]']        \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 125, 512)             2048      ['conv1d_8[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 125, 512)             0         ['batch_normalization_9[0][0]'\n",
            "                                                                    , 'batch_normalization_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 125, 512)             0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)           (None, 125, 128)             65664     ['activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 125, 128)             512       ['conv1d_9[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 125, 128)             0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)          (None, 125, 128)             49280     ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 125, 128)             512       ['conv1d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 125, 128)             0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)          (None, 125, 512)             66048     ['activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 125, 512)             2048      ['conv1d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 125, 512)             0         ['batch_normalization_13[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 125, 512)             0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)          (None, 63, 256)              393472    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 63, 256)              1024      ['conv1d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 63, 256)              0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)          (None, 63, 256)              196864    ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 63, 256)              1024      ['conv1d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 63, 256)              0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " res4a_branch (Conv1D)       (None, 63, 1024)             787456    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)          (None, 63, 1024)             525312    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 63, 1024)             4096      ['res4a_branch[0][0]']        \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 63, 1024)             4096      ['conv1d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 63, 1024)             0         ['batch_normalization_16[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 63, 1024)             0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)          (None, 63, 256)              262400    ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 63, 256)              1024      ['conv1d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 63, 256)              0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)          (None, 63, 256)              196864    ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 63, 256)              1024      ['conv1d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 63, 256)              0         ['batch_normalization_19[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)          (None, 63, 1024)             263168    ['activation_17[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 63, 1024)             4096      ['conv1d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 63, 1024)             0         ['batch_normalization_20[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 63, 1024)             0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)          (None, 32, 256)              786688    ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 32, 256)              1024      ['conv1d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 32, 256)              0         ['batch_normalization_21[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)          (None, 32, 256)              196864    ['activation_19[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 32, 256)              1024      ['conv1d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 32, 256)              0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " res5a_branch (Conv1D)       (None, 32, 2048)             1574912   ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)          (None, 32, 2048)             2099200   ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 32, 2048)             8192      ['res5a_branch[0][0]']        \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 32, 2048)             8192      ['conv1d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 32, 2048)             0         ['batch_normalization_23[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 32, 2048)             0         ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)          (None, 32, 256)              524544    ['activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 32, 256)              1024      ['conv1d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 32, 256)              0         ['batch_normalization_25[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)          (None, 32, 256)              196864    ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 32, 256)              1024      ['conv1d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 32, 256)              0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)          (None, 32, 2048)             526336    ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 32, 2048)             8192      ['conv1d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 32, 2048)             0         ['batch_normalization_27[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 32, 2048)             0         ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 2048)                 0         ['activation_24[0][0]']       \n",
            " GlobalAveragePooling1D)                                                                          \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 1, 2048)              0         ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1, 128)               262272    ['reshape[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1, 2048)              264192    ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 32, 2048)             0         ['activation_24[0][0]',       \n",
            "                                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_transpose (Conv1DTr  (None, 64, 64)               393280    ['multiply[0][0]']            \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 64, 64)               256       ['conv1d_transpose[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 64)               0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4096)                 0         ['leaky_re_lu[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 4096)                 0         ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 15)                   61455     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10364111 (39.54 MB)\n",
            "Trainable params: 10306383 (39.32 MB)\n",
            "Non-trainable params: 57728 (225.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "NoF = 64  # Number of filters in the first convolutional layer\n",
        "SE_RATIO = 16  # Squeeze-and-Excitation ratio\n",
        "\n",
        "nb_classes = 15\n",
        "\n",
        "\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "initializer2 = tf.keras.initializers.HeUniform(seed=0)\n",
        "\n",
        "def squeeze_excitation_block(X, ratio=16):\n",
        "    num_channels = X.shape[-1]\n",
        "    se = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
        "    se = tf.keras.layers.Reshape((1, num_channels))(se)\n",
        "    se = tf.keras.layers.Dense(num_channels // ratio, activation='relu', kernel_initializer=initializer)(se)\n",
        "    se = tf.keras.layers.Dense(num_channels, activation='sigmoid', kernel_initializer=initializer)(se)\n",
        "    return tf.keras.layers.Multiply()([X, se])\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, Add\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
        "initializer2 = tf.keras.initializers.HeUniform(seed=0)\n",
        "Stride=1\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "\n",
        "  #defining name basis\n",
        "  ConvNameBase = 'res' + str(stage) + block + '_branch'\n",
        "  BatchNormBase = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "  #retrieve filters\n",
        "  F1,F2,F3 = filters\n",
        "\n",
        "  #save the input value. You'll need this later to add back the main path\n",
        "  x_shortcut = X\n",
        "\n",
        "  ### First component of the main path ###\n",
        "  X = tf.keras.layers.Conv1D(filters=F1, kernel_size=1, strides=Stride, padding='same',\n",
        "             kernel_initializer=initializer#, name=ConvNameBase\n",
        "             )(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                         )(X)\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  ### Second component of main path ###\n",
        "  X = tf.keras.layers.Conv1D(filters=F2, kernel_size=f, strides=Stride, padding='same',\n",
        "             kernel_initializer=initializer, #name=ConvNameBase\n",
        "             )(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                         )(X)\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  #Third Component of main path\n",
        "  X = tf.keras.layers.Conv1D(filters=F3, kernel_size=1, strides=Stride, padding='same',\n",
        "             kernel_initializer=initializer#,name=ConvNameBase\n",
        "             )(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                         )(X)\n",
        "\n",
        "  #Final step: add shortcut to the main path, and pass it through ReLU activation\n",
        "  X = tf.keras.layers.Add()([X, x_shortcut])\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "\n",
        "  #Defining name bases\n",
        "  ConvNameBase = 'res' + str(stage) + block + '_branch'\n",
        "  BatchNormBase = 'res' + str(stage) + block + '_branch'\n",
        "\n",
        "  #retrive n_filters\n",
        "  F1, F2, F3 = filters\n",
        "\n",
        "  #Save the input value\n",
        "  x_shortcut = X\n",
        "\n",
        "  #First component of the main path\n",
        "  X = tf.keras.layers.Conv1D(F1, strides=s, kernel_size=f, kernel_initializer=initializer,\n",
        "             padding='same'#, name=ConvNameBase\n",
        "             )(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                         )(X)\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  #Second Component of main path\n",
        "  X = tf.keras.layers.Conv1D(filters=F2, kernel_size=f, strides=Stride, padding='same',\n",
        "             kernel_initializer=initializer#, name=ConvNameBase\n",
        "             )(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                         )(X)\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  #Third component of main path\n",
        "  X = tf.keras.layers.Conv1D(filters=F3, kernel_size=f, strides=Stride, padding='same',\n",
        "             kernel_initializer=initializer, name=ConvNameBase)(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "  )(X)\n",
        "\n",
        "  ###### SHORTCUT PATH ######\n",
        "  x_shortcut = tf.keras.layers.Conv1D(filters = F3, kernel_size=1, strides=s,\n",
        "                      padding='same', #name=ConvNameBase,\n",
        "                      kernel_initializer=initializer)(x_shortcut)\n",
        "  x_shortcut = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,\n",
        "                                  )(x_shortcut)\n",
        "\n",
        "  #Add shortcut to main path and pass in through ReLU activation\n",
        "  X = tf.keras.layers.Add()([X, x_shortcut])\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "\n",
        "  return X\n",
        "\n",
        "def create_model():\n",
        "    input_shape = (1000, 1)\n",
        "\n",
        "    x_input = tf.keras.layers.Input(input_shape)\n",
        "    X = tf.keras.layers.ZeroPadding1D(padding=3)(x_input)\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(NoF, kernel_size=7, strides=2, name='Convolution1',\n",
        "                               kernel_initializer=initializer)(X)\n",
        "    X = tf.keras.layers.BatchNormalization(name=\"BatchNormStage1\")(X)\n",
        "    X = tf.keras.layers.Activation('relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(3, strides=2)(X)\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[NoF, NoF, NoF * 4], stage=2, block='a', s=1)\n",
        "    X = identity_block(X, 3, [NoF, NoF, NoF * 4], stage=2, block='b')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[NoF * 2, NoF * 2, NoF * 8], stage=3, block='a')\n",
        "    X = identity_block(X, 3, [NoF * 2, NoF * 2, NoF * 8], stage=3, block='b')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[NoF * 4, NoF * 4, NoF * 16], stage=4, block='a')\n",
        "    X = identity_block(X, 3, [NoF * 4, NoF * 4, NoF * 16], stage=4, block='b')\n",
        "\n",
        "    X = convolutional_block(X, f=3, filters=[NoF * 4, NoF * 4, NoF * 32], stage=5, block='a')\n",
        "    X = identity_block(X, f=3, filters=[NoF * 4, NoF * 4, NoF * 32], stage=5, block='b')\n",
        "\n",
        "    # SE mechanism before transposed convolutional layers\n",
        "    X = squeeze_excitation_block(X)\n",
        "\n",
        "    # Transposed convolutional layers\n",
        "    X = Conv1DTranspose(filters=NoF, kernel_size=3, strides=2, padding='same', kernel_initializer=initializer)(X)\n",
        "    X = tf.keras.layers.BatchNormalization(axis=2, momentum=0.99, trainable=False,)(X)\n",
        "    X = LeakyReLU(alpha=0.2)(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    # Dropout regularization\n",
        "    X = Dropout(0.5)(X)\n",
        "\n",
        "    X = Dense(nb_classes, activation='softmax', kernel_initializer=initializer)(X)\n",
        "\n",
        "    res_net = models.Model(inputs=x_input, outputs=X, name='ResNet29')\n",
        "\n",
        "    return res_net\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HvrCkIQjJXd"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  res_net = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=res_net)\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=10, mode='min'), ModelCheckpoint('/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
        "  res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "  Known_data_X_reference, Known_data_X_train_label = shuffle(Known_data_X_reference, Known_data_X_train_label)\n",
        "\n",
        "  history = res_net.fit(Known_data_X_reference, Known_data_X_train_label, epochs=200, batch_size=32, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)\n",
        "\n",
        "  test_loss, test_acc = res_net.evaluate(Known_data_X_test, Known_data_X_test_label)\n",
        "\n",
        "  print('Test accuracy, 01_SJ11:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzqOXpJUhDVE",
        "outputId": "ba0210d7-881f-41ea-8ec5-2b4cee63a29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 48s 954ms/step - loss: 2.6347 - accuracy: 0.5740\n",
            "Test accuracy, 01 run: 0.5740000009536743\n",
            "47/47 [==============================] - 35s 713ms/step - loss: 3.1187 - accuracy: 0.5707\n",
            "Test accuracy, 02 run: 0.5706666707992554\n",
            "47/47 [==============================] - 42s 867ms/step - loss: 2.6488 - accuracy: 0.5753\n",
            "Test accuracy, 03 run: 0.5753333568572998\n",
            "47/47 [==============================] - 37s 690ms/step - loss: 2.4719 - accuracy: 0.5833\n",
            "Test accuracy, 04 run: 0.5833333134651184\n",
            "47/47 [==============================] - 34s 668ms/step - loss: 2.9781 - accuracy: 0.5480\n",
            "Test accuracy, 05 run: 0.5479999780654907\n",
            "47/47 [==============================] - 45s 885ms/step - loss: 3.1219 - accuracy: 0.5773\n",
            "Test accuracy, 06 run: 0.5773333311080933\n",
            "47/47 [==============================] - 36s 705ms/step - loss: 3.2779 - accuracy: 0.5660\n",
            "Test accuracy, 07 run: 0.5659999847412109\n",
            "47/47 [==============================] - 37s 749ms/step - loss: 2.1928 - accuracy: 0.5953\n",
            "Test accuracy, 08 run: 0.5953333377838135\n",
            "47/47 [==============================] - 36s 724ms/step - loss: 2.8475 - accuracy: 0.5620\n",
            "Test accuracy, 09 run: 0.5619999766349792\n",
            "47/47 [==============================] - 33s 666ms/step - loss: 2.4664 - accuracy: 0.5753\n",
            "Test accuracy, 10 run: 0.5753333568572998\n",
            "47/47 [==============================] - 36s 725ms/step - loss: 2.5789 - accuracy: 0.5880\n",
            "Test accuracy, 11 run: 0.5879999995231628\n",
            "47/47 [==============================] - 34s 691ms/step - loss: 2.7938 - accuracy: 0.5467\n",
            "Test accuracy, 12 run: 0.54666668176651\n",
            "47/47 [==============================] - 36s 723ms/step - loss: 2.8783 - accuracy: 0.5667\n",
            "Test accuracy, 13 run: 0.5666666626930237\n",
            "47/47 [==============================] - 33s 664ms/step - loss: 2.5654 - accuracy: 0.5720\n",
            "Test accuracy, 14 run: 0.5720000267028809\n",
            "47/47 [==============================] - 39s 781ms/step - loss: 2.4099 - accuracy: 0.5820\n",
            "Test accuracy, 15 run: 0.5820000171661377\n",
            "47/47 [==============================] - 35s 694ms/step - loss: 2.9534 - accuracy: 0.5587\n",
            "Test accuracy, 16 run: 0.5586666464805603\n",
            "47/47 [==============================] - 37s 712ms/step - loss: 2.8338 - accuracy: 0.5960\n",
            "Test accuracy, 17 run: 0.5960000157356262\n",
            "47/47 [==============================] - 39s 780ms/step - loss: 2.5771 - accuracy: 0.6013\n",
            "Test accuracy, 18 run: 0.6013333201408386\n",
            "47/47 [==============================] - 36s 732ms/step - loss: 2.6255 - accuracy: 0.5980\n",
            "Test accuracy, 19 run: 0.5979999899864197\n",
            "47/47 [==============================] - 37s 703ms/step - loss: 3.2296 - accuracy: 0.5607\n",
            "Test accuracy, 20 run: 0.5606666803359985\n"
          ]
        }
      ],
      "source": [
        "# Now run and see the models with the best validation accuracy\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 01 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/02_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 02 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/03_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 03 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/04_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 04 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/05_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 05 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/06_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 06 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/07_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 07 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/08_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 08 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/09_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 09 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/10_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 10 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/11_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 11 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/12_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 12 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/13_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 13 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 14 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 15 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 16 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 17 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 18 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 19 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 20 run:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwvJhk93hDWM",
        "outputId": "aab8e53b-1c38-40f5-ff41-6556ff38074f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 142: val_loss did not improve from 0.21060\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0900 - accuracy: 0.9667 - val_loss: 0.2115 - val_accuracy: 0.9433\n",
            "Epoch 143/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1088 - accuracy: 0.9583\n",
            "Epoch 143: val_loss did not improve from 0.21060\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1094 - accuracy: 0.9575 - val_loss: 0.2158 - val_accuracy: 0.9433\n",
            "Epoch 144/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1043 - accuracy: 0.9609\n",
            "Epoch 144: val_loss did not improve from 0.21060\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1053 - accuracy: 0.9592 - val_loss: 0.2115 - val_accuracy: 0.9433\n",
            "Epoch 145/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0939 - accuracy: 0.9661\n",
            "Epoch 145: val_loss improved from 0.21060 to 0.20745, saving model to /content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.0932 - accuracy: 0.9667 - val_loss: 0.2074 - val_accuracy: 0.9367\n",
            "Epoch 146/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1039 - accuracy: 0.9635\n",
            "Epoch 146: val_loss did not improve from 0.20745\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1019 - accuracy: 0.9642 - val_loss: 0.2090 - val_accuracy: 0.9433\n",
            "Epoch 147/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9637\n",
            "Epoch 147: val_loss improved from 0.20745 to 0.20698, saving model to /content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0994 - accuracy: 0.9625 - val_loss: 0.2070 - val_accuracy: 0.9367\n",
            "Epoch 148/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9625\n",
            "Epoch 148: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 3s 73ms/step - loss: 0.1028 - accuracy: 0.9625 - val_loss: 0.2108 - val_accuracy: 0.9367\n",
            "Epoch 149/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9644\n",
            "Epoch 149: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 41ms/step - loss: 0.1050 - accuracy: 0.9650 - val_loss: 0.2186 - val_accuracy: 0.9367\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9679\n",
            "Epoch 150: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0886 - accuracy: 0.9683 - val_loss: 0.2114 - val_accuracy: 0.9400\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0976 - accuracy: 0.9645\n",
            "Epoch 151: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 41ms/step - loss: 0.0992 - accuracy: 0.9642 - val_loss: 0.2118 - val_accuracy: 0.9333\n",
            "Epoch 152/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9725\n",
            "Epoch 152: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.0857 - accuracy: 0.9725 - val_loss: 0.2109 - val_accuracy: 0.9333\n",
            "Epoch 153/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9683\n",
            "Epoch 153: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0900 - accuracy: 0.9683 - val_loss: 0.2117 - val_accuracy: 0.9367\n",
            "Epoch 154/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9725\n",
            "Epoch 154: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0812 - accuracy: 0.9725 - val_loss: 0.2084 - val_accuracy: 0.9400\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9721\n",
            "Epoch 155: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0877 - accuracy: 0.9725 - val_loss: 0.2073 - val_accuracy: 0.9400\n",
            "Epoch 156/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9654\n",
            "Epoch 156: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0920 - accuracy: 0.9658 - val_loss: 0.2073 - val_accuracy: 0.9400\n",
            "Epoch 157/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0803 - accuracy: 0.9696\n",
            "Epoch 157: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0823 - accuracy: 0.9692 - val_loss: 0.2077 - val_accuracy: 0.9400\n",
            "Epoch 158/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0992 - accuracy: 0.9696\n",
            "Epoch 158: val_loss did not improve from 0.20698\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0988 - accuracy: 0.9692 - val_loss: 0.2080 - val_accuracy: 0.9400\n",
            "Epoch 159/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9725\n",
            "Epoch 159: val_loss improved from 0.20698 to 0.20441, saving model to /content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.0774 - accuracy: 0.9725 - val_loss: 0.2044 - val_accuracy: 0.9433\n",
            "Epoch 160/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9725\n",
            "Epoch 160: val_loss did not improve from 0.20441\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0826 - accuracy: 0.9725 - val_loss: 0.2052 - val_accuracy: 0.9433\n",
            "Epoch 161/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9700\n",
            "Epoch 161: val_loss did not improve from 0.20441\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0843 - accuracy: 0.9700 - val_loss: 0.2079 - val_accuracy: 0.9433\n",
            "Epoch 162/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9721\n",
            "Epoch 162: val_loss improved from 0.20441 to 0.20364, saving model to /content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0868 - accuracy: 0.9725 - val_loss: 0.2036 - val_accuracy: 0.9367\n",
            "Epoch 163/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0826 - accuracy: 0.9688\n",
            "Epoch 163: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0834 - accuracy: 0.9692 - val_loss: 0.2052 - val_accuracy: 0.9433\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9696\n",
            "Epoch 164: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0885 - accuracy: 0.9700 - val_loss: 0.2073 - val_accuracy: 0.9433\n",
            "Epoch 165/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9658\n",
            "Epoch 165: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0995 - accuracy: 0.9658 - val_loss: 0.2099 - val_accuracy: 0.9433\n",
            "Epoch 166/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9800\n",
            "Epoch 166: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.0779 - accuracy: 0.9800 - val_loss: 0.2049 - val_accuracy: 0.9433\n",
            "Epoch 167/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9637\n",
            "Epoch 167: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0896 - accuracy: 0.9642 - val_loss: 0.2061 - val_accuracy: 0.9433\n",
            "Epoch 168/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9747\n",
            "Epoch 168: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0664 - accuracy: 0.9742 - val_loss: 0.2039 - val_accuracy: 0.9467\n",
            "Epoch 169/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9725\n",
            "Epoch 169: val_loss did not improve from 0.20364\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0735 - accuracy: 0.9725 - val_loss: 0.2074 - val_accuracy: 0.9433\n",
            "Epoch 170/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9704\n",
            "Epoch 170: val_loss improved from 0.20364 to 0.20215, saving model to /content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.0711 - accuracy: 0.9708 - val_loss: 0.2021 - val_accuracy: 0.9400\n",
            "Epoch 171/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9747\n",
            "Epoch 171: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0769 - accuracy: 0.9742 - val_loss: 0.2059 - val_accuracy: 0.9367\n",
            "Epoch 172/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9654\n",
            "Epoch 172: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0931 - accuracy: 0.9650 - val_loss: 0.2069 - val_accuracy: 0.9333\n",
            "Epoch 173/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9733\n",
            "Epoch 173: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0807 - accuracy: 0.9733 - val_loss: 0.2068 - val_accuracy: 0.9400\n",
            "Epoch 174/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9725\n",
            "Epoch 174: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0792 - accuracy: 0.9725 - val_loss: 0.2069 - val_accuracy: 0.9467\n",
            "Epoch 175/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9764\n",
            "Epoch 175: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.0687 - accuracy: 0.9767 - val_loss: 0.2022 - val_accuracy: 0.9433\n",
            "Epoch 176/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0858 - accuracy: 0.9696\n",
            "Epoch 176: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0867 - accuracy: 0.9683 - val_loss: 0.2040 - val_accuracy: 0.9333\n",
            "Epoch 177/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9704\n",
            "Epoch 177: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0812 - accuracy: 0.9700 - val_loss: 0.2051 - val_accuracy: 0.9467\n",
            "Epoch 178/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0784 - accuracy: 0.9688\n",
            "Epoch 178: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0786 - accuracy: 0.9692 - val_loss: 0.2102 - val_accuracy: 0.9433\n",
            "Epoch 179/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9792\n",
            "Epoch 179: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0606 - accuracy: 0.9792 - val_loss: 0.2101 - val_accuracy: 0.9367\n",
            "Epoch 180/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9755\n",
            "Epoch 180: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0698 - accuracy: 0.9758 - val_loss: 0.2088 - val_accuracy: 0.9400\n",
            "Epoch 181/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0844 - accuracy: 0.9748\n",
            "Epoch 181: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0825 - accuracy: 0.9750 - val_loss: 0.2117 - val_accuracy: 0.9300\n",
            "Epoch 182/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9833\n",
            "Epoch 182: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.2050 - val_accuracy: 0.9400\n",
            "Epoch 183/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9792\n",
            "Epoch 183: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0634 - accuracy: 0.9792 - val_loss: 0.2083 - val_accuracy: 0.9367\n",
            "Epoch 184/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9780\n",
            "Epoch 184: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 3s 75ms/step - loss: 0.0691 - accuracy: 0.9783 - val_loss: 0.2083 - val_accuracy: 0.9367\n",
            "Epoch 185/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9747\n",
            "Epoch 185: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0646 - accuracy: 0.9742 - val_loss: 0.2041 - val_accuracy: 0.9433\n",
            "Epoch 186/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9704\n",
            "Epoch 186: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0790 - accuracy: 0.9708 - val_loss: 0.2089 - val_accuracy: 0.9433\n",
            "Epoch 187/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9755\n",
            "Epoch 187: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0778 - accuracy: 0.9750 - val_loss: 0.2151 - val_accuracy: 0.9267\n",
            "Epoch 188/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0784 - accuracy: 0.9714\n",
            "Epoch 188: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0766 - accuracy: 0.9725 - val_loss: 0.2092 - val_accuracy: 0.9367\n",
            "Epoch 189/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0588 - accuracy: 0.9852\n",
            "Epoch 189: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0612 - accuracy: 0.9842 - val_loss: 0.2087 - val_accuracy: 0.9433\n",
            "Epoch 190/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9738\n",
            "Epoch 190: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0753 - accuracy: 0.9742 - val_loss: 0.2113 - val_accuracy: 0.9367\n",
            "Epoch 191/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9733\n",
            "Epoch 191: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0620 - accuracy: 0.9733 - val_loss: 0.2117 - val_accuracy: 0.9433\n",
            "Epoch 192/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9696\n",
            "Epoch 192: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0816 - accuracy: 0.9700 - val_loss: 0.2120 - val_accuracy: 0.9433\n",
            "Epoch 193/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9772\n",
            "Epoch 193: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0644 - accuracy: 0.9767 - val_loss: 0.2107 - val_accuracy: 0.9367\n",
            "Epoch 194/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9806\n",
            "Epoch 194: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0606 - accuracy: 0.9808 - val_loss: 0.2086 - val_accuracy: 0.9467\n",
            "Epoch 195/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9725\n",
            "Epoch 195: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0690 - accuracy: 0.9725 - val_loss: 0.2127 - val_accuracy: 0.9400\n",
            "Epoch 196/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9814\n",
            "Epoch 196: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0531 - accuracy: 0.9817 - val_loss: 0.2109 - val_accuracy: 0.9467\n",
            "Epoch 197/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9738\n",
            "Epoch 197: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0714 - accuracy: 0.9742 - val_loss: 0.2099 - val_accuracy: 0.9400\n",
            "Epoch 198/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 0.9848\n",
            "Epoch 198: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0515 - accuracy: 0.9850 - val_loss: 0.2084 - val_accuracy: 0.9433\n",
            "Epoch 199/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9808\n",
            "Epoch 199: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0503 - accuracy: 0.9808 - val_loss: 0.2095 - val_accuracy: 0.9433\n",
            "Epoch 200/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0573 - accuracy: 0.9831\n",
            "Epoch 200: val_loss did not improve from 0.20215\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.2082 - val_accuracy: 0.9467\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.4263 - accuracy: 0.8887\n",
            "Test accuracy, 14 run, after finetuning: 0.8886666893959045\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.0654 - accuracy: 0.6375\n",
            "Epoch 1: val_loss improved from inf to 1.26819, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 44s 542ms/step - loss: 2.0654 - accuracy: 0.6375 - val_loss: 1.2682 - val_accuracy: 0.7433\n",
            "Epoch 2/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.2397 - accuracy: 0.7078\n",
            "Epoch 2: val_loss improved from 1.26819 to 0.87317, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 1.2302 - accuracy: 0.7083 - val_loss: 0.8732 - val_accuracy: 0.7600\n",
            "Epoch 3/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.9951 - accuracy: 0.7292\n",
            "Epoch 3: val_loss improved from 0.87317 to 0.74487, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.9951 - accuracy: 0.7292 - val_loss: 0.7449 - val_accuracy: 0.7667\n",
            "Epoch 4/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.8363 - accuracy: 0.7558\n",
            "Epoch 4: val_loss improved from 0.74487 to 0.68870, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.8363 - accuracy: 0.7558 - val_loss: 0.6887 - val_accuracy: 0.7800\n",
            "Epoch 5/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.7615 - accuracy: 0.7633\n",
            "Epoch 5: val_loss improved from 0.68870 to 0.65149, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.7615 - accuracy: 0.7633 - val_loss: 0.6515 - val_accuracy: 0.7900\n",
            "Epoch 6/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.7189 - accuracy: 0.7675\n",
            "Epoch 6: val_loss improved from 0.65149 to 0.62400, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.7189 - accuracy: 0.7675 - val_loss: 0.6240 - val_accuracy: 0.7933\n",
            "Epoch 7/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6718 - accuracy: 0.7700\n",
            "Epoch 7: val_loss improved from 0.62400 to 0.59840, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.6580 - accuracy: 0.7733 - val_loss: 0.5984 - val_accuracy: 0.7933\n",
            "Epoch 8/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6157 - accuracy: 0.8003\n",
            "Epoch 8: val_loss improved from 0.59840 to 0.57878, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.6224 - accuracy: 0.7983 - val_loss: 0.5788 - val_accuracy: 0.8167\n",
            "Epoch 9/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6092 - accuracy: 0.7995\n",
            "Epoch 9: val_loss improved from 0.57878 to 0.56227, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.6091 - accuracy: 0.7983 - val_loss: 0.5623 - val_accuracy: 0.8167\n",
            "Epoch 10/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.7981\n",
            "Epoch 10: val_loss improved from 0.56227 to 0.54423, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.6133 - accuracy: 0.8000 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 11/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5939 - accuracy: 0.7995\n",
            "Epoch 11: val_loss improved from 0.54423 to 0.53062, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.5869 - accuracy: 0.8025 - val_loss: 0.5306 - val_accuracy: 0.8233\n",
            "Epoch 12/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5775 - accuracy: 0.8050\n",
            "Epoch 12: val_loss improved from 0.53062 to 0.51633, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.5775 - accuracy: 0.8050 - val_loss: 0.5163 - val_accuracy: 0.8300\n",
            "Epoch 13/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5101 - accuracy: 0.8209\n",
            "Epoch 13: val_loss improved from 0.51633 to 0.50378, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.5053 - accuracy: 0.8233 - val_loss: 0.5038 - val_accuracy: 0.8300\n",
            "Epoch 14/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5312 - accuracy: 0.8082\n",
            "Epoch 14: val_loss improved from 0.50378 to 0.49126, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.5354 - accuracy: 0.8067 - val_loss: 0.4913 - val_accuracy: 0.8333\n",
            "Epoch 15/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4886 - accuracy: 0.8394\n",
            "Epoch 15: val_loss improved from 0.49126 to 0.48179, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 149ms/step - loss: 0.4936 - accuracy: 0.8367 - val_loss: 0.4818 - val_accuracy: 0.8433\n",
            "Epoch 16/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4892 - accuracy: 0.8299\n",
            "Epoch 16: val_loss improved from 0.48179 to 0.47128, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4897 - accuracy: 0.8300 - val_loss: 0.4713 - val_accuracy: 0.8400\n",
            "Epoch 17/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.8361\n",
            "Epoch 17: val_loss improved from 0.47128 to 0.46407, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4586 - accuracy: 0.8383 - val_loss: 0.4641 - val_accuracy: 0.8367\n",
            "Epoch 18/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4584 - accuracy: 0.8395\n",
            "Epoch 18: val_loss improved from 0.46407 to 0.45971, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.4631 - accuracy: 0.8392 - val_loss: 0.4597 - val_accuracy: 0.8467\n",
            "Epoch 19/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4395 - accuracy: 0.8490\n",
            "Epoch 19: val_loss improved from 0.45971 to 0.44669, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4509 - accuracy: 0.8433 - val_loss: 0.4467 - val_accuracy: 0.8433\n",
            "Epoch 20/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4594 - accuracy: 0.8404\n",
            "Epoch 20: val_loss improved from 0.44669 to 0.43998, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.4582 - accuracy: 0.8400 - val_loss: 0.4400 - val_accuracy: 0.8533\n",
            "Epoch 21/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.8421\n",
            "Epoch 21: val_loss improved from 0.43998 to 0.43297, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4640 - accuracy: 0.8417 - val_loss: 0.4330 - val_accuracy: 0.8533\n",
            "Epoch 22/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4224 - accuracy: 0.8602\n",
            "Epoch 22: val_loss improved from 0.43297 to 0.42664, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.4203 - accuracy: 0.8592 - val_loss: 0.4266 - val_accuracy: 0.8533\n",
            "Epoch 23/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3925 - accuracy: 0.8617\n",
            "Epoch 23: val_loss improved from 0.42664 to 0.41806, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.3925 - accuracy: 0.8617 - val_loss: 0.4181 - val_accuracy: 0.8533\n",
            "Epoch 24/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8598\n",
            "Epoch 24: val_loss improved from 0.41806 to 0.41148, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.4152 - accuracy: 0.8592 - val_loss: 0.4115 - val_accuracy: 0.8567\n",
            "Epoch 25/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8514\n",
            "Epoch 25: val_loss improved from 0.41148 to 0.40802, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.4242 - accuracy: 0.8517 - val_loss: 0.4080 - val_accuracy: 0.8633\n",
            "Epoch 26/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3865 - accuracy: 0.8698\n",
            "Epoch 26: val_loss improved from 0.40802 to 0.40296, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.3832 - accuracy: 0.8717 - val_loss: 0.4030 - val_accuracy: 0.8700\n",
            "Epoch 27/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8600\n",
            "Epoch 27: val_loss improved from 0.40296 to 0.39449, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4029 - accuracy: 0.8600 - val_loss: 0.3945 - val_accuracy: 0.8733\n",
            "Epoch 28/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3900 - accuracy: 0.8615\n",
            "Epoch 28: val_loss improved from 0.39449 to 0.39007, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.3907 - accuracy: 0.8600 - val_loss: 0.3901 - val_accuracy: 0.8767\n",
            "Epoch 29/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3687 - accuracy: 0.8674\n",
            "Epoch 29: val_loss improved from 0.39007 to 0.38545, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 136ms/step - loss: 0.3685 - accuracy: 0.8675 - val_loss: 0.3854 - val_accuracy: 0.8767\n",
            "Epoch 30/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3831 - accuracy: 0.8716\n",
            "Epoch 30: val_loss improved from 0.38545 to 0.38207, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3821 - accuracy: 0.8717 - val_loss: 0.3821 - val_accuracy: 0.8767\n",
            "Epoch 31/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8800\n",
            "Epoch 31: val_loss improved from 0.38207 to 0.37808, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3656 - accuracy: 0.8800 - val_loss: 0.3781 - val_accuracy: 0.8800\n",
            "Epoch 32/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3550 - accuracy: 0.8877\n",
            "Epoch 32: val_loss improved from 0.37808 to 0.37155, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3537 - accuracy: 0.8875 - val_loss: 0.3716 - val_accuracy: 0.8800\n",
            "Epoch 33/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3537 - accuracy: 0.8776\n",
            "Epoch 33: val_loss improved from 0.37155 to 0.36974, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.3566 - accuracy: 0.8758 - val_loss: 0.3697 - val_accuracy: 0.8767\n",
            "Epoch 34/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8623\n",
            "Epoch 34: val_loss improved from 0.36974 to 0.36643, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.3602 - accuracy: 0.8617 - val_loss: 0.3664 - val_accuracy: 0.8800\n",
            "Epoch 35/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.8877\n",
            "Epoch 35: val_loss improved from 0.36643 to 0.36204, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.3292 - accuracy: 0.8867 - val_loss: 0.3620 - val_accuracy: 0.8833\n",
            "Epoch 36/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.8750\n",
            "Epoch 36: val_loss improved from 0.36204 to 0.35599, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.3415 - accuracy: 0.8750 - val_loss: 0.3560 - val_accuracy: 0.8833\n",
            "Epoch 37/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8842\n",
            "Epoch 37: val_loss improved from 0.35599 to 0.35395, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.3289 - accuracy: 0.8842 - val_loss: 0.3540 - val_accuracy: 0.8800\n",
            "Epoch 38/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3393 - accuracy: 0.8784\n",
            "Epoch 38: val_loss improved from 0.35395 to 0.34796, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.3377 - accuracy: 0.8792 - val_loss: 0.3480 - val_accuracy: 0.8833\n",
            "Epoch 39/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8817\n",
            "Epoch 39: val_loss improved from 0.34796 to 0.34320, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.3208 - accuracy: 0.8817 - val_loss: 0.3432 - val_accuracy: 0.8833\n",
            "Epoch 40/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8808\n",
            "Epoch 40: val_loss improved from 0.34320 to 0.33948, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 128ms/step - loss: 0.3345 - accuracy: 0.8808 - val_loss: 0.3395 - val_accuracy: 0.8833\n",
            "Epoch 41/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3081 - accuracy: 0.8894\n",
            "Epoch 41: val_loss improved from 0.33948 to 0.33654, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.3106 - accuracy: 0.8883 - val_loss: 0.3365 - val_accuracy: 0.8833\n",
            "Epoch 42/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3138 - accuracy: 0.8919\n",
            "Epoch 42: val_loss improved from 0.33654 to 0.32821, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.3138 - accuracy: 0.8917 - val_loss: 0.3282 - val_accuracy: 0.8833\n",
            "Epoch 43/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.8950\n",
            "Epoch 43: val_loss improved from 0.32821 to 0.32637, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.2918 - accuracy: 0.8950 - val_loss: 0.3264 - val_accuracy: 0.8833\n",
            "Epoch 44/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8851\n",
            "Epoch 44: val_loss improved from 0.32637 to 0.32576, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.3159 - accuracy: 0.8858 - val_loss: 0.3258 - val_accuracy: 0.8833\n",
            "Epoch 45/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.8834\n",
            "Epoch 45: val_loss improved from 0.32576 to 0.32423, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.3045 - accuracy: 0.8842 - val_loss: 0.3242 - val_accuracy: 0.8867\n",
            "Epoch 46/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3098 - accuracy: 0.8941\n",
            "Epoch 46: val_loss improved from 0.32423 to 0.31963, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3091 - accuracy: 0.8917 - val_loss: 0.3196 - val_accuracy: 0.8833\n",
            "Epoch 47/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3049 - accuracy: 0.8941\n",
            "Epoch 47: val_loss improved from 0.31963 to 0.31763, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.3053 - accuracy: 0.8925 - val_loss: 0.3176 - val_accuracy: 0.8867\n",
            "Epoch 48/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8961\n",
            "Epoch 48: val_loss improved from 0.31763 to 0.31524, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 144ms/step - loss: 0.2888 - accuracy: 0.8975 - val_loss: 0.3152 - val_accuracy: 0.8900\n",
            "Epoch 49/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2697 - accuracy: 0.8986\n",
            "Epoch 49: val_loss did not improve from 0.31524\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.2678 - accuracy: 0.8992 - val_loss: 0.3162 - val_accuracy: 0.8833\n",
            "Epoch 50/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8925\n",
            "Epoch 50: val_loss improved from 0.31524 to 0.31293, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 9s 238ms/step - loss: 0.2913 - accuracy: 0.8925 - val_loss: 0.3129 - val_accuracy: 0.8833\n",
            "Epoch 51/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2870 - accuracy: 0.8993\n",
            "Epoch 51: val_loss improved from 0.31293 to 0.30837, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.2827 - accuracy: 0.8992 - val_loss: 0.3084 - val_accuracy: 0.8867\n",
            "Epoch 52/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9240\n",
            "Epoch 52: val_loss improved from 0.30837 to 0.30644, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2475 - accuracy: 0.9233 - val_loss: 0.3064 - val_accuracy: 0.8867\n",
            "Epoch 53/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9020\n",
            "Epoch 53: val_loss improved from 0.30644 to 0.30359, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2926 - accuracy: 0.9008 - val_loss: 0.3036 - val_accuracy: 0.8967\n",
            "Epoch 54/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9108\n",
            "Epoch 54: val_loss improved from 0.30359 to 0.30252, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 119ms/step - loss: 0.2592 - accuracy: 0.9108 - val_loss: 0.3025 - val_accuracy: 0.8967\n",
            "Epoch 55/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9142\n",
            "Epoch 55: val_loss improved from 0.30252 to 0.30081, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2460 - accuracy: 0.9142 - val_loss: 0.3008 - val_accuracy: 0.8867\n",
            "Epoch 56/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9088\n",
            "Epoch 56: val_loss improved from 0.30081 to 0.29561, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2642 - accuracy: 0.9083 - val_loss: 0.2956 - val_accuracy: 0.8967\n",
            "Epoch 57/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2733 - accuracy: 0.8936\n",
            "Epoch 57: val_loss did not improve from 0.29561\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2720 - accuracy: 0.8942 - val_loss: 0.2966 - val_accuracy: 0.8967\n",
            "Epoch 58/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2607 - accuracy: 0.9080\n",
            "Epoch 58: val_loss improved from 0.29561 to 0.29499, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.2618 - accuracy: 0.9058 - val_loss: 0.2950 - val_accuracy: 0.8967\n",
            "Epoch 59/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.8978\n",
            "Epoch 59: val_loss improved from 0.29499 to 0.29125, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2631 - accuracy: 0.8975 - val_loss: 0.2913 - val_accuracy: 0.8967\n",
            "Epoch 60/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9092\n",
            "Epoch 60: val_loss did not improve from 0.29125\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2597 - accuracy: 0.9092 - val_loss: 0.2923 - val_accuracy: 0.8967\n",
            "Epoch 61/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9092\n",
            "Epoch 61: val_loss did not improve from 0.29125\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2396 - accuracy: 0.9092 - val_loss: 0.2915 - val_accuracy: 0.8933\n",
            "Epoch 62/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9130\n",
            "Epoch 62: val_loss improved from 0.29125 to 0.28562, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.2445 - accuracy: 0.9125 - val_loss: 0.2856 - val_accuracy: 0.9000\n",
            "Epoch 63/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9096\n",
            "Epoch 63: val_loss improved from 0.28562 to 0.28323, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 119ms/step - loss: 0.2210 - accuracy: 0.9100 - val_loss: 0.2832 - val_accuracy: 0.9033\n",
            "Epoch 64/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2291 - accuracy: 0.9227\n",
            "Epoch 64: val_loss improved from 0.28323 to 0.28064, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2273 - accuracy: 0.9225 - val_loss: 0.2806 - val_accuracy: 0.9000\n",
            "Epoch 65/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9122\n",
            "Epoch 65: val_loss improved from 0.28064 to 0.27789, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2519 - accuracy: 0.9125 - val_loss: 0.2779 - val_accuracy: 0.9067\n",
            "Epoch 66/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9189\n",
            "Epoch 66: val_loss did not improve from 0.27789\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.2326 - accuracy: 0.9183 - val_loss: 0.2802 - val_accuracy: 0.9000\n",
            "Epoch 67/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2159 - accuracy: 0.9262\n",
            "Epoch 67: val_loss did not improve from 0.27789\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.2243 - accuracy: 0.9233 - val_loss: 0.2784 - val_accuracy: 0.8967\n",
            "Epoch 68/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2359 - accuracy: 0.9175\n",
            "Epoch 68: val_loss improved from 0.27789 to 0.27601, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 134ms/step - loss: 0.2351 - accuracy: 0.9183 - val_loss: 0.2760 - val_accuracy: 0.9033\n",
            "Epoch 69/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9167\n",
            "Epoch 69: val_loss improved from 0.27601 to 0.27156, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2253 - accuracy: 0.9167 - val_loss: 0.2716 - val_accuracy: 0.9033\n",
            "Epoch 70/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2170 - accuracy: 0.9236\n",
            "Epoch 70: val_loss did not improve from 0.27156\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2189 - accuracy: 0.9233 - val_loss: 0.2734 - val_accuracy: 0.9033\n",
            "Epoch 71/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9130\n",
            "Epoch 71: val_loss did not improve from 0.27156\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.2433 - accuracy: 0.9133 - val_loss: 0.2733 - val_accuracy: 0.9033\n",
            "Epoch 72/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9139\n",
            "Epoch 72: val_loss improved from 0.27156 to 0.27004, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.2296 - accuracy: 0.9150 - val_loss: 0.2700 - val_accuracy: 0.9033\n",
            "Epoch 73/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9248\n",
            "Epoch 73: val_loss did not improve from 0.27004\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2075 - accuracy: 0.9258 - val_loss: 0.2703 - val_accuracy: 0.9033\n",
            "Epoch 74/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2001 - accuracy: 0.9288\n",
            "Epoch 74: val_loss improved from 0.27004 to 0.26812, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2009 - accuracy: 0.9292 - val_loss: 0.2681 - val_accuracy: 0.9067\n",
            "Epoch 75/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.9332\n",
            "Epoch 75: val_loss improved from 0.26812 to 0.26435, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.1880 - accuracy: 0.9325 - val_loss: 0.2643 - val_accuracy: 0.9067\n",
            "Epoch 76/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9299\n",
            "Epoch 76: val_loss improved from 0.26435 to 0.26424, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.2055 - accuracy: 0.9275 - val_loss: 0.2642 - val_accuracy: 0.9033\n",
            "Epoch 77/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9367\n",
            "Epoch 77: val_loss improved from 0.26424 to 0.26320, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.1875 - accuracy: 0.9367 - val_loss: 0.2632 - val_accuracy: 0.9100\n",
            "Epoch 78/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9240\n",
            "Epoch 78: val_loss improved from 0.26320 to 0.25899, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2016 - accuracy: 0.9250 - val_loss: 0.2590 - val_accuracy: 0.9133\n",
            "Epoch 79/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2115 - accuracy: 0.9297\n",
            "Epoch 79: val_loss improved from 0.25899 to 0.25803, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.2118 - accuracy: 0.9292 - val_loss: 0.2580 - val_accuracy: 0.9067\n",
            "Epoch 80/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9367\n",
            "Epoch 80: val_loss improved from 0.25803 to 0.25663, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.1964 - accuracy: 0.9375 - val_loss: 0.2566 - val_accuracy: 0.9133\n",
            "Epoch 81/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9299\n",
            "Epoch 81: val_loss did not improve from 0.25663\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2034 - accuracy: 0.9300 - val_loss: 0.2573 - val_accuracy: 0.9100\n",
            "Epoch 82/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9358\n",
            "Epoch 82: val_loss did not improve from 0.25663\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1804 - accuracy: 0.9358 - val_loss: 0.2585 - val_accuracy: 0.9100\n",
            "Epoch 83/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9425\n",
            "Epoch 83: val_loss improved from 0.25663 to 0.25647, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1767 - accuracy: 0.9425 - val_loss: 0.2565 - val_accuracy: 0.9100\n",
            "Epoch 84/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9358\n",
            "Epoch 84: val_loss improved from 0.25647 to 0.25456, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1979 - accuracy: 0.9367 - val_loss: 0.2546 - val_accuracy: 0.9167\n",
            "Epoch 85/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9400\n",
            "Epoch 85: val_loss did not improve from 0.25456\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1821 - accuracy: 0.9383 - val_loss: 0.2574 - val_accuracy: 0.9100\n",
            "Epoch 86/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9350\n",
            "Epoch 86: val_loss improved from 0.25456 to 0.25148, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1874 - accuracy: 0.9350 - val_loss: 0.2515 - val_accuracy: 0.9133\n",
            "Epoch 87/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9300\n",
            "Epoch 87: val_loss improved from 0.25148 to 0.25139, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1800 - accuracy: 0.9300 - val_loss: 0.2514 - val_accuracy: 0.9167\n",
            "Epoch 88/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9350\n",
            "Epoch 88: val_loss did not improve from 0.25139\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1953 - accuracy: 0.9350 - val_loss: 0.2528 - val_accuracy: 0.9167\n",
            "Epoch 89/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9417\n",
            "Epoch 89: val_loss did not improve from 0.25139\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1819 - accuracy: 0.9417 - val_loss: 0.2533 - val_accuracy: 0.9167\n",
            "Epoch 90/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9417\n",
            "Epoch 90: val_loss improved from 0.25139 to 0.25035, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 123ms/step - loss: 0.1768 - accuracy: 0.9425 - val_loss: 0.2504 - val_accuracy: 0.9133\n",
            "Epoch 91/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1783 - accuracy: 0.9367\n",
            "Epoch 91: val_loss did not improve from 0.25035\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1764 - accuracy: 0.9375 - val_loss: 0.2510 - val_accuracy: 0.9100\n",
            "Epoch 92/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9333\n",
            "Epoch 92: val_loss improved from 0.25035 to 0.24723, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1789 - accuracy: 0.9333 - val_loss: 0.2472 - val_accuracy: 0.9167\n",
            "Epoch 93/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9299\n",
            "Epoch 93: val_loss did not improve from 0.24723\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1863 - accuracy: 0.9292 - val_loss: 0.2482 - val_accuracy: 0.9133\n",
            "Epoch 94/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9383\n",
            "Epoch 94: val_loss improved from 0.24723 to 0.24608, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 130ms/step - loss: 0.1639 - accuracy: 0.9392 - val_loss: 0.2461 - val_accuracy: 0.9133\n",
            "Epoch 95/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9324\n",
            "Epoch 95: val_loss did not improve from 0.24608\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1756 - accuracy: 0.9325 - val_loss: 0.2465 - val_accuracy: 0.9133\n",
            "Epoch 96/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9417\n",
            "Epoch 96: val_loss improved from 0.24608 to 0.23901, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.1657 - accuracy: 0.9417 - val_loss: 0.2390 - val_accuracy: 0.9167\n",
            "Epoch 97/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9267\n",
            "Epoch 97: val_loss did not improve from 0.23901\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1816 - accuracy: 0.9267 - val_loss: 0.2419 - val_accuracy: 0.9200\n",
            "Epoch 98/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9358\n",
            "Epoch 98: val_loss did not improve from 0.23901\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1646 - accuracy: 0.9358 - val_loss: 0.2433 - val_accuracy: 0.9167\n",
            "Epoch 99/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9400\n",
            "Epoch 99: val_loss did not improve from 0.23901\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.1669 - accuracy: 0.9400 - val_loss: 0.2399 - val_accuracy: 0.9133\n",
            "Epoch 100/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9400\n",
            "Epoch 100: val_loss improved from 0.23901 to 0.23879, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 143ms/step - loss: 0.1621 - accuracy: 0.9408 - val_loss: 0.2388 - val_accuracy: 0.9167\n",
            "Epoch 101/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9400\n",
            "Epoch 101: val_loss improved from 0.23879 to 0.23636, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.1586 - accuracy: 0.9408 - val_loss: 0.2364 - val_accuracy: 0.9167\n",
            "Epoch 102/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9341\n",
            "Epoch 102: val_loss improved from 0.23636 to 0.23218, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 131ms/step - loss: 0.1784 - accuracy: 0.9333 - val_loss: 0.2322 - val_accuracy: 0.9200\n",
            "Epoch 103/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9417\n",
            "Epoch 103: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1601 - accuracy: 0.9417 - val_loss: 0.2350 - val_accuracy: 0.9167\n",
            "Epoch 104/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9392\n",
            "Epoch 104: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.1724 - accuracy: 0.9400 - val_loss: 0.2366 - val_accuracy: 0.9200\n",
            "Epoch 105/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1508 - accuracy: 0.9453\n",
            "Epoch 105: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1584 - accuracy: 0.9442 - val_loss: 0.2366 - val_accuracy: 0.9200\n",
            "Epoch 106/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1432 - accuracy: 0.9497\n",
            "Epoch 106: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.1396 - accuracy: 0.9517 - val_loss: 0.2348 - val_accuracy: 0.9200\n",
            "Epoch 107/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.9468\n",
            "Epoch 107: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 3s 75ms/step - loss: 0.1418 - accuracy: 0.9467 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
            "Epoch 108/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9483\n",
            "Epoch 108: val_loss did not improve from 0.23218\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1503 - accuracy: 0.9483 - val_loss: 0.2364 - val_accuracy: 0.9167\n",
            "Epoch 109/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1350 - accuracy: 0.9575\n",
            "Epoch 109: val_loss improved from 0.23218 to 0.23036, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 151ms/step - loss: 0.1382 - accuracy: 0.9567 - val_loss: 0.2304 - val_accuracy: 0.9167\n",
            "Epoch 110/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1670 - accuracy: 0.9426\n",
            "Epoch 110: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.2362 - val_accuracy: 0.9200\n",
            "Epoch 111/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9400\n",
            "Epoch 111: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1674 - accuracy: 0.9400 - val_loss: 0.2367 - val_accuracy: 0.9133\n",
            "Epoch 112/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9417\n",
            "Epoch 112: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1581 - accuracy: 0.9417 - val_loss: 0.2342 - val_accuracy: 0.9133\n",
            "Epoch 113/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1444 - accuracy: 0.9459\n",
            "Epoch 113: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.1443 - accuracy: 0.9458 - val_loss: 0.2315 - val_accuracy: 0.9200\n",
            "Epoch 114/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9485\n",
            "Epoch 114: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.1507 - accuracy: 0.9483 - val_loss: 0.2325 - val_accuracy: 0.9200\n",
            "Epoch 115/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9467\n",
            "Epoch 115: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1377 - accuracy: 0.9467 - val_loss: 0.2374 - val_accuracy: 0.9200\n",
            "Epoch 116/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1490 - accuracy: 0.9527\n",
            "Epoch 116: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.1476 - accuracy: 0.9533 - val_loss: 0.2349 - val_accuracy: 0.9200\n",
            "Epoch 117/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9409\n",
            "Epoch 117: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1410 - accuracy: 0.9408 - val_loss: 0.2325 - val_accuracy: 0.9200\n",
            "Epoch 118/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9475\n",
            "Epoch 118: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1336 - accuracy: 0.9475 - val_loss: 0.2313 - val_accuracy: 0.9200\n",
            "Epoch 119/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9558\n",
            "Epoch 119: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1224 - accuracy: 0.9558 - val_loss: 0.2307 - val_accuracy: 0.9200\n",
            "Epoch 120/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9467\n",
            "Epoch 120: val_loss did not improve from 0.23036\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1573 - accuracy: 0.9467 - val_loss: 0.2326 - val_accuracy: 0.9200\n",
            "Epoch 121/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9558\n",
            "Epoch 121: val_loss improved from 0.23036 to 0.22992, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 162ms/step - loss: 0.1244 - accuracy: 0.9558 - val_loss: 0.2299 - val_accuracy: 0.9200\n",
            "Epoch 122/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9586\n",
            "Epoch 122: val_loss did not improve from 0.22992\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1280 - accuracy: 0.9592 - val_loss: 0.2311 - val_accuracy: 0.9200\n",
            "Epoch 123/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9552\n",
            "Epoch 123: val_loss did not improve from 0.22992\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1202 - accuracy: 0.9558 - val_loss: 0.2336 - val_accuracy: 0.9200\n",
            "Epoch 124/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9467\n",
            "Epoch 124: val_loss improved from 0.22992 to 0.22947, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.1402 - accuracy: 0.9467 - val_loss: 0.2295 - val_accuracy: 0.9267\n",
            "Epoch 125/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9459\n",
            "Epoch 125: val_loss improved from 0.22947 to 0.22548, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 137ms/step - loss: 0.1521 - accuracy: 0.9458 - val_loss: 0.2255 - val_accuracy: 0.9233\n",
            "Epoch 126/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9443\n",
            "Epoch 126: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.1504 - accuracy: 0.9433 - val_loss: 0.2257 - val_accuracy: 0.9233\n",
            "Epoch 127/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9527\n",
            "Epoch 127: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1292 - accuracy: 0.9533 - val_loss: 0.2306 - val_accuracy: 0.9233\n",
            "Epoch 128/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9519\n",
            "Epoch 128: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1204 - accuracy: 0.9525 - val_loss: 0.2278 - val_accuracy: 0.9233\n",
            "Epoch 129/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9633\n",
            "Epoch 129: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1235 - accuracy: 0.9633 - val_loss: 0.2289 - val_accuracy: 0.9267\n",
            "Epoch 130/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9558\n",
            "Epoch 130: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1253 - accuracy: 0.9558 - val_loss: 0.2290 - val_accuracy: 0.9233\n",
            "Epoch 131/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9637\n",
            "Epoch 131: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.1083 - accuracy: 0.9642 - val_loss: 0.2328 - val_accuracy: 0.9200\n",
            "Epoch 132/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9400\n",
            "Epoch 132: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1416 - accuracy: 0.9400 - val_loss: 0.2320 - val_accuracy: 0.9200\n",
            "Epoch 133/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9525\n",
            "Epoch 133: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1366 - accuracy: 0.9525 - val_loss: 0.2277 - val_accuracy: 0.9200\n",
            "Epoch 134/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9575\n",
            "Epoch 134: val_loss did not improve from 0.22548\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1209 - accuracy: 0.9575 - val_loss: 0.2257 - val_accuracy: 0.9200\n",
            "Epoch 135/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9625\n",
            "Epoch 135: val_loss improved from 0.22548 to 0.22347, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.1201 - accuracy: 0.9625 - val_loss: 0.2235 - val_accuracy: 0.9267\n",
            "Epoch 136/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9525\n",
            "Epoch 136: val_loss improved from 0.22347 to 0.22267, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 119ms/step - loss: 0.1392 - accuracy: 0.9525 - val_loss: 0.2227 - val_accuracy: 0.9200\n",
            "Epoch 137/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9514\n",
            "Epoch 137: val_loss did not improve from 0.22267\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1282 - accuracy: 0.9525 - val_loss: 0.2259 - val_accuracy: 0.9200\n",
            "Epoch 138/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9578\n",
            "Epoch 138: val_loss improved from 0.22267 to 0.22218, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.1159 - accuracy: 0.9583 - val_loss: 0.2222 - val_accuracy: 0.9267\n",
            "Epoch 139/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9535\n",
            "Epoch 139: val_loss improved from 0.22218 to 0.22032, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.1256 - accuracy: 0.9542 - val_loss: 0.2203 - val_accuracy: 0.9200\n",
            "Epoch 140/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9611\n",
            "Epoch 140: val_loss did not improve from 0.22032\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1204 - accuracy: 0.9608 - val_loss: 0.2225 - val_accuracy: 0.9233\n",
            "Epoch 141/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9442\n",
            "Epoch 141: val_loss did not improve from 0.22032\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.1337 - accuracy: 0.9442 - val_loss: 0.2232 - val_accuracy: 0.9267\n",
            "Epoch 142/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9586\n",
            "Epoch 142: val_loss improved from 0.22032 to 0.21934, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.1225 - accuracy: 0.9592 - val_loss: 0.2193 - val_accuracy: 0.9233\n",
            "Epoch 143/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9608\n",
            "Epoch 143: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1136 - accuracy: 0.9608 - val_loss: 0.2219 - val_accuracy: 0.9233\n",
            "Epoch 144/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9595\n",
            "Epoch 144: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1091 - accuracy: 0.9583 - val_loss: 0.2198 - val_accuracy: 0.9200\n",
            "Epoch 145/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9620\n",
            "Epoch 145: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.2218 - val_accuracy: 0.9233\n",
            "Epoch 146/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1068 - accuracy: 0.9592\n",
            "Epoch 146: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1072 - accuracy: 0.9583 - val_loss: 0.2204 - val_accuracy: 0.9200\n",
            "Epoch 147/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9633\n",
            "Epoch 147: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.0926 - accuracy: 0.9633 - val_loss: 0.2208 - val_accuracy: 0.9233\n",
            "Epoch 148/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9611\n",
            "Epoch 148: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.2205 - val_accuracy: 0.9300\n",
            "Epoch 149/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9654\n",
            "Epoch 149: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1017 - accuracy: 0.9650 - val_loss: 0.2225 - val_accuracy: 0.9300\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9535\n",
            "Epoch 150: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1084 - accuracy: 0.9533 - val_loss: 0.2211 - val_accuracy: 0.9233\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9628\n",
            "Epoch 151: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1024 - accuracy: 0.9625 - val_loss: 0.2223 - val_accuracy: 0.9233\n",
            "Epoch 152/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9628\n",
            "Epoch 152: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1073 - accuracy: 0.9633 - val_loss: 0.2208 - val_accuracy: 0.9233\n",
            "Epoch 153/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9692\n",
            "Epoch 153: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0975 - accuracy: 0.9692 - val_loss: 0.2214 - val_accuracy: 0.9267\n",
            "Epoch 154/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9618\n",
            "Epoch 154: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.2246 - val_accuracy: 0.9233\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9688\n",
            "Epoch 155: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.0822 - accuracy: 0.9692 - val_loss: 0.2240 - val_accuracy: 0.9267\n",
            "Epoch 156/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9654\n",
            "Epoch 156: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0986 - accuracy: 0.9658 - val_loss: 0.2238 - val_accuracy: 0.9267\n",
            "Epoch 157/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9608\n",
            "Epoch 157: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1024 - accuracy: 0.9608 - val_loss: 0.2210 - val_accuracy: 0.9267\n",
            "Epoch 158/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9667\n",
            "Epoch 158: val_loss did not improve from 0.21934\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1034 - accuracy: 0.9667 - val_loss: 0.2195 - val_accuracy: 0.9233\n",
            "Epoch 159/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9592\n",
            "Epoch 159: val_loss improved from 0.21934 to 0.21809, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 123ms/step - loss: 0.1156 - accuracy: 0.9592 - val_loss: 0.2181 - val_accuracy: 0.9333\n",
            "Epoch 160/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9620\n",
            "Epoch 160: val_loss did not improve from 0.21809\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0940 - accuracy: 0.9625 - val_loss: 0.2203 - val_accuracy: 0.9300\n",
            "Epoch 161/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9617\n",
            "Epoch 161: val_loss did not improve from 0.21809\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0972 - accuracy: 0.9617 - val_loss: 0.2195 - val_accuracy: 0.9233\n",
            "Epoch 162/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9637\n",
            "Epoch 162: val_loss improved from 0.21809 to 0.21720, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.2172 - val_accuracy: 0.9233\n",
            "Epoch 163/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9642\n",
            "Epoch 163: val_loss did not improve from 0.21720\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0936 - accuracy: 0.9642 - val_loss: 0.2220 - val_accuracy: 0.9233\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9662\n",
            "Epoch 164: val_loss did not improve from 0.21720\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0995 - accuracy: 0.9658 - val_loss: 0.2195 - val_accuracy: 0.9233\n",
            "Epoch 165/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0950 - accuracy: 0.9644\n",
            "Epoch 165: val_loss improved from 0.21720 to 0.21353, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.0936 - accuracy: 0.9650 - val_loss: 0.2135 - val_accuracy: 0.9267\n",
            "Epoch 166/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9603\n",
            "Epoch 166: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.1046 - accuracy: 0.9583 - val_loss: 0.2172 - val_accuracy: 0.9300\n",
            "Epoch 167/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9688\n",
            "Epoch 167: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.0924 - accuracy: 0.9692 - val_loss: 0.2161 - val_accuracy: 0.9233\n",
            "Epoch 168/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9658\n",
            "Epoch 168: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.0964 - accuracy: 0.9658 - val_loss: 0.2174 - val_accuracy: 0.9233\n",
            "Epoch 169/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9764\n",
            "Epoch 169: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 3s 70ms/step - loss: 0.0763 - accuracy: 0.9767 - val_loss: 0.2181 - val_accuracy: 0.9300\n",
            "Epoch 170/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0917 - accuracy: 0.9688\n",
            "Epoch 170: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0915 - accuracy: 0.9683 - val_loss: 0.2145 - val_accuracy: 0.9267\n",
            "Epoch 171/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9700\n",
            "Epoch 171: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0929 - accuracy: 0.9700 - val_loss: 0.2157 - val_accuracy: 0.9267\n",
            "Epoch 172/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9742\n",
            "Epoch 172: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0836 - accuracy: 0.9742 - val_loss: 0.2151 - val_accuracy: 0.9233\n",
            "Epoch 173/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9692\n",
            "Epoch 173: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0951 - accuracy: 0.9692 - val_loss: 0.2159 - val_accuracy: 0.9300\n",
            "Epoch 174/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9692\n",
            "Epoch 174: val_loss did not improve from 0.21353\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.0893 - accuracy: 0.9692 - val_loss: 0.2150 - val_accuracy: 0.9267\n",
            "Epoch 175/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9642\n",
            "Epoch 175: val_loss improved from 0.21353 to 0.21308, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 141ms/step - loss: 0.0850 - accuracy: 0.9642 - val_loss: 0.2131 - val_accuracy: 0.9267\n",
            "Epoch 176/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9654\n",
            "Epoch 176: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1008 - accuracy: 0.9650 - val_loss: 0.2198 - val_accuracy: 0.9233\n",
            "Epoch 177/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9688\n",
            "Epoch 177: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0742 - accuracy: 0.9692 - val_loss: 0.2188 - val_accuracy: 0.9267\n",
            "Epoch 178/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9667\n",
            "Epoch 178: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0801 - accuracy: 0.9667 - val_loss: 0.2184 - val_accuracy: 0.9267\n",
            "Epoch 179/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9721\n",
            "Epoch 179: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 57ms/step - loss: 0.0819 - accuracy: 0.9725 - val_loss: 0.2165 - val_accuracy: 0.9233\n",
            "Epoch 180/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9637\n",
            "Epoch 180: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.0913 - accuracy: 0.9642 - val_loss: 0.2136 - val_accuracy: 0.9267\n",
            "Epoch 181/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9675\n",
            "Epoch 181: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.0933 - accuracy: 0.9675 - val_loss: 0.2182 - val_accuracy: 0.9267\n",
            "Epoch 182/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9808\n",
            "Epoch 182: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0691 - accuracy: 0.9808 - val_loss: 0.2209 - val_accuracy: 0.9200\n",
            "Epoch 183/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9772\n",
            "Epoch 183: val_loss did not improve from 0.21308\n",
            "38/38 [==============================] - 3s 74ms/step - loss: 0.0690 - accuracy: 0.9775 - val_loss: 0.2184 - val_accuracy: 0.9233\n",
            "Epoch 184/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9658\n",
            "Epoch 184: val_loss improved from 0.21308 to 0.21189, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 146ms/step - loss: 0.0830 - accuracy: 0.9658 - val_loss: 0.2119 - val_accuracy: 0.9233\n",
            "Epoch 185/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9708\n",
            "Epoch 185: val_loss did not improve from 0.21189\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.0810 - accuracy: 0.9708 - val_loss: 0.2165 - val_accuracy: 0.9300\n",
            "Epoch 186/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9758\n",
            "Epoch 186: val_loss improved from 0.21189 to 0.21153, saving model to /content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.0744 - accuracy: 0.9758 - val_loss: 0.2115 - val_accuracy: 0.9267\n",
            "Epoch 187/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9797\n",
            "Epoch 187: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0729 - accuracy: 0.9792 - val_loss: 0.2183 - val_accuracy: 0.9300\n",
            "Epoch 188/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9679\n",
            "Epoch 188: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0867 - accuracy: 0.9683 - val_loss: 0.2151 - val_accuracy: 0.9300\n",
            "Epoch 189/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9700\n",
            "Epoch 189: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0726 - accuracy: 0.9700 - val_loss: 0.2240 - val_accuracy: 0.9233\n",
            "Epoch 190/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9683\n",
            "Epoch 190: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.0898 - accuracy: 0.9683 - val_loss: 0.2178 - val_accuracy: 0.9300\n",
            "Epoch 191/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9758\n",
            "Epoch 191: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0694 - accuracy: 0.9758 - val_loss: 0.2206 - val_accuracy: 0.9267\n",
            "Epoch 192/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0723 - accuracy: 0.9714\n",
            "Epoch 192: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0698 - accuracy: 0.9725 - val_loss: 0.2186 - val_accuracy: 0.9267\n",
            "Epoch 193/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9721\n",
            "Epoch 193: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0707 - accuracy: 0.9725 - val_loss: 0.2248 - val_accuracy: 0.9300\n",
            "Epoch 194/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9747\n",
            "Epoch 194: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0699 - accuracy: 0.9750 - val_loss: 0.2177 - val_accuracy: 0.9333\n",
            "Epoch 195/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9692\n",
            "Epoch 195: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0816 - accuracy: 0.9692 - val_loss: 0.2220 - val_accuracy: 0.9267\n",
            "Epoch 196/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9780\n",
            "Epoch 196: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0635 - accuracy: 0.9775 - val_loss: 0.2195 - val_accuracy: 0.9267\n",
            "Epoch 197/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9758\n",
            "Epoch 197: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0834 - accuracy: 0.9758 - val_loss: 0.2159 - val_accuracy: 0.9300\n",
            "Epoch 198/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9733\n",
            "Epoch 198: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0688 - accuracy: 0.9733 - val_loss: 0.2147 - val_accuracy: 0.9300\n",
            "Epoch 199/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9733\n",
            "Epoch 199: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.0751 - accuracy: 0.9733 - val_loss: 0.2195 - val_accuracy: 0.9233\n",
            "Epoch 200/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0708 - accuracy: 0.9772\n",
            "Epoch 200: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.0700 - accuracy: 0.9775 - val_loss: 0.2135 - val_accuracy: 0.9267\n",
            "Epoch 201/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9772\n",
            "Epoch 201: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0618 - accuracy: 0.9775 - val_loss: 0.2161 - val_accuracy: 0.9267\n",
            "Epoch 202/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9806\n",
            "Epoch 202: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.2159 - val_accuracy: 0.9267\n",
            "Epoch 203/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9806\n",
            "Epoch 203: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0688 - accuracy: 0.9808 - val_loss: 0.2158 - val_accuracy: 0.9300\n",
            "Epoch 204/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9764\n",
            "Epoch 204: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0652 - accuracy: 0.9767 - val_loss: 0.2217 - val_accuracy: 0.9233\n",
            "Epoch 205/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9730\n",
            "Epoch 205: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0766 - accuracy: 0.9733 - val_loss: 0.2175 - val_accuracy: 0.9300\n",
            "Epoch 206/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9742\n",
            "Epoch 206: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0772 - accuracy: 0.9742 - val_loss: 0.2151 - val_accuracy: 0.9267\n",
            "Epoch 207/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9721\n",
            "Epoch 207: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0757 - accuracy: 0.9725 - val_loss: 0.2225 - val_accuracy: 0.9267\n",
            "Epoch 208/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9808\n",
            "Epoch 208: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.2227 - val_accuracy: 0.9233\n",
            "Epoch 209/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9767\n",
            "Epoch 209: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0685 - accuracy: 0.9767 - val_loss: 0.2196 - val_accuracy: 0.9333\n",
            "Epoch 210/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9667\n",
            "Epoch 210: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0823 - accuracy: 0.9667 - val_loss: 0.2147 - val_accuracy: 0.9267\n",
            "Epoch 211/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9772\n",
            "Epoch 211: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.2154 - val_accuracy: 0.9267\n",
            "Epoch 212/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9721\n",
            "Epoch 212: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0791 - accuracy: 0.9725 - val_loss: 0.2168 - val_accuracy: 0.9267\n",
            "Epoch 213/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9767\n",
            "Epoch 213: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0671 - accuracy: 0.9767 - val_loss: 0.2198 - val_accuracy: 0.9267\n",
            "Epoch 214/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9814\n",
            "Epoch 214: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0616 - accuracy: 0.9808 - val_loss: 0.2209 - val_accuracy: 0.9267\n",
            "Epoch 215/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9797\n",
            "Epoch 215: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0631 - accuracy: 0.9800 - val_loss: 0.2199 - val_accuracy: 0.9300\n",
            "Epoch 216/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0706 - accuracy: 0.9705\n",
            "Epoch 216: val_loss did not improve from 0.21153\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0684 - accuracy: 0.9717 - val_loss: 0.2213 - val_accuracy: 0.9267\n",
            "47/47 [==============================] - 1s 19ms/step - loss: 0.3990 - accuracy: 0.8887\n",
            "Test accuracy, 15 run, after finetuning: 0.8886666893959045\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.4204 - accuracy: 0.6317\n",
            "Epoch 1: val_loss improved from inf to 1.62815, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 44s 552ms/step - loss: 2.4204 - accuracy: 0.6317 - val_loss: 1.6281 - val_accuracy: 0.6833\n",
            "Epoch 2/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4615 - accuracy: 0.6926\n",
            "Epoch 2: val_loss improved from 1.62815 to 1.06062, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 1.4625 - accuracy: 0.6908 - val_loss: 1.0606 - val_accuracy: 0.7533\n",
            "Epoch 3/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 1.0747 - accuracy: 0.7325\n",
            "Epoch 3: val_loss improved from 1.06062 to 0.83761, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 126ms/step - loss: 1.0747 - accuracy: 0.7325 - val_loss: 0.8376 - val_accuracy: 0.7767\n",
            "Epoch 4/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.9011 - accuracy: 0.7533\n",
            "Epoch 4: val_loss improved from 0.83761 to 0.73529, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.9011 - accuracy: 0.7533 - val_loss: 0.7353 - val_accuracy: 0.7967\n",
            "Epoch 5/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.8251 - accuracy: 0.7686\n",
            "Epoch 5: val_loss improved from 0.73529 to 0.67079, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.8297 - accuracy: 0.7683 - val_loss: 0.6708 - val_accuracy: 0.8133\n",
            "Epoch 6/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7479 - accuracy: 0.7846\n",
            "Epoch 6: val_loss improved from 0.67079 to 0.62727, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.7486 - accuracy: 0.7858 - val_loss: 0.6273 - val_accuracy: 0.8167\n",
            "Epoch 7/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7039 - accuracy: 0.7812\n",
            "Epoch 7: val_loss improved from 0.62727 to 0.59104, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.7015 - accuracy: 0.7817 - val_loss: 0.5910 - val_accuracy: 0.8233\n",
            "Epoch 8/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.7983\n",
            "Epoch 8: val_loss improved from 0.59104 to 0.56272, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.6617 - accuracy: 0.7983 - val_loss: 0.5627 - val_accuracy: 0.8333\n",
            "Epoch 9/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6450 - accuracy: 0.8032\n",
            "Epoch 9: val_loss improved from 0.56272 to 0.54062, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.6462 - accuracy: 0.8025 - val_loss: 0.5406 - val_accuracy: 0.8333\n",
            "Epoch 10/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6471 - accuracy: 0.7933\n",
            "Epoch 10: val_loss improved from 0.54062 to 0.52054, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.6471 - accuracy: 0.7933 - val_loss: 0.5205 - val_accuracy: 0.8400\n",
            "Epoch 11/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.8067\n",
            "Epoch 11: val_loss improved from 0.52054 to 0.50421, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.5843 - accuracy: 0.8067 - val_loss: 0.5042 - val_accuracy: 0.8433\n",
            "Epoch 12/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5225 - accuracy: 0.8220\n",
            "Epoch 12: val_loss improved from 0.50421 to 0.49253, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.5320 - accuracy: 0.8208 - val_loss: 0.4925 - val_accuracy: 0.8400\n",
            "Epoch 13/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5943 - accuracy: 0.8083\n",
            "Epoch 13: val_loss improved from 0.49253 to 0.47671, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.5925 - accuracy: 0.8092 - val_loss: 0.4767 - val_accuracy: 0.8433\n",
            "Epoch 14/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5796 - accuracy: 0.8117\n",
            "Epoch 14: val_loss improved from 0.47671 to 0.46467, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.5745 - accuracy: 0.8133 - val_loss: 0.4647 - val_accuracy: 0.8433\n",
            "Epoch 15/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.8342\n",
            "Epoch 15: val_loss improved from 0.46467 to 0.45411, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4786 - accuracy: 0.8342 - val_loss: 0.4541 - val_accuracy: 0.8600\n",
            "Epoch 16/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5166 - accuracy: 0.8217\n",
            "Epoch 16: val_loss improved from 0.45411 to 0.44606, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.5166 - accuracy: 0.8217 - val_loss: 0.4461 - val_accuracy: 0.8567\n",
            "Epoch 17/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4800 - accuracy: 0.8458\n",
            "Epoch 17: val_loss improved from 0.44606 to 0.43664, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.4800 - accuracy: 0.8458 - val_loss: 0.4366 - val_accuracy: 0.8567\n",
            "Epoch 18/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4519 - accuracy: 0.8345\n",
            "Epoch 18: val_loss improved from 0.43664 to 0.42931, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4563 - accuracy: 0.8333 - val_loss: 0.4293 - val_accuracy: 0.8567\n",
            "Epoch 19/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.8285\n",
            "Epoch 19: val_loss improved from 0.42931 to 0.41967, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4794 - accuracy: 0.8292 - val_loss: 0.4197 - val_accuracy: 0.8600\n",
            "Epoch 20/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8564\n",
            "Epoch 20: val_loss improved from 0.41967 to 0.41360, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.4222 - accuracy: 0.8567 - val_loss: 0.4136 - val_accuracy: 0.8600\n",
            "Epoch 21/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4317 - accuracy: 0.8490\n",
            "Epoch 21: val_loss improved from 0.41360 to 0.41194, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.4327 - accuracy: 0.8467 - val_loss: 0.4119 - val_accuracy: 0.8600\n",
            "Epoch 22/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4397 - accuracy: 0.8568\n",
            "Epoch 22: val_loss improved from 0.41194 to 0.40820, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 163ms/step - loss: 0.4367 - accuracy: 0.8592 - val_loss: 0.4082 - val_accuracy: 0.8633\n",
            "Epoch 23/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.8617\n",
            "Epoch 23: val_loss improved from 0.40820 to 0.39856, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 8s 222ms/step - loss: 0.4130 - accuracy: 0.8617 - val_loss: 0.3986 - val_accuracy: 0.8667\n",
            "Epoch 24/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8533\n",
            "Epoch 24: val_loss improved from 0.39856 to 0.39390, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.4335 - accuracy: 0.8533 - val_loss: 0.3939 - val_accuracy: 0.8667\n",
            "Epoch 25/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4295 - accuracy: 0.8490\n",
            "Epoch 25: val_loss improved from 0.39390 to 0.38948, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.4392 - accuracy: 0.8467 - val_loss: 0.3895 - val_accuracy: 0.8667\n",
            "Epoch 26/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3831 - accuracy: 0.8681\n",
            "Epoch 26: val_loss improved from 0.38948 to 0.38239, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.3888 - accuracy: 0.8650 - val_loss: 0.3824 - val_accuracy: 0.8633\n",
            "Epoch 27/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3929 - accuracy: 0.8637\n",
            "Epoch 27: val_loss improved from 0.38239 to 0.37683, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.3918 - accuracy: 0.8625 - val_loss: 0.3768 - val_accuracy: 0.8667\n",
            "Epoch 28/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.8775\n",
            "Epoch 28: val_loss improved from 0.37683 to 0.37619, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.3483 - accuracy: 0.8775 - val_loss: 0.3762 - val_accuracy: 0.8667\n",
            "Epoch 29/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4044 - accuracy: 0.8628\n",
            "Epoch 29: val_loss improved from 0.37619 to 0.36659, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4033 - accuracy: 0.8642 - val_loss: 0.3666 - val_accuracy: 0.8733\n",
            "Epoch 30/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3577 - accuracy: 0.8759\n",
            "Epoch 30: val_loss did not improve from 0.36659\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.3567 - accuracy: 0.8775 - val_loss: 0.3669 - val_accuracy: 0.8700\n",
            "Epoch 31/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.8875\n",
            "Epoch 31: val_loss improved from 0.36659 to 0.36189, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.3571 - accuracy: 0.8875 - val_loss: 0.3619 - val_accuracy: 0.8700\n",
            "Epoch 32/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8784\n",
            "Epoch 32: val_loss improved from 0.36189 to 0.35828, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.3421 - accuracy: 0.8767 - val_loss: 0.3583 - val_accuracy: 0.8733\n",
            "Epoch 33/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8742\n",
            "Epoch 33: val_loss improved from 0.35828 to 0.35714, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.3461 - accuracy: 0.8742 - val_loss: 0.3571 - val_accuracy: 0.8733\n",
            "Epoch 34/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3580 - accuracy: 0.8750\n",
            "Epoch 34: val_loss improved from 0.35714 to 0.35346, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3625 - accuracy: 0.8733 - val_loss: 0.3535 - val_accuracy: 0.8733\n",
            "Epoch 35/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3406 - accuracy: 0.8851\n",
            "Epoch 35: val_loss improved from 0.35346 to 0.34816, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.3390 - accuracy: 0.8858 - val_loss: 0.3482 - val_accuracy: 0.8767\n",
            "Epoch 36/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8775\n",
            "Epoch 36: val_loss improved from 0.34816 to 0.34428, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.3476 - accuracy: 0.8775 - val_loss: 0.3443 - val_accuracy: 0.8767\n",
            "Epoch 37/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8833\n",
            "Epoch 37: val_loss improved from 0.34428 to 0.34062, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3270 - accuracy: 0.8833 - val_loss: 0.3406 - val_accuracy: 0.8767\n",
            "Epoch 38/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8986\n",
            "Epoch 38: val_loss improved from 0.34062 to 0.33711, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.3210 - accuracy: 0.8992 - val_loss: 0.3371 - val_accuracy: 0.8800\n",
            "Epoch 39/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3174 - accuracy: 0.8950\n",
            "Epoch 39: val_loss improved from 0.33711 to 0.33497, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.3148 - accuracy: 0.8958 - val_loss: 0.3350 - val_accuracy: 0.8800\n",
            "Epoch 40/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8894\n",
            "Epoch 40: val_loss improved from 0.33497 to 0.33363, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2969 - accuracy: 0.8892 - val_loss: 0.3336 - val_accuracy: 0.8833\n",
            "Epoch 41/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8910\n",
            "Epoch 41: val_loss did not improve from 0.33363\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.3013 - accuracy: 0.8925 - val_loss: 0.3341 - val_accuracy: 0.8800\n",
            "Epoch 42/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3143 - accuracy: 0.8908\n",
            "Epoch 42: val_loss improved from 0.33363 to 0.32700, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.3143 - accuracy: 0.8908 - val_loss: 0.3270 - val_accuracy: 0.8833\n",
            "Epoch 43/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8894\n",
            "Epoch 43: val_loss improved from 0.32700 to 0.32405, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2927 - accuracy: 0.8908 - val_loss: 0.3241 - val_accuracy: 0.8833\n",
            "Epoch 44/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8910\n",
            "Epoch 44: val_loss improved from 0.32405 to 0.32113, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2874 - accuracy: 0.8900 - val_loss: 0.3211 - val_accuracy: 0.8867\n",
            "Epoch 45/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9008\n",
            "Epoch 45: val_loss improved from 0.32113 to 0.31883, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.2767 - accuracy: 0.9008 - val_loss: 0.3188 - val_accuracy: 0.8833\n",
            "Epoch 46/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8902\n",
            "Epoch 46: val_loss did not improve from 0.31883\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.2992 - accuracy: 0.8900 - val_loss: 0.3189 - val_accuracy: 0.8833\n",
            "Epoch 47/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.8877\n",
            "Epoch 47: val_loss improved from 0.31883 to 0.31522, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.3138 - accuracy: 0.8875 - val_loss: 0.3152 - val_accuracy: 0.8833\n",
            "Epoch 48/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.9037\n",
            "Epoch 48: val_loss improved from 0.31522 to 0.31300, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2729 - accuracy: 0.9042 - val_loss: 0.3130 - val_accuracy: 0.8867\n",
            "Epoch 49/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2977 - accuracy: 0.8978\n",
            "Epoch 49: val_loss improved from 0.31300 to 0.31014, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2969 - accuracy: 0.8983 - val_loss: 0.3101 - val_accuracy: 0.8867\n",
            "Epoch 50/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8975\n",
            "Epoch 50: val_loss improved from 0.31014 to 0.30886, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.2938 - accuracy: 0.8975 - val_loss: 0.3089 - val_accuracy: 0.8833\n",
            "Epoch 51/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9133\n",
            "Epoch 51: val_loss improved from 0.30886 to 0.30559, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2586 - accuracy: 0.9133 - val_loss: 0.3056 - val_accuracy: 0.8833\n",
            "Epoch 52/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9130\n",
            "Epoch 52: val_loss improved from 0.30559 to 0.30347, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2534 - accuracy: 0.9117 - val_loss: 0.3035 - val_accuracy: 0.8867\n",
            "Epoch 53/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.9088\n",
            "Epoch 53: val_loss improved from 0.30347 to 0.30090, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2710 - accuracy: 0.9100 - val_loss: 0.3009 - val_accuracy: 0.8833\n",
            "Epoch 54/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2405 - accuracy: 0.9089\n",
            "Epoch 54: val_loss improved from 0.30090 to 0.29939, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.2547 - accuracy: 0.9050 - val_loss: 0.2994 - val_accuracy: 0.8867\n",
            "Epoch 55/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9158\n",
            "Epoch 55: val_loss improved from 0.29939 to 0.29830, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2471 - accuracy: 0.9158 - val_loss: 0.2983 - val_accuracy: 0.8867\n",
            "Epoch 56/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.9012\n",
            "Epoch 56: val_loss improved from 0.29830 to 0.29629, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2629 - accuracy: 0.9025 - val_loss: 0.2963 - val_accuracy: 0.8867\n",
            "Epoch 57/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2396 - accuracy: 0.9158\n",
            "Epoch 57: val_loss improved from 0.29629 to 0.29543, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.2389 - accuracy: 0.9167 - val_loss: 0.2954 - val_accuracy: 0.8867\n",
            "Epoch 58/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9113\n",
            "Epoch 58: val_loss improved from 0.29543 to 0.29307, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.2576 - accuracy: 0.9100 - val_loss: 0.2931 - val_accuracy: 0.8933\n",
            "Epoch 59/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9231\n",
            "Epoch 59: val_loss improved from 0.29307 to 0.29193, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2203 - accuracy: 0.9225 - val_loss: 0.2919 - val_accuracy: 0.8900\n",
            "Epoch 60/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2249 - accuracy: 0.9210\n",
            "Epoch 60: val_loss improved from 0.29193 to 0.29109, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2307 - accuracy: 0.9200 - val_loss: 0.2911 - val_accuracy: 0.8867\n",
            "Epoch 61/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.9208\n",
            "Epoch 61: val_loss improved from 0.29109 to 0.28827, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.2348 - accuracy: 0.9208 - val_loss: 0.2883 - val_accuracy: 0.8933\n",
            "Epoch 62/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9172\n",
            "Epoch 62: val_loss did not improve from 0.28827\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2469 - accuracy: 0.9175 - val_loss: 0.2901 - val_accuracy: 0.8867\n",
            "Epoch 63/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2393 - accuracy: 0.9158\n",
            "Epoch 63: val_loss did not improve from 0.28827\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2393 - accuracy: 0.9158 - val_loss: 0.2903 - val_accuracy: 0.8867\n",
            "Epoch 64/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2366 - accuracy: 0.9115\n",
            "Epoch 64: val_loss improved from 0.28827 to 0.28680, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.2375 - accuracy: 0.9117 - val_loss: 0.2868 - val_accuracy: 0.8900\n",
            "Epoch 65/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9133\n",
            "Epoch 65: val_loss improved from 0.28680 to 0.28569, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2267 - accuracy: 0.9133 - val_loss: 0.2857 - val_accuracy: 0.8933\n",
            "Epoch 66/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2152 - accuracy: 0.9206\n",
            "Epoch 66: val_loss improved from 0.28569 to 0.28224, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.2141 - accuracy: 0.9208 - val_loss: 0.2822 - val_accuracy: 0.9000\n",
            "Epoch 67/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9231\n",
            "Epoch 67: val_loss did not improve from 0.28224\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2127 - accuracy: 0.9242 - val_loss: 0.2874 - val_accuracy: 0.8900\n",
            "Epoch 68/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9183\n",
            "Epoch 68: val_loss did not improve from 0.28224\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2310 - accuracy: 0.9183 - val_loss: 0.2854 - val_accuracy: 0.8867\n",
            "Epoch 69/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2074 - accuracy: 0.9288\n",
            "Epoch 69: val_loss did not improve from 0.28224\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2113 - accuracy: 0.9283 - val_loss: 0.2823 - val_accuracy: 0.8933\n",
            "Epoch 70/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2098 - accuracy: 0.9193\n",
            "Epoch 70: val_loss did not improve from 0.28224\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2110 - accuracy: 0.9192 - val_loss: 0.2824 - val_accuracy: 0.8967\n",
            "Epoch 71/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2074 - accuracy: 0.9192\n",
            "Epoch 71: val_loss improved from 0.28224 to 0.28034, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2074 - accuracy: 0.9192 - val_loss: 0.2803 - val_accuracy: 0.9000\n",
            "Epoch 72/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9181\n",
            "Epoch 72: val_loss improved from 0.28034 to 0.27999, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.2116 - accuracy: 0.9167 - val_loss: 0.2800 - val_accuracy: 0.8933\n",
            "Epoch 73/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2115 - accuracy: 0.9306\n",
            "Epoch 73: val_loss improved from 0.27999 to 0.27822, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2140 - accuracy: 0.9292 - val_loss: 0.2782 - val_accuracy: 0.8933\n",
            "Epoch 74/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9292\n",
            "Epoch 74: val_loss improved from 0.27822 to 0.27670, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2017 - accuracy: 0.9292 - val_loss: 0.2767 - val_accuracy: 0.8967\n",
            "Epoch 75/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2222 - accuracy: 0.9240\n",
            "Epoch 75: val_loss did not improve from 0.27670\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2229 - accuracy: 0.9242 - val_loss: 0.2775 - val_accuracy: 0.8933\n",
            "Epoch 76/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9217\n",
            "Epoch 76: val_loss improved from 0.27670 to 0.27663, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1965 - accuracy: 0.9217 - val_loss: 0.2766 - val_accuracy: 0.8967\n",
            "Epoch 77/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.9358\n",
            "Epoch 77: val_loss improved from 0.27663 to 0.27433, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.1960 - accuracy: 0.9358 - val_loss: 0.2743 - val_accuracy: 0.9033\n",
            "Epoch 78/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1935 - accuracy: 0.9291\n",
            "Epoch 78: val_loss did not improve from 0.27433\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1928 - accuracy: 0.9292 - val_loss: 0.2748 - val_accuracy: 0.9000\n",
            "Epoch 79/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1854 - accuracy: 0.9392\n",
            "Epoch 79: val_loss improved from 0.27433 to 0.27303, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1845 - accuracy: 0.9400 - val_loss: 0.2730 - val_accuracy: 0.9000\n",
            "Epoch 80/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1966 - accuracy: 0.9306\n",
            "Epoch 80: val_loss improved from 0.27303 to 0.27233, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.1976 - accuracy: 0.9292 - val_loss: 0.2723 - val_accuracy: 0.9000\n",
            "Epoch 81/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1828 - accuracy: 0.9340\n",
            "Epoch 81: val_loss improved from 0.27233 to 0.27074, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.1854 - accuracy: 0.9342 - val_loss: 0.2707 - val_accuracy: 0.9067\n",
            "Epoch 82/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9358\n",
            "Epoch 82: val_loss improved from 0.27074 to 0.27020, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1779 - accuracy: 0.9358 - val_loss: 0.2702 - val_accuracy: 0.9067\n",
            "Epoch 83/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1752 - accuracy: 0.9342\n",
            "Epoch 83: val_loss did not improve from 0.27020\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1752 - accuracy: 0.9342 - val_loss: 0.2705 - val_accuracy: 0.9067\n",
            "Epoch 84/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9375\n",
            "Epoch 84: val_loss did not improve from 0.27020\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1685 - accuracy: 0.9375 - val_loss: 0.2717 - val_accuracy: 0.9100\n",
            "Epoch 85/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9317\n",
            "Epoch 85: val_loss improved from 0.27020 to 0.26901, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.1844 - accuracy: 0.9317 - val_loss: 0.2690 - val_accuracy: 0.9100\n",
            "Epoch 86/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9451\n",
            "Epoch 86: val_loss improved from 0.26901 to 0.26645, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1770 - accuracy: 0.9442 - val_loss: 0.2665 - val_accuracy: 0.9067\n",
            "Epoch 87/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9442\n",
            "Epoch 87: val_loss did not improve from 0.26645\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1644 - accuracy: 0.9442 - val_loss: 0.2675 - val_accuracy: 0.9100\n",
            "Epoch 88/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9443\n",
            "Epoch 88: val_loss improved from 0.26645 to 0.26546, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1678 - accuracy: 0.9450 - val_loss: 0.2655 - val_accuracy: 0.9033\n",
            "Epoch 89/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9333\n",
            "Epoch 89: val_loss improved from 0.26546 to 0.26442, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.1775 - accuracy: 0.9333 - val_loss: 0.2644 - val_accuracy: 0.9100\n",
            "Epoch 90/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9517\n",
            "Epoch 90: val_loss improved from 0.26442 to 0.26023, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1487 - accuracy: 0.9517 - val_loss: 0.2602 - val_accuracy: 0.9067\n",
            "Epoch 91/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9409\n",
            "Epoch 91: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1630 - accuracy: 0.9417 - val_loss: 0.2620 - val_accuracy: 0.9067\n",
            "Epoch 92/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9485\n",
            "Epoch 92: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1564 - accuracy: 0.9483 - val_loss: 0.2628 - val_accuracy: 0.9067\n",
            "Epoch 93/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9341\n",
            "Epoch 93: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1796 - accuracy: 0.9342 - val_loss: 0.2605 - val_accuracy: 0.9133\n",
            "Epoch 94/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9325\n",
            "Epoch 94: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1718 - accuracy: 0.9325 - val_loss: 0.2619 - val_accuracy: 0.9100\n",
            "Epoch 95/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.9485\n",
            "Epoch 95: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1444 - accuracy: 0.9483 - val_loss: 0.2607 - val_accuracy: 0.9100\n",
            "Epoch 96/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9325\n",
            "Epoch 96: val_loss did not improve from 0.26023\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.1576 - accuracy: 0.9325 - val_loss: 0.2620 - val_accuracy: 0.9067\n",
            "Epoch 97/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9485\n",
            "Epoch 97: val_loss improved from 0.26023 to 0.25899, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.1561 - accuracy: 0.9475 - val_loss: 0.2590 - val_accuracy: 0.9100\n",
            "Epoch 98/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9510\n",
            "Epoch 98: val_loss did not improve from 0.25899\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1434 - accuracy: 0.9508 - val_loss: 0.2604 - val_accuracy: 0.9167\n",
            "Epoch 99/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9350\n",
            "Epoch 99: val_loss improved from 0.25899 to 0.25577, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1657 - accuracy: 0.9350 - val_loss: 0.2558 - val_accuracy: 0.9133\n",
            "Epoch 100/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1474 - accuracy: 0.9500\n",
            "Epoch 100: val_loss improved from 0.25577 to 0.25474, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.1474 - accuracy: 0.9500 - val_loss: 0.2547 - val_accuracy: 0.9100\n",
            "Epoch 101/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9483\n",
            "Epoch 101: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1497 - accuracy: 0.9483 - val_loss: 0.2589 - val_accuracy: 0.9100\n",
            "Epoch 102/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.9540\n",
            "Epoch 102: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1358 - accuracy: 0.9550 - val_loss: 0.2586 - val_accuracy: 0.9100\n",
            "Epoch 103/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9535\n",
            "Epoch 103: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1406 - accuracy: 0.9542 - val_loss: 0.2579 - val_accuracy: 0.9100\n",
            "Epoch 104/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9561\n",
            "Epoch 104: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1309 - accuracy: 0.9550 - val_loss: 0.2596 - val_accuracy: 0.9100\n",
            "Epoch 105/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9433\n",
            "Epoch 105: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1385 - accuracy: 0.9433 - val_loss: 0.2564 - val_accuracy: 0.9133\n",
            "Epoch 106/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9459\n",
            "Epoch 106: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.1403 - accuracy: 0.9467 - val_loss: 0.2589 - val_accuracy: 0.9100\n",
            "Epoch 107/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9502\n",
            "Epoch 107: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1347 - accuracy: 0.9508 - val_loss: 0.2585 - val_accuracy: 0.9133\n",
            "Epoch 108/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1411 - accuracy: 0.9549\n",
            "Epoch 108: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1405 - accuracy: 0.9550 - val_loss: 0.2553 - val_accuracy: 0.9100\n",
            "Epoch 109/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.9434\n",
            "Epoch 109: val_loss did not improve from 0.25474\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1410 - accuracy: 0.9442 - val_loss: 0.2557 - val_accuracy: 0.9067\n",
            "Epoch 110/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.9517\n",
            "Epoch 110: val_loss improved from 0.25474 to 0.25301, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1442 - accuracy: 0.9517 - val_loss: 0.2530 - val_accuracy: 0.9167\n",
            "Epoch 111/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9508\n",
            "Epoch 111: val_loss improved from 0.25301 to 0.25040, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.1341 - accuracy: 0.9508 - val_loss: 0.2504 - val_accuracy: 0.9133\n",
            "Epoch 112/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9578\n",
            "Epoch 112: val_loss did not improve from 0.25040\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1384 - accuracy: 0.9583 - val_loss: 0.2539 - val_accuracy: 0.9100\n",
            "Epoch 113/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9603\n",
            "Epoch 113: val_loss did not improve from 0.25040\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1203 - accuracy: 0.9608 - val_loss: 0.2525 - val_accuracy: 0.9133\n",
            "Epoch 114/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9502\n",
            "Epoch 114: val_loss improved from 0.25040 to 0.24914, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.1410 - accuracy: 0.9508 - val_loss: 0.2491 - val_accuracy: 0.9133\n",
            "Epoch 115/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1275 - accuracy: 0.9549\n",
            "Epoch 115: val_loss did not improve from 0.24914\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1262 - accuracy: 0.9550 - val_loss: 0.2499 - val_accuracy: 0.9100\n",
            "Epoch 116/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.9470\n",
            "Epoch 116: val_loss did not improve from 0.24914\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1338 - accuracy: 0.9475 - val_loss: 0.2523 - val_accuracy: 0.9167\n",
            "Epoch 117/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9627\n",
            "Epoch 117: val_loss did not improve from 0.24914\n",
            "38/38 [==============================] - 2s 41ms/step - loss: 0.1164 - accuracy: 0.9642 - val_loss: 0.2517 - val_accuracy: 0.9100\n",
            "Epoch 118/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.9583\n",
            "Epoch 118: val_loss did not improve from 0.24914\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1196 - accuracy: 0.9592 - val_loss: 0.2525 - val_accuracy: 0.9100\n",
            "Epoch 119/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9592\n",
            "Epoch 119: val_loss did not improve from 0.24914\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.1172 - accuracy: 0.9592 - val_loss: 0.2520 - val_accuracy: 0.9133\n",
            "Epoch 120/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9575\n",
            "Epoch 120: val_loss improved from 0.24914 to 0.24863, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 132ms/step - loss: 0.1095 - accuracy: 0.9575 - val_loss: 0.2486 - val_accuracy: 0.9133\n",
            "Epoch 121/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9527\n",
            "Epoch 121: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1285 - accuracy: 0.9533 - val_loss: 0.2489 - val_accuracy: 0.9133\n",
            "Epoch 122/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9628\n",
            "Epoch 122: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1126 - accuracy: 0.9633 - val_loss: 0.2511 - val_accuracy: 0.9100\n",
            "Epoch 123/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9535\n",
            "Epoch 123: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1232 - accuracy: 0.9542 - val_loss: 0.2496 - val_accuracy: 0.9133\n",
            "Epoch 124/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1097 - accuracy: 0.9583\n",
            "Epoch 124: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1089 - accuracy: 0.9592 - val_loss: 0.2504 - val_accuracy: 0.9100\n",
            "Epoch 125/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9592\n",
            "Epoch 125: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1052 - accuracy: 0.9608 - val_loss: 0.2489 - val_accuracy: 0.9167\n",
            "Epoch 126/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9561\n",
            "Epoch 126: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 3s 83ms/step - loss: 0.1065 - accuracy: 0.9567 - val_loss: 0.2515 - val_accuracy: 0.9167\n",
            "Epoch 127/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1179 - accuracy: 0.9557\n",
            "Epoch 127: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1159 - accuracy: 0.9575 - val_loss: 0.2504 - val_accuracy: 0.9100\n",
            "Epoch 128/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9550\n",
            "Epoch 128: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1213 - accuracy: 0.9550 - val_loss: 0.2488 - val_accuracy: 0.9100\n",
            "Epoch 129/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1115 - accuracy: 0.9575\n",
            "Epoch 129: val_loss did not improve from 0.24863\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1138 - accuracy: 0.9567 - val_loss: 0.2495 - val_accuracy: 0.9133\n",
            "Epoch 130/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9667\n",
            "Epoch 130: val_loss improved from 0.24863 to 0.24801, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.0960 - accuracy: 0.9667 - val_loss: 0.2480 - val_accuracy: 0.9100\n",
            "Epoch 131/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9569\n",
            "Epoch 131: val_loss improved from 0.24801 to 0.24775, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.1248 - accuracy: 0.9567 - val_loss: 0.2478 - val_accuracy: 0.9100\n",
            "Epoch 132/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1068 - accuracy: 0.9635\n",
            "Epoch 132: val_loss improved from 0.24775 to 0.24761, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 0.2476 - val_accuracy: 0.9133\n",
            "Epoch 133/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9583\n",
            "Epoch 133: val_loss improved from 0.24761 to 0.24502, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1114 - accuracy: 0.9583 - val_loss: 0.2450 - val_accuracy: 0.9100\n",
            "Epoch 134/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9583\n",
            "Epoch 134: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1094 - accuracy: 0.9583 - val_loss: 0.2455 - val_accuracy: 0.9067\n",
            "Epoch 135/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9595\n",
            "Epoch 135: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1074 - accuracy: 0.9592 - val_loss: 0.2468 - val_accuracy: 0.9133\n",
            "Epoch 136/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1053 - accuracy: 0.9705\n",
            "Epoch 136: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1052 - accuracy: 0.9700 - val_loss: 0.2463 - val_accuracy: 0.9133\n",
            "Epoch 137/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9617\n",
            "Epoch 137: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1083 - accuracy: 0.9617 - val_loss: 0.2500 - val_accuracy: 0.9067\n",
            "Epoch 138/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0963 - accuracy: 0.9679\n",
            "Epoch 138: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0947 - accuracy: 0.9683 - val_loss: 0.2460 - val_accuracy: 0.9100\n",
            "Epoch 139/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1067 - accuracy: 0.9567\n",
            "Epoch 139: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1067 - accuracy: 0.9567 - val_loss: 0.2488 - val_accuracy: 0.9067\n",
            "Epoch 140/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9628\n",
            "Epoch 140: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1115 - accuracy: 0.9633 - val_loss: 0.2492 - val_accuracy: 0.9100\n",
            "Epoch 141/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9675\n",
            "Epoch 141: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0957 - accuracy: 0.9675 - val_loss: 0.2484 - val_accuracy: 0.9100\n",
            "Epoch 142/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0967 - accuracy: 0.9688\n",
            "Epoch 142: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0960 - accuracy: 0.9692 - val_loss: 0.2478 - val_accuracy: 0.9100\n",
            "Epoch 143/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9658\n",
            "Epoch 143: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1032 - accuracy: 0.9658 - val_loss: 0.2477 - val_accuracy: 0.9100\n",
            "Epoch 144/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0991 - accuracy: 0.9618\n",
            "Epoch 144: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0993 - accuracy: 0.9625 - val_loss: 0.2472 - val_accuracy: 0.9100\n",
            "Epoch 145/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9658\n",
            "Epoch 145: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0963 - accuracy: 0.9658 - val_loss: 0.2480 - val_accuracy: 0.9100\n",
            "Epoch 146/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9679\n",
            "Epoch 146: val_loss did not improve from 0.24502\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0919 - accuracy: 0.9683 - val_loss: 0.2493 - val_accuracy: 0.9133\n",
            "Epoch 147/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9595\n",
            "Epoch 147: val_loss improved from 0.24502 to 0.24502, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 129ms/step - loss: 0.0958 - accuracy: 0.9592 - val_loss: 0.2450 - val_accuracy: 0.9100\n",
            "Epoch 148/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9738\n",
            "Epoch 148: val_loss improved from 0.24502 to 0.24293, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0796 - accuracy: 0.9742 - val_loss: 0.2429 - val_accuracy: 0.9067\n",
            "Epoch 149/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0976 - accuracy: 0.9662\n",
            "Epoch 149: val_loss did not improve from 0.24293\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0973 - accuracy: 0.9658 - val_loss: 0.2435 - val_accuracy: 0.9100\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9738\n",
            "Epoch 150: val_loss did not improve from 0.24293\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0861 - accuracy: 0.9717 - val_loss: 0.2443 - val_accuracy: 0.9100\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9696\n",
            "Epoch 151: val_loss improved from 0.24293 to 0.24160, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0847 - accuracy: 0.9692 - val_loss: 0.2416 - val_accuracy: 0.9100\n",
            "Epoch 152/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0865 - accuracy: 0.9679\n",
            "Epoch 152: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0841 - accuracy: 0.9692 - val_loss: 0.2437 - val_accuracy: 0.9100\n",
            "Epoch 153/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9738\n",
            "Epoch 153: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0765 - accuracy: 0.9742 - val_loss: 0.2435 - val_accuracy: 0.9133\n",
            "Epoch 154/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0880 - accuracy: 0.9670\n",
            "Epoch 154: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0897 - accuracy: 0.9667 - val_loss: 0.2436 - val_accuracy: 0.9100\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9780\n",
            "Epoch 155: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0771 - accuracy: 0.9783 - val_loss: 0.2462 - val_accuracy: 0.9100\n",
            "Epoch 156/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0830 - accuracy: 0.9714\n",
            "Epoch 156: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0817 - accuracy: 0.9717 - val_loss: 0.2489 - val_accuracy: 0.9133\n",
            "Epoch 157/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0755 - accuracy: 0.9705\n",
            "Epoch 157: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0751 - accuracy: 0.9708 - val_loss: 0.2483 - val_accuracy: 0.9100\n",
            "Epoch 158/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0812 - accuracy: 0.9731\n",
            "Epoch 158: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0800 - accuracy: 0.9733 - val_loss: 0.2471 - val_accuracy: 0.9167\n",
            "Epoch 159/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0820 - accuracy: 0.9637\n",
            "Epoch 159: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0829 - accuracy: 0.9633 - val_loss: 0.2465 - val_accuracy: 0.9167\n",
            "Epoch 160/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9671\n",
            "Epoch 160: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0894 - accuracy: 0.9667 - val_loss: 0.2444 - val_accuracy: 0.9167\n",
            "Epoch 161/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9742\n",
            "Epoch 161: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0792 - accuracy: 0.9742 - val_loss: 0.2435 - val_accuracy: 0.9167\n",
            "Epoch 162/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9688\n",
            "Epoch 162: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0906 - accuracy: 0.9683 - val_loss: 0.2446 - val_accuracy: 0.9100\n",
            "Epoch 163/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9683\n",
            "Epoch 163: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.0736 - accuracy: 0.9683 - val_loss: 0.2420 - val_accuracy: 0.9133\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9806\n",
            "Epoch 164: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0708 - accuracy: 0.9800 - val_loss: 0.2420 - val_accuracy: 0.9100\n",
            "Epoch 165/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9675\n",
            "Epoch 165: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0813 - accuracy: 0.9675 - val_loss: 0.2449 - val_accuracy: 0.9100\n",
            "Epoch 166/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9780\n",
            "Epoch 166: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0679 - accuracy: 0.9783 - val_loss: 0.2445 - val_accuracy: 0.9133\n",
            "Epoch 167/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9780\n",
            "Epoch 167: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0698 - accuracy: 0.9783 - val_loss: 0.2444 - val_accuracy: 0.9100\n",
            "Epoch 168/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9704\n",
            "Epoch 168: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0785 - accuracy: 0.9700 - val_loss: 0.2484 - val_accuracy: 0.9100\n",
            "Epoch 169/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9750\n",
            "Epoch 169: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0812 - accuracy: 0.9750 - val_loss: 0.2439 - val_accuracy: 0.9100\n",
            "Epoch 170/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9750\n",
            "Epoch 170: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0710 - accuracy: 0.9750 - val_loss: 0.2452 - val_accuracy: 0.9100\n",
            "Epoch 171/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9758\n",
            "Epoch 171: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0692 - accuracy: 0.9758 - val_loss: 0.2436 - val_accuracy: 0.9100\n",
            "Epoch 172/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9767\n",
            "Epoch 172: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0730 - accuracy: 0.9767 - val_loss: 0.2461 - val_accuracy: 0.9167\n",
            "Epoch 173/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9767\n",
            "Epoch 173: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0666 - accuracy: 0.9767 - val_loss: 0.2473 - val_accuracy: 0.9200\n",
            "Epoch 174/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0612 - accuracy: 0.9772\n",
            "Epoch 174: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0608 - accuracy: 0.9775 - val_loss: 0.2470 - val_accuracy: 0.9100\n",
            "Epoch 175/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9747\n",
            "Epoch 175: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 0.2444 - val_accuracy: 0.9133\n",
            "Epoch 176/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9817\n",
            "Epoch 176: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0627 - accuracy: 0.9817 - val_loss: 0.2449 - val_accuracy: 0.9167\n",
            "Epoch 177/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9747\n",
            "Epoch 177: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0792 - accuracy: 0.9750 - val_loss: 0.2462 - val_accuracy: 0.9167\n",
            "Epoch 178/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0728 - accuracy: 0.9774\n",
            "Epoch 178: val_loss did not improve from 0.24160\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0723 - accuracy: 0.9775 - val_loss: 0.2443 - val_accuracy: 0.9133\n",
            "Epoch 179/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9717\n",
            "Epoch 179: val_loss improved from 0.24160 to 0.24075, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.0748 - accuracy: 0.9717 - val_loss: 0.2408 - val_accuracy: 0.9100\n",
            "Epoch 180/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9797\n",
            "Epoch 180: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0713 - accuracy: 0.9792 - val_loss: 0.2410 - val_accuracy: 0.9100\n",
            "Epoch 181/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0743 - accuracy: 0.9731\n",
            "Epoch 181: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.2428 - val_accuracy: 0.9100\n",
            "Epoch 182/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9831\n",
            "Epoch 182: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0638 - accuracy: 0.9833 - val_loss: 0.2470 - val_accuracy: 0.9100\n",
            "Epoch 183/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9825\n",
            "Epoch 183: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0647 - accuracy: 0.9825 - val_loss: 0.2458 - val_accuracy: 0.9100\n",
            "Epoch 184/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9747\n",
            "Epoch 184: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.0801 - accuracy: 0.9742 - val_loss: 0.2455 - val_accuracy: 0.9100\n",
            "Epoch 185/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9775\n",
            "Epoch 185: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0618 - accuracy: 0.9775 - val_loss: 0.2442 - val_accuracy: 0.9100\n",
            "Epoch 186/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9742\n",
            "Epoch 186: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0695 - accuracy: 0.9742 - val_loss: 0.2443 - val_accuracy: 0.9200\n",
            "Epoch 187/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0598 - accuracy: 0.9792\n",
            "Epoch 187: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0600 - accuracy: 0.9792 - val_loss: 0.2410 - val_accuracy: 0.9133\n",
            "Epoch 188/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0667 - accuracy: 0.9767\n",
            "Epoch 188: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0667 - accuracy: 0.9767 - val_loss: 0.2409 - val_accuracy: 0.9100\n",
            "Epoch 189/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9767\n",
            "Epoch 189: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0628 - accuracy: 0.9767 - val_loss: 0.2436 - val_accuracy: 0.9100\n",
            "Epoch 190/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9806\n",
            "Epoch 190: val_loss did not improve from 0.24075\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0703 - accuracy: 0.9792 - val_loss: 0.2415 - val_accuracy: 0.9133\n",
            "Epoch 191/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9730\n",
            "Epoch 191: val_loss improved from 0.24075 to 0.23843, saving model to /content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.0651 - accuracy: 0.9733 - val_loss: 0.2384 - val_accuracy: 0.9200\n",
            "Epoch 192/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9772\n",
            "Epoch 192: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0697 - accuracy: 0.9775 - val_loss: 0.2392 - val_accuracy: 0.9200\n",
            "Epoch 193/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9755\n",
            "Epoch 193: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0666 - accuracy: 0.9758 - val_loss: 0.2404 - val_accuracy: 0.9100\n",
            "Epoch 194/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0626 - accuracy: 0.9748\n",
            "Epoch 194: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0618 - accuracy: 0.9750 - val_loss: 0.2405 - val_accuracy: 0.9133\n",
            "Epoch 195/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9840\n",
            "Epoch 195: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0567 - accuracy: 0.9825 - val_loss: 0.2420 - val_accuracy: 0.9133\n",
            "Epoch 196/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0550 - accuracy: 0.9826\n",
            "Epoch 196: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.0548 - accuracy: 0.9825 - val_loss: 0.2439 - val_accuracy: 0.9100\n",
            "Epoch 197/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9783\n",
            "Epoch 197: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 57ms/step - loss: 0.0634 - accuracy: 0.9783 - val_loss: 0.2450 - val_accuracy: 0.9133\n",
            "Epoch 198/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9873\n",
            "Epoch 198: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0471 - accuracy: 0.9867 - val_loss: 0.2466 - val_accuracy: 0.9133\n",
            "Epoch 199/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9780\n",
            "Epoch 199: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.0624 - accuracy: 0.9775 - val_loss: 0.2437 - val_accuracy: 0.9100\n",
            "Epoch 200/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0590 - accuracy: 0.9818\n",
            "Epoch 200: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0575 - accuracy: 0.9825 - val_loss: 0.2422 - val_accuracy: 0.9133\n",
            "Epoch 201/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9814\n",
            "Epoch 201: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0616 - accuracy: 0.9817 - val_loss: 0.2425 - val_accuracy: 0.9133\n",
            "Epoch 202/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0652 - accuracy: 0.9800\n",
            "Epoch 202: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.0661 - accuracy: 0.9800 - val_loss: 0.2405 - val_accuracy: 0.9133\n",
            "Epoch 203/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9875\n",
            "Epoch 203: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0498 - accuracy: 0.9875 - val_loss: 0.2434 - val_accuracy: 0.9100\n",
            "Epoch 204/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9814\n",
            "Epoch 204: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0573 - accuracy: 0.9808 - val_loss: 0.2467 - val_accuracy: 0.9133\n",
            "Epoch 205/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9692\n",
            "Epoch 205: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0716 - accuracy: 0.9692 - val_loss: 0.2469 - val_accuracy: 0.9133\n",
            "Epoch 206/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9783\n",
            "Epoch 206: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0595 - accuracy: 0.9783 - val_loss: 0.2451 - val_accuracy: 0.9167\n",
            "Epoch 207/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0632 - accuracy: 0.9783\n",
            "Epoch 207: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0616 - accuracy: 0.9792 - val_loss: 0.2434 - val_accuracy: 0.9200\n",
            "Epoch 208/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0410 - accuracy: 0.9882\n",
            "Epoch 208: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0405 - accuracy: 0.9883 - val_loss: 0.2431 - val_accuracy: 0.9133\n",
            "Epoch 209/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9867\n",
            "Epoch 209: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0479 - accuracy: 0.9867 - val_loss: 0.2463 - val_accuracy: 0.9100\n",
            "Epoch 210/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9792\n",
            "Epoch 210: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0538 - accuracy: 0.9792 - val_loss: 0.2474 - val_accuracy: 0.9133\n",
            "Epoch 211/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9800\n",
            "Epoch 211: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0571 - accuracy: 0.9800 - val_loss: 0.2462 - val_accuracy: 0.9133\n",
            "Epoch 212/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9825\n",
            "Epoch 212: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0460 - accuracy: 0.9825 - val_loss: 0.2466 - val_accuracy: 0.9100\n",
            "Epoch 213/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9825\n",
            "Epoch 213: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0488 - accuracy: 0.9825 - val_loss: 0.2470 - val_accuracy: 0.9167\n",
            "Epoch 214/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9797\n",
            "Epoch 214: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0503 - accuracy: 0.9792 - val_loss: 0.2458 - val_accuracy: 0.9167\n",
            "Epoch 215/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9783\n",
            "Epoch 215: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0589 - accuracy: 0.9783 - val_loss: 0.2427 - val_accuracy: 0.9200\n",
            "Epoch 216/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0497 - accuracy: 0.9878\n",
            "Epoch 216: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0490 - accuracy: 0.9875 - val_loss: 0.2403 - val_accuracy: 0.9100\n",
            "Epoch 217/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 0.9831\n",
            "Epoch 217: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0474 - accuracy: 0.9833 - val_loss: 0.2450 - val_accuracy: 0.9200\n",
            "Epoch 218/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0424 - accuracy: 0.9856\n",
            "Epoch 218: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0419 - accuracy: 0.9858 - val_loss: 0.2430 - val_accuracy: 0.9133\n",
            "Epoch 219/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9817\n",
            "Epoch 219: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0571 - accuracy: 0.9817 - val_loss: 0.2449 - val_accuracy: 0.9167\n",
            "Epoch 220/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9817\n",
            "Epoch 220: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0462 - accuracy: 0.9817 - val_loss: 0.2451 - val_accuracy: 0.9133\n",
            "Epoch 221/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9831\n",
            "Epoch 221: val_loss did not improve from 0.23843\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.2469 - val_accuracy: 0.9100\n",
            "47/47 [==============================] - 2s 17ms/step - loss: 0.4647 - accuracy: 0.8767\n",
            "Test accuracy, 16 run, after finetuning: 0.8766666650772095\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.4841 - accuracy: 0.6475\n",
            "Epoch 1: val_loss improved from inf to 1.68561, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 44s 545ms/step - loss: 2.4841 - accuracy: 0.6475 - val_loss: 1.6856 - val_accuracy: 0.7100\n",
            "Epoch 2/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.5942 - accuracy: 0.7128\n",
            "Epoch 2: val_loss improved from 1.68561 to 1.13190, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 1.6158 - accuracy: 0.7108 - val_loss: 1.1319 - val_accuracy: 0.7467\n",
            "Epoch 3/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 1.1981 - accuracy: 0.7326\n",
            "Epoch 3: val_loss improved from 1.13190 to 0.86884, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 1.1739 - accuracy: 0.7350 - val_loss: 0.8688 - val_accuracy: 0.7867\n",
            "Epoch 4/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.9818 - accuracy: 0.7578\n",
            "Epoch 4: val_loss improved from 0.86884 to 0.73515, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.9720 - accuracy: 0.7617 - val_loss: 0.7351 - val_accuracy: 0.7967\n",
            "Epoch 5/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.8814 - accuracy: 0.7725\n",
            "Epoch 5: val_loss improved from 0.73515 to 0.64736, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.8814 - accuracy: 0.7725 - val_loss: 0.6474 - val_accuracy: 0.8033\n",
            "Epoch 6/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7986 - accuracy: 0.7872\n",
            "Epoch 6: val_loss improved from 0.64736 to 0.59323, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.7963 - accuracy: 0.7875 - val_loss: 0.5932 - val_accuracy: 0.8067\n",
            "Epoch 7/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.7350 - accuracy: 0.7873\n",
            "Epoch 7: val_loss improved from 0.59323 to 0.55632, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.7276 - accuracy: 0.7875 - val_loss: 0.5563 - val_accuracy: 0.8100\n",
            "Epoch 8/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7150 - accuracy: 0.7838\n",
            "Epoch 8: val_loss improved from 0.55632 to 0.52317, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 155ms/step - loss: 0.7135 - accuracy: 0.7842 - val_loss: 0.5232 - val_accuracy: 0.8167\n",
            "Epoch 9/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6539 - accuracy: 0.8033\n",
            "Epoch 9: val_loss improved from 0.52317 to 0.50041, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 7s 193ms/step - loss: 0.6539 - accuracy: 0.8033 - val_loss: 0.5004 - val_accuracy: 0.8233\n",
            "Epoch 10/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6628 - accuracy: 0.8003\n",
            "Epoch 10: val_loss improved from 0.50041 to 0.48170, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.6498 - accuracy: 0.8033 - val_loss: 0.4817 - val_accuracy: 0.8300\n",
            "Epoch 11/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.8193\n",
            "Epoch 11: val_loss improved from 0.48170 to 0.46704, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.6043 - accuracy: 0.8192 - val_loss: 0.4670 - val_accuracy: 0.8367\n",
            "Epoch 12/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.8183\n",
            "Epoch 12: val_loss improved from 0.46704 to 0.45200, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.5870 - accuracy: 0.8183 - val_loss: 0.4520 - val_accuracy: 0.8367\n",
            "Epoch 13/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.8375\n",
            "Epoch 13: val_loss improved from 0.45200 to 0.44056, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.5396 - accuracy: 0.8375 - val_loss: 0.4406 - val_accuracy: 0.8367\n",
            "Epoch 14/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5701 - accuracy: 0.8194\n",
            "Epoch 14: val_loss improved from 0.44056 to 0.43183, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.5616 - accuracy: 0.8225 - val_loss: 0.4318 - val_accuracy: 0.8400\n",
            "Epoch 15/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.8208\n",
            "Epoch 15: val_loss improved from 0.43183 to 0.41872, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.5576 - accuracy: 0.8208 - val_loss: 0.4187 - val_accuracy: 0.8433\n",
            "Epoch 16/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8183\n",
            "Epoch 16: val_loss improved from 0.41872 to 0.41427, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.5297 - accuracy: 0.8183 - val_loss: 0.4143 - val_accuracy: 0.8433\n",
            "Epoch 17/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5221 - accuracy: 0.8403\n",
            "Epoch 17: val_loss improved from 0.41427 to 0.40427, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.5259 - accuracy: 0.8375 - val_loss: 0.4043 - val_accuracy: 0.8433\n",
            "Epoch 18/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5300 - accuracy: 0.8184\n",
            "Epoch 18: val_loss improved from 0.40427 to 0.39533, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.5238 - accuracy: 0.8208 - val_loss: 0.3953 - val_accuracy: 0.8400\n",
            "Epoch 19/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5133 - accuracy: 0.8290\n",
            "Epoch 19: val_loss improved from 0.39533 to 0.38901, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.5060 - accuracy: 0.8317 - val_loss: 0.3890 - val_accuracy: 0.8467\n",
            "Epoch 20/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4692 - accuracy: 0.8394\n",
            "Epoch 20: val_loss improved from 0.38901 to 0.38488, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.4595 - accuracy: 0.8417 - val_loss: 0.3849 - val_accuracy: 0.8500\n",
            "Epoch 21/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.8404\n",
            "Epoch 21: val_loss improved from 0.38488 to 0.37662, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.4736 - accuracy: 0.8392 - val_loss: 0.3766 - val_accuracy: 0.8500\n",
            "Epoch 22/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8533\n",
            "Epoch 22: val_loss improved from 0.37662 to 0.36808, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4521 - accuracy: 0.8533 - val_loss: 0.3681 - val_accuracy: 0.8533\n",
            "Epoch 23/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.8350\n",
            "Epoch 23: val_loss improved from 0.36808 to 0.36346, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.4684 - accuracy: 0.8350 - val_loss: 0.3635 - val_accuracy: 0.8467\n",
            "Epoch 24/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4291 - accuracy: 0.8539\n",
            "Epoch 24: val_loss improved from 0.36346 to 0.35797, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.4314 - accuracy: 0.8542 - val_loss: 0.3580 - val_accuracy: 0.8467\n",
            "Epoch 25/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3965 - accuracy: 0.8633\n",
            "Epoch 25: val_loss improved from 0.35797 to 0.35371, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.3965 - accuracy: 0.8633 - val_loss: 0.3537 - val_accuracy: 0.8533\n",
            "Epoch 26/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8581\n",
            "Epoch 26: val_loss improved from 0.35371 to 0.35076, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3921 - accuracy: 0.8583 - val_loss: 0.3508 - val_accuracy: 0.8500\n",
            "Epoch 27/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8615\n",
            "Epoch 27: val_loss improved from 0.35076 to 0.34346, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.4047 - accuracy: 0.8617 - val_loss: 0.3435 - val_accuracy: 0.8600\n",
            "Epoch 28/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8640\n",
            "Epoch 28: val_loss improved from 0.34346 to 0.33580, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 138ms/step - loss: 0.3793 - accuracy: 0.8633 - val_loss: 0.3358 - val_accuracy: 0.8700\n",
            "Epoch 29/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4007 - accuracy: 0.8550\n",
            "Epoch 29: val_loss improved from 0.33580 to 0.33516, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.4007 - accuracy: 0.8550 - val_loss: 0.3352 - val_accuracy: 0.8600\n",
            "Epoch 30/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8517\n",
            "Epoch 30: val_loss improved from 0.33516 to 0.33077, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.4036 - accuracy: 0.8517 - val_loss: 0.3308 - val_accuracy: 0.8633\n",
            "Epoch 31/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3687 - accuracy: 0.8637\n",
            "Epoch 31: val_loss improved from 0.33077 to 0.32907, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.3646 - accuracy: 0.8658 - val_loss: 0.3291 - val_accuracy: 0.8667\n",
            "Epoch 32/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3959 - accuracy: 0.8598\n",
            "Epoch 32: val_loss improved from 0.32907 to 0.32347, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.3933 - accuracy: 0.8608 - val_loss: 0.3235 - val_accuracy: 0.8667\n",
            "Epoch 33/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8843\n",
            "Epoch 33: val_loss improved from 0.32347 to 0.32132, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.3322 - accuracy: 0.8858 - val_loss: 0.3213 - val_accuracy: 0.8667\n",
            "Epoch 34/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.8692\n",
            "Epoch 34: val_loss improved from 0.32132 to 0.31760, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.3636 - accuracy: 0.8692 - val_loss: 0.3176 - val_accuracy: 0.8800\n",
            "Epoch 35/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.8834\n",
            "Epoch 35: val_loss improved from 0.31760 to 0.31636, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.3351 - accuracy: 0.8833 - val_loss: 0.3164 - val_accuracy: 0.8767\n",
            "Epoch 36/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3531 - accuracy: 0.8828\n",
            "Epoch 36: val_loss improved from 0.31636 to 0.31366, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3606 - accuracy: 0.8783 - val_loss: 0.3137 - val_accuracy: 0.8767\n",
            "Epoch 37/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3608 - accuracy: 0.8689\n",
            "Epoch 37: val_loss improved from 0.31366 to 0.31058, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3597 - accuracy: 0.8692 - val_loss: 0.3106 - val_accuracy: 0.8733\n",
            "Epoch 38/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.8758\n",
            "Epoch 38: val_loss did not improve from 0.31058\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.3415 - accuracy: 0.8767 - val_loss: 0.3125 - val_accuracy: 0.8800\n",
            "Epoch 39/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3481 - accuracy: 0.8733\n",
            "Epoch 39: val_loss improved from 0.31058 to 0.30573, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.3429 - accuracy: 0.8742 - val_loss: 0.3057 - val_accuracy: 0.8800\n",
            "Epoch 40/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8758\n",
            "Epoch 40: val_loss improved from 0.30573 to 0.30199, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3253 - accuracy: 0.8758 - val_loss: 0.3020 - val_accuracy: 0.8800\n",
            "Epoch 41/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8817\n",
            "Epoch 41: val_loss improved from 0.30199 to 0.29890, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.3355 - accuracy: 0.8817 - val_loss: 0.2989 - val_accuracy: 0.8833\n",
            "Epoch 42/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3286 - accuracy: 0.8837\n",
            "Epoch 42: val_loss improved from 0.29890 to 0.29665, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3213 - accuracy: 0.8858 - val_loss: 0.2966 - val_accuracy: 0.8833\n",
            "Epoch 43/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3254 - accuracy: 0.8851\n",
            "Epoch 43: val_loss improved from 0.29665 to 0.29561, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 125ms/step - loss: 0.3261 - accuracy: 0.8850 - val_loss: 0.2956 - val_accuracy: 0.8833\n",
            "Epoch 44/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3189 - accuracy: 0.8872\n",
            "Epoch 44: val_loss improved from 0.29561 to 0.29272, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3161 - accuracy: 0.8883 - val_loss: 0.2927 - val_accuracy: 0.8900\n",
            "Epoch 45/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.8925\n",
            "Epoch 45: val_loss improved from 0.29272 to 0.28765, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3048 - accuracy: 0.8925 - val_loss: 0.2877 - val_accuracy: 0.8867\n",
            "Epoch 46/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8970\n",
            "Epoch 46: val_loss improved from 0.28765 to 0.28493, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2965 - accuracy: 0.8975 - val_loss: 0.2849 - val_accuracy: 0.9000\n",
            "Epoch 47/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2755 - accuracy: 0.8950\n",
            "Epoch 47: val_loss did not improve from 0.28493\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.2759 - accuracy: 0.8958 - val_loss: 0.2888 - val_accuracy: 0.8933\n",
            "Epoch 48/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.8889\n",
            "Epoch 48: val_loss improved from 0.28493 to 0.28259, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.2976 - accuracy: 0.8908 - val_loss: 0.2826 - val_accuracy: 0.8900\n",
            "Epoch 49/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.8976\n",
            "Epoch 49: val_loss improved from 0.28259 to 0.28126, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.3103 - accuracy: 0.8950 - val_loss: 0.2813 - val_accuracy: 0.8833\n",
            "Epoch 50/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9088\n",
            "Epoch 50: val_loss improved from 0.28126 to 0.27778, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2771 - accuracy: 0.9083 - val_loss: 0.2778 - val_accuracy: 0.9033\n",
            "Epoch 51/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2747 - accuracy: 0.9036\n",
            "Epoch 51: val_loss improved from 0.27778 to 0.27403, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.2780 - accuracy: 0.9042 - val_loss: 0.2740 - val_accuracy: 0.9033\n",
            "Epoch 52/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8983\n",
            "Epoch 52: val_loss improved from 0.27403 to 0.27391, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2767 - accuracy: 0.8983 - val_loss: 0.2739 - val_accuracy: 0.9033\n",
            "Epoch 53/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.8910\n",
            "Epoch 53: val_loss improved from 0.27391 to 0.26613, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2788 - accuracy: 0.8917 - val_loss: 0.2661 - val_accuracy: 0.9067\n",
            "Epoch 54/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2718 - accuracy: 0.9062\n",
            "Epoch 54: val_loss improved from 0.26613 to 0.26213, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.2709 - accuracy: 0.9067 - val_loss: 0.2621 - val_accuracy: 0.9100\n",
            "Epoch 55/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2596 - accuracy: 0.9062\n",
            "Epoch 55: val_loss did not improve from 0.26213\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.2627 - accuracy: 0.9058 - val_loss: 0.2635 - val_accuracy: 0.9067\n",
            "Epoch 56/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2646 - accuracy: 0.9045\n",
            "Epoch 56: val_loss did not improve from 0.26213\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2644 - accuracy: 0.9058 - val_loss: 0.2628 - val_accuracy: 0.9033\n",
            "Epoch 57/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2508 - accuracy: 0.9106\n",
            "Epoch 57: val_loss did not improve from 0.26213\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2584 - accuracy: 0.9108 - val_loss: 0.2632 - val_accuracy: 0.9067\n",
            "Epoch 58/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9050\n",
            "Epoch 58: val_loss did not improve from 0.26213\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.2571 - accuracy: 0.9050 - val_loss: 0.2651 - val_accuracy: 0.9133\n",
            "Epoch 59/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9181\n",
            "Epoch 59: val_loss improved from 0.26213 to 0.26154, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.2463 - accuracy: 0.9167 - val_loss: 0.2615 - val_accuracy: 0.9133\n",
            "Epoch 60/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9042\n",
            "Epoch 60: val_loss improved from 0.26154 to 0.25631, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.2569 - accuracy: 0.9042 - val_loss: 0.2563 - val_accuracy: 0.9200\n",
            "Epoch 61/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9139\n",
            "Epoch 61: val_loss improved from 0.25631 to 0.25506, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2339 - accuracy: 0.9133 - val_loss: 0.2551 - val_accuracy: 0.9200\n",
            "Epoch 62/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2457 - accuracy: 0.9097\n",
            "Epoch 62: val_loss improved from 0.25506 to 0.25059, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2470 - accuracy: 0.9092 - val_loss: 0.2506 - val_accuracy: 0.9133\n",
            "Epoch 63/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9105\n",
            "Epoch 63: val_loss improved from 0.25059 to 0.24829, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.2406 - accuracy: 0.9100 - val_loss: 0.2483 - val_accuracy: 0.9200\n",
            "Epoch 64/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2505 - accuracy: 0.8984\n",
            "Epoch 64: val_loss improved from 0.24829 to 0.24829, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.2450 - accuracy: 0.9008 - val_loss: 0.2483 - val_accuracy: 0.9167\n",
            "Epoch 65/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9231\n",
            "Epoch 65: val_loss did not improve from 0.24829\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2384 - accuracy: 0.9233 - val_loss: 0.2509 - val_accuracy: 0.9233\n",
            "Epoch 66/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9125\n",
            "Epoch 66: val_loss improved from 0.24829 to 0.24718, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2297 - accuracy: 0.9125 - val_loss: 0.2472 - val_accuracy: 0.9233\n",
            "Epoch 67/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9147\n",
            "Epoch 67: val_loss improved from 0.24718 to 0.24667, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.2409 - accuracy: 0.9133 - val_loss: 0.2467 - val_accuracy: 0.9200\n",
            "Epoch 68/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9231\n",
            "Epoch 68: val_loss improved from 0.24667 to 0.24362, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.2176 - accuracy: 0.9225 - val_loss: 0.2436 - val_accuracy: 0.9233\n",
            "Epoch 69/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9142\n",
            "Epoch 69: val_loss improved from 0.24362 to 0.24143, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.2260 - accuracy: 0.9142 - val_loss: 0.2414 - val_accuracy: 0.9200\n",
            "Epoch 70/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9248\n",
            "Epoch 70: val_loss did not improve from 0.24143\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2077 - accuracy: 0.9258 - val_loss: 0.2427 - val_accuracy: 0.9200\n",
            "Epoch 71/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9283\n",
            "Epoch 71: val_loss improved from 0.24143 to 0.24131, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.2167 - accuracy: 0.9283 - val_loss: 0.2413 - val_accuracy: 0.9233\n",
            "Epoch 72/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2196 - accuracy: 0.9253\n",
            "Epoch 72: val_loss improved from 0.24131 to 0.24006, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2203 - accuracy: 0.9225 - val_loss: 0.2401 - val_accuracy: 0.9200\n",
            "Epoch 73/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9324\n",
            "Epoch 73: val_loss did not improve from 0.24006\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2108 - accuracy: 0.9317 - val_loss: 0.2420 - val_accuracy: 0.9300\n",
            "Epoch 74/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9258\n",
            "Epoch 74: val_loss improved from 0.24006 to 0.23849, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.2138 - accuracy: 0.9258 - val_loss: 0.2385 - val_accuracy: 0.9133\n",
            "Epoch 75/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9275\n",
            "Epoch 75: val_loss did not improve from 0.23849\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2226 - accuracy: 0.9275 - val_loss: 0.2426 - val_accuracy: 0.9267\n",
            "Epoch 76/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9299\n",
            "Epoch 76: val_loss improved from 0.23849 to 0.23826, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1992 - accuracy: 0.9300 - val_loss: 0.2383 - val_accuracy: 0.9200\n",
            "Epoch 77/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9275\n",
            "Epoch 77: val_loss improved from 0.23826 to 0.23723, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2094 - accuracy: 0.9275 - val_loss: 0.2372 - val_accuracy: 0.9200\n",
            "Epoch 78/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2132 - accuracy: 0.9358\n",
            "Epoch 78: val_loss improved from 0.23723 to 0.23246, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2121 - accuracy: 0.9350 - val_loss: 0.2325 - val_accuracy: 0.9233\n",
            "Epoch 79/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1865 - accuracy: 0.9341\n",
            "Epoch 79: val_loss did not improve from 0.23246\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1941 - accuracy: 0.9325 - val_loss: 0.2360 - val_accuracy: 0.9333\n",
            "Epoch 80/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9231\n",
            "Epoch 80: val_loss did not improve from 0.23246\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2124 - accuracy: 0.9233 - val_loss: 0.2343 - val_accuracy: 0.9300\n",
            "Epoch 81/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9383\n",
            "Epoch 81: val_loss improved from 0.23246 to 0.23245, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.1888 - accuracy: 0.9375 - val_loss: 0.2324 - val_accuracy: 0.9233\n",
            "Epoch 82/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9291\n",
            "Epoch 82: val_loss did not improve from 0.23245\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1825 - accuracy: 0.9300 - val_loss: 0.2334 - val_accuracy: 0.9333\n",
            "Epoch 83/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9358\n",
            "Epoch 83: val_loss improved from 0.23245 to 0.22999, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1856 - accuracy: 0.9358 - val_loss: 0.2300 - val_accuracy: 0.9333\n",
            "Epoch 84/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1841 - accuracy: 0.9410\n",
            "Epoch 84: val_loss improved from 0.22999 to 0.22840, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1865 - accuracy: 0.9408 - val_loss: 0.2284 - val_accuracy: 0.9367\n",
            "Epoch 85/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2015 - accuracy: 0.9236\n",
            "Epoch 85: val_loss improved from 0.22840 to 0.22402, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.2017 - accuracy: 0.9250 - val_loss: 0.2240 - val_accuracy: 0.9300\n",
            "Epoch 86/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1839 - accuracy: 0.9425\n",
            "Epoch 86: val_loss did not improve from 0.22402\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1839 - accuracy: 0.9425 - val_loss: 0.2268 - val_accuracy: 0.9333\n",
            "Epoch 87/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9375\n",
            "Epoch 87: val_loss did not improve from 0.22402\n",
            "38/38 [==============================] - 2s 41ms/step - loss: 0.1809 - accuracy: 0.9375 - val_loss: 0.2287 - val_accuracy: 0.9400\n",
            "Epoch 88/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9317\n",
            "Epoch 88: val_loss improved from 0.22402 to 0.22321, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1720 - accuracy: 0.9317 - val_loss: 0.2232 - val_accuracy: 0.9367\n",
            "Epoch 89/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9341\n",
            "Epoch 89: val_loss did not improve from 0.22321\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1808 - accuracy: 0.9350 - val_loss: 0.2259 - val_accuracy: 0.9300\n",
            "Epoch 90/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1637 - accuracy: 0.9358\n",
            "Epoch 90: val_loss did not improve from 0.22321\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1659 - accuracy: 0.9358 - val_loss: 0.2260 - val_accuracy: 0.9333\n",
            "Epoch 91/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9459\n",
            "Epoch 91: val_loss improved from 0.22321 to 0.22033, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.1512 - accuracy: 0.9458 - val_loss: 0.2203 - val_accuracy: 0.9433\n",
            "Epoch 92/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9400\n",
            "Epoch 92: val_loss improved from 0.22033 to 0.21940, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1769 - accuracy: 0.9408 - val_loss: 0.2194 - val_accuracy: 0.9333\n",
            "Epoch 93/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1535 - accuracy: 0.9540\n",
            "Epoch 93: val_loss did not improve from 0.21940\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1557 - accuracy: 0.9525 - val_loss: 0.2204 - val_accuracy: 0.9400\n",
            "Epoch 94/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9408\n",
            "Epoch 94: val_loss did not improve from 0.21940\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1650 - accuracy: 0.9408 - val_loss: 0.2222 - val_accuracy: 0.9367\n",
            "Epoch 95/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1512 - accuracy: 0.9479\n",
            "Epoch 95: val_loss did not improve from 0.21940\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1532 - accuracy: 0.9467 - val_loss: 0.2225 - val_accuracy: 0.9400\n",
            "Epoch 96/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1609 - accuracy: 0.9350\n",
            "Epoch 96: val_loss did not improve from 0.21940\n",
            "38/38 [==============================] - 2s 66ms/step - loss: 0.1609 - accuracy: 0.9350 - val_loss: 0.2247 - val_accuracy: 0.9467\n",
            "Epoch 97/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1533 - accuracy: 0.9436\n",
            "Epoch 97: val_loss improved from 0.21940 to 0.21890, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 131ms/step - loss: 0.1565 - accuracy: 0.9425 - val_loss: 0.2189 - val_accuracy: 0.9367\n",
            "Epoch 98/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1552 - accuracy: 0.9383\n",
            "Epoch 98: val_loss did not improve from 0.21890\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1552 - accuracy: 0.9383 - val_loss: 0.2208 - val_accuracy: 0.9400\n",
            "Epoch 99/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9467\n",
            "Epoch 99: val_loss improved from 0.21890 to 0.21640, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1616 - accuracy: 0.9467 - val_loss: 0.2164 - val_accuracy: 0.9433\n",
            "Epoch 100/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1510 - accuracy: 0.9488\n",
            "Epoch 100: val_loss improved from 0.21640 to 0.21531, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.1547 - accuracy: 0.9475 - val_loss: 0.2153 - val_accuracy: 0.9433\n",
            "Epoch 101/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.9505\n",
            "Epoch 101: val_loss did not improve from 0.21531\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1388 - accuracy: 0.9500 - val_loss: 0.2167 - val_accuracy: 0.9433\n",
            "Epoch 102/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1494 - accuracy: 0.9462\n",
            "Epoch 102: val_loss did not improve from 0.21531\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1465 - accuracy: 0.9475 - val_loss: 0.2178 - val_accuracy: 0.9467\n",
            "Epoch 103/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9508\n",
            "Epoch 103: val_loss improved from 0.21531 to 0.21509, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1538 - accuracy: 0.9508 - val_loss: 0.2151 - val_accuracy: 0.9467\n",
            "Epoch 104/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9375\n",
            "Epoch 104: val_loss improved from 0.21509 to 0.21118, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1633 - accuracy: 0.9375 - val_loss: 0.2112 - val_accuracy: 0.9433\n",
            "Epoch 105/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9514\n",
            "Epoch 105: val_loss did not improve from 0.21118\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1358 - accuracy: 0.9525 - val_loss: 0.2135 - val_accuracy: 0.9500\n",
            "Epoch 106/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9558\n",
            "Epoch 106: val_loss did not improve from 0.21118\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1396 - accuracy: 0.9558 - val_loss: 0.2192 - val_accuracy: 0.9467\n",
            "Epoch 107/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9492\n",
            "Epoch 107: val_loss did not improve from 0.21118\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.1428 - accuracy: 0.9492 - val_loss: 0.2160 - val_accuracy: 0.9500\n",
            "Epoch 108/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9586\n",
            "Epoch 108: val_loss did not improve from 0.21118\n",
            "38/38 [==============================] - 2s 57ms/step - loss: 0.1245 - accuracy: 0.9583 - val_loss: 0.2141 - val_accuracy: 0.9467\n",
            "Epoch 109/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1437 - accuracy: 0.9488\n",
            "Epoch 109: val_loss did not improve from 0.21118\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1403 - accuracy: 0.9508 - val_loss: 0.2176 - val_accuracy: 0.9467\n",
            "Epoch 110/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.9542\n",
            "Epoch 110: val_loss improved from 0.21118 to 0.20879, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.1331 - accuracy: 0.9542 - val_loss: 0.2088 - val_accuracy: 0.9433\n",
            "Epoch 111/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9467\n",
            "Epoch 111: val_loss did not improve from 0.20879\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1322 - accuracy: 0.9467 - val_loss: 0.2092 - val_accuracy: 0.9433\n",
            "Epoch 112/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9592\n",
            "Epoch 112: val_loss did not improve from 0.20879\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1166 - accuracy: 0.9592 - val_loss: 0.2112 - val_accuracy: 0.9467\n",
            "Epoch 113/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.9549\n",
            "Epoch 113: val_loss did not improve from 0.20879\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1322 - accuracy: 0.9567 - val_loss: 0.2148 - val_accuracy: 0.9500\n",
            "Epoch 114/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1186 - accuracy: 0.9583\n",
            "Epoch 114: val_loss did not improve from 0.20879\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1186 - accuracy: 0.9583 - val_loss: 0.2186 - val_accuracy: 0.9500\n",
            "Epoch 115/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9492\n",
            "Epoch 115: val_loss improved from 0.20879 to 0.20744, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 119ms/step - loss: 0.1433 - accuracy: 0.9492 - val_loss: 0.2074 - val_accuracy: 0.9500\n",
            "Epoch 116/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1344 - accuracy: 0.9540\n",
            "Epoch 116: val_loss did not improve from 0.20744\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1324 - accuracy: 0.9558 - val_loss: 0.2088 - val_accuracy: 0.9500\n",
            "Epoch 117/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9558\n",
            "Epoch 117: val_loss improved from 0.20744 to 0.20655, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.1239 - accuracy: 0.9558 - val_loss: 0.2065 - val_accuracy: 0.9467\n",
            "Epoch 118/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9535\n",
            "Epoch 118: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1274 - accuracy: 0.9542 - val_loss: 0.2131 - val_accuracy: 0.9500\n",
            "Epoch 119/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1214 - accuracy: 0.9601\n",
            "Epoch 119: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1180 - accuracy: 0.9608 - val_loss: 0.2117 - val_accuracy: 0.9467\n",
            "Epoch 120/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9645\n",
            "Epoch 120: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1161 - accuracy: 0.9617 - val_loss: 0.2128 - val_accuracy: 0.9500\n",
            "Epoch 121/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9550\n",
            "Epoch 121: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1281 - accuracy: 0.9550 - val_loss: 0.2113 - val_accuracy: 0.9467\n",
            "Epoch 122/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9595\n",
            "Epoch 122: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1172 - accuracy: 0.9600 - val_loss: 0.2153 - val_accuracy: 0.9500\n",
            "Epoch 123/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9561\n",
            "Epoch 123: val_loss did not improve from 0.20655\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1222 - accuracy: 0.9558 - val_loss: 0.2094 - val_accuracy: 0.9500\n",
            "Epoch 124/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9488\n",
            "Epoch 124: val_loss improved from 0.20655 to 0.20238, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.1204 - accuracy: 0.9492 - val_loss: 0.2024 - val_accuracy: 0.9500\n",
            "Epoch 125/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9519\n",
            "Epoch 125: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1266 - accuracy: 0.9517 - val_loss: 0.2073 - val_accuracy: 0.9500\n",
            "Epoch 126/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9583\n",
            "Epoch 126: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.1267 - accuracy: 0.9583 - val_loss: 0.2123 - val_accuracy: 0.9500\n",
            "Epoch 127/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9620\n",
            "Epoch 127: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.2084 - val_accuracy: 0.9500\n",
            "Epoch 128/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1043 - accuracy: 0.9653\n",
            "Epoch 128: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1026 - accuracy: 0.9658 - val_loss: 0.2075 - val_accuracy: 0.9500\n",
            "Epoch 129/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9645\n",
            "Epoch 129: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1011 - accuracy: 0.9633 - val_loss: 0.2128 - val_accuracy: 0.9500\n",
            "Epoch 130/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1116 - accuracy: 0.9544\n",
            "Epoch 130: val_loss did not improve from 0.20238\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1128 - accuracy: 0.9542 - val_loss: 0.2078 - val_accuracy: 0.9533\n",
            "Epoch 131/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9575\n",
            "Epoch 131: val_loss improved from 0.20238 to 0.20115, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 126ms/step - loss: 0.1175 - accuracy: 0.9575 - val_loss: 0.2012 - val_accuracy: 0.9500\n",
            "Epoch 132/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9628\n",
            "Epoch 132: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1061 - accuracy: 0.9625 - val_loss: 0.2089 - val_accuracy: 0.9500\n",
            "Epoch 133/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9642\n",
            "Epoch 133: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1082 - accuracy: 0.9642 - val_loss: 0.2084 - val_accuracy: 0.9500\n",
            "Epoch 134/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9642\n",
            "Epoch 134: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0990 - accuracy: 0.9642 - val_loss: 0.2078 - val_accuracy: 0.9500\n",
            "Epoch 135/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9628\n",
            "Epoch 135: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1020 - accuracy: 0.9633 - val_loss: 0.2020 - val_accuracy: 0.9500\n",
            "Epoch 136/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1017 - accuracy: 0.9653\n",
            "Epoch 136: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1022 - accuracy: 0.9650 - val_loss: 0.2064 - val_accuracy: 0.9533\n",
            "Epoch 137/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9645\n",
            "Epoch 137: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1094 - accuracy: 0.9633 - val_loss: 0.2020 - val_accuracy: 0.9533\n",
            "Epoch 138/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9650\n",
            "Epoch 138: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0963 - accuracy: 0.9650 - val_loss: 0.2027 - val_accuracy: 0.9500\n",
            "Epoch 139/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1128 - accuracy: 0.9628\n",
            "Epoch 139: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1120 - accuracy: 0.9633 - val_loss: 0.2130 - val_accuracy: 0.9500\n",
            "Epoch 140/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9637\n",
            "Epoch 140: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0996 - accuracy: 0.9633 - val_loss: 0.2127 - val_accuracy: 0.9533\n",
            "Epoch 141/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0874 - accuracy: 0.9705\n",
            "Epoch 141: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0878 - accuracy: 0.9708 - val_loss: 0.2094 - val_accuracy: 0.9500\n",
            "Epoch 142/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0931 - accuracy: 0.9679\n",
            "Epoch 142: val_loss did not improve from 0.20115\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0916 - accuracy: 0.9683 - val_loss: 0.2075 - val_accuracy: 0.9533\n",
            "Epoch 143/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9625\n",
            "Epoch 143: val_loss improved from 0.20115 to 0.19994, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 140ms/step - loss: 0.0991 - accuracy: 0.9625 - val_loss: 0.1999 - val_accuracy: 0.9500\n",
            "Epoch 144/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9738\n",
            "Epoch 144: val_loss did not improve from 0.19994\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0867 - accuracy: 0.9733 - val_loss: 0.2064 - val_accuracy: 0.9533\n",
            "Epoch 145/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9742\n",
            "Epoch 145: val_loss did not improve from 0.19994\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0813 - accuracy: 0.9742 - val_loss: 0.2045 - val_accuracy: 0.9533\n",
            "Epoch 146/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0920 - accuracy: 0.9722\n",
            "Epoch 146: val_loss did not improve from 0.19994\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0915 - accuracy: 0.9725 - val_loss: 0.2059 - val_accuracy: 0.9500\n",
            "Epoch 147/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9688\n",
            "Epoch 147: val_loss improved from 0.19994 to 0.19929, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0900 - accuracy: 0.9692 - val_loss: 0.1993 - val_accuracy: 0.9500\n",
            "Epoch 148/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9708\n",
            "Epoch 148: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0866 - accuracy: 0.9708 - val_loss: 0.2071 - val_accuracy: 0.9500\n",
            "Epoch 149/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9650\n",
            "Epoch 149: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0964 - accuracy: 0.9650 - val_loss: 0.2024 - val_accuracy: 0.9500\n",
            "Epoch 150/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0840 - accuracy: 0.9731\n",
            "Epoch 150: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0844 - accuracy: 0.9725 - val_loss: 0.2036 - val_accuracy: 0.9533\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9671\n",
            "Epoch 151: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0853 - accuracy: 0.9675 - val_loss: 0.2037 - val_accuracy: 0.9500\n",
            "Epoch 152/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9758\n",
            "Epoch 152: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0776 - accuracy: 0.9758 - val_loss: 0.2108 - val_accuracy: 0.9500\n",
            "Epoch 153/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9708\n",
            "Epoch 153: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.0862 - accuracy: 0.9708 - val_loss: 0.2097 - val_accuracy: 0.9500\n",
            "Epoch 154/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9671\n",
            "Epoch 154: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0980 - accuracy: 0.9675 - val_loss: 0.2139 - val_accuracy: 0.9500\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9730\n",
            "Epoch 155: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0875 - accuracy: 0.9733 - val_loss: 0.2034 - val_accuracy: 0.9467\n",
            "Epoch 156/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0868 - accuracy: 0.9696\n",
            "Epoch 156: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0868 - accuracy: 0.9692 - val_loss: 0.2053 - val_accuracy: 0.9467\n",
            "Epoch 157/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9667\n",
            "Epoch 157: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0854 - accuracy: 0.9667 - val_loss: 0.2061 - val_accuracy: 0.9500\n",
            "Epoch 158/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9671\n",
            "Epoch 158: val_loss did not improve from 0.19929\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 0.2053 - val_accuracy: 0.9533\n",
            "Epoch 159/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9789\n",
            "Epoch 159: val_loss improved from 0.19929 to 0.19793, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 130ms/step - loss: 0.0723 - accuracy: 0.9792 - val_loss: 0.1979 - val_accuracy: 0.9533\n",
            "Epoch 160/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0774 - accuracy: 0.9721\n",
            "Epoch 160: val_loss improved from 0.19793 to 0.19650, saving model to /content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0765 - accuracy: 0.9725 - val_loss: 0.1965 - val_accuracy: 0.9500\n",
            "Epoch 161/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9730\n",
            "Epoch 161: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0739 - accuracy: 0.9733 - val_loss: 0.2066 - val_accuracy: 0.9533\n",
            "Epoch 162/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9708\n",
            "Epoch 162: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0744 - accuracy: 0.9708 - val_loss: 0.2092 - val_accuracy: 0.9533\n",
            "Epoch 163/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9789\n",
            "Epoch 163: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0700 - accuracy: 0.9783 - val_loss: 0.2033 - val_accuracy: 0.9500\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9688\n",
            "Epoch 164: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 63ms/step - loss: 0.0885 - accuracy: 0.9692 - val_loss: 0.2090 - val_accuracy: 0.9500\n",
            "Epoch 165/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9742\n",
            "Epoch 165: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0712 - accuracy: 0.9742 - val_loss: 0.2073 - val_accuracy: 0.9500\n",
            "Epoch 166/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9789\n",
            "Epoch 166: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0750 - accuracy: 0.9792 - val_loss: 0.2031 - val_accuracy: 0.9467\n",
            "Epoch 167/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0758 - accuracy: 0.9714\n",
            "Epoch 167: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0758 - accuracy: 0.9717 - val_loss: 0.2013 - val_accuracy: 0.9500\n",
            "Epoch 168/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0686 - accuracy: 0.9766\n",
            "Epoch 168: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0674 - accuracy: 0.9775 - val_loss: 0.2081 - val_accuracy: 0.9533\n",
            "Epoch 169/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0772 - accuracy: 0.9748\n",
            "Epoch 169: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0787 - accuracy: 0.9750 - val_loss: 0.2063 - val_accuracy: 0.9500\n",
            "Epoch 170/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0642 - accuracy: 0.9800\n",
            "Epoch 170: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.2068 - val_accuracy: 0.9500\n",
            "Epoch 171/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9783\n",
            "Epoch 171: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0650 - accuracy: 0.9783 - val_loss: 0.2124 - val_accuracy: 0.9467\n",
            "Epoch 172/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9775\n",
            "Epoch 172: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0738 - accuracy: 0.9775 - val_loss: 0.2142 - val_accuracy: 0.9467\n",
            "Epoch 173/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0788 - accuracy: 0.9704\n",
            "Epoch 173: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0791 - accuracy: 0.9700 - val_loss: 0.2116 - val_accuracy: 0.9500\n",
            "Epoch 174/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9700\n",
            "Epoch 174: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0877 - accuracy: 0.9700 - val_loss: 0.2122 - val_accuracy: 0.9500\n",
            "Epoch 175/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9797\n",
            "Epoch 175: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0625 - accuracy: 0.9792 - val_loss: 0.2112 - val_accuracy: 0.9533\n",
            "Epoch 176/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9748\n",
            "Epoch 176: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0662 - accuracy: 0.9750 - val_loss: 0.2166 - val_accuracy: 0.9500\n",
            "Epoch 177/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9747\n",
            "Epoch 177: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0775 - accuracy: 0.9742 - val_loss: 0.2045 - val_accuracy: 0.9500\n",
            "Epoch 178/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9767\n",
            "Epoch 178: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0725 - accuracy: 0.9767 - val_loss: 0.2019 - val_accuracy: 0.9467\n",
            "Epoch 179/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0746 - accuracy: 0.9766\n",
            "Epoch 179: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0726 - accuracy: 0.9775 - val_loss: 0.2002 - val_accuracy: 0.9467\n",
            "Epoch 180/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9800\n",
            "Epoch 180: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0570 - accuracy: 0.9800 - val_loss: 0.2034 - val_accuracy: 0.9533\n",
            "Epoch 181/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9800\n",
            "Epoch 181: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0634 - accuracy: 0.9800 - val_loss: 0.2039 - val_accuracy: 0.9533\n",
            "Epoch 182/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0726 - accuracy: 0.9740\n",
            "Epoch 182: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0732 - accuracy: 0.9733 - val_loss: 0.2012 - val_accuracy: 0.9500\n",
            "Epoch 183/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9825\n",
            "Epoch 183: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0535 - accuracy: 0.9825 - val_loss: 0.2010 - val_accuracy: 0.9467\n",
            "Epoch 184/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9814\n",
            "Epoch 184: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 3s 76ms/step - loss: 0.0603 - accuracy: 0.9817 - val_loss: 0.2050 - val_accuracy: 0.9500\n",
            "Epoch 185/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0630 - accuracy: 0.9789\n",
            "Epoch 185: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0635 - accuracy: 0.9783 - val_loss: 0.2028 - val_accuracy: 0.9500\n",
            "Epoch 186/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0621 - accuracy: 0.9818\n",
            "Epoch 186: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0634 - accuracy: 0.9808 - val_loss: 0.2116 - val_accuracy: 0.9500\n",
            "Epoch 187/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9758\n",
            "Epoch 187: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0657 - accuracy: 0.9758 - val_loss: 0.2094 - val_accuracy: 0.9467\n",
            "Epoch 188/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9800\n",
            "Epoch 188: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0638 - accuracy: 0.9800 - val_loss: 0.2037 - val_accuracy: 0.9467\n",
            "Epoch 189/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9789\n",
            "Epoch 189: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 0.2037 - val_accuracy: 0.9500\n",
            "Epoch 190/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0602 - accuracy: 0.9818\n",
            "Epoch 190: val_loss did not improve from 0.19650\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0601 - accuracy: 0.9817 - val_loss: 0.2091 - val_accuracy: 0.9500\n",
            "47/47 [==============================] - 1s 15ms/step - loss: 0.4184 - accuracy: 0.8920\n",
            "Test accuracy, 17 run, after finetuning: 0.8920000195503235\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.2808 - accuracy: 0.6358\n",
            "Epoch 1: val_loss improved from inf to 1.40948, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 44s 546ms/step - loss: 2.2808 - accuracy: 0.6358 - val_loss: 1.4095 - val_accuracy: 0.7333\n",
            "Epoch 2/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.4250 - accuracy: 0.7002\n",
            "Epoch 2: val_loss improved from 1.40948 to 0.93918, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 1.4199 - accuracy: 0.7017 - val_loss: 0.9392 - val_accuracy: 0.7633\n",
            "Epoch 3/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.0705 - accuracy: 0.7399\n",
            "Epoch 3: val_loss improved from 0.93918 to 0.71333, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 128ms/step - loss: 1.0691 - accuracy: 0.7392 - val_loss: 0.7133 - val_accuracy: 0.8000\n",
            "Epoch 4/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.8587 - accuracy: 0.7608\n",
            "Epoch 4: val_loss improved from 0.71333 to 0.61202, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.8587 - accuracy: 0.7608 - val_loss: 0.6120 - val_accuracy: 0.8200\n",
            "Epoch 5/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7635\n",
            "Epoch 5: val_loss improved from 0.61202 to 0.56286, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.7738 - accuracy: 0.7633 - val_loss: 0.5629 - val_accuracy: 0.8200\n",
            "Epoch 6/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7310 - accuracy: 0.7694\n",
            "Epoch 6: val_loss improved from 0.56286 to 0.52980, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.7232 - accuracy: 0.7717 - val_loss: 0.5298 - val_accuracy: 0.8333\n",
            "Epoch 7/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6856 - accuracy: 0.7770\n",
            "Epoch 7: val_loss improved from 0.52980 to 0.50434, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.6882 - accuracy: 0.7750 - val_loss: 0.5043 - val_accuracy: 0.8333\n",
            "Epoch 8/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.8067\n",
            "Epoch 8: val_loss improved from 0.50434 to 0.48816, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.6451 - accuracy: 0.8067 - val_loss: 0.4882 - val_accuracy: 0.8400\n",
            "Epoch 9/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5976 - accuracy: 0.8133\n",
            "Epoch 9: val_loss improved from 0.48816 to 0.47104, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 7s 178ms/step - loss: 0.6026 - accuracy: 0.8125 - val_loss: 0.4710 - val_accuracy: 0.8567\n",
            "Epoch 10/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.8050\n",
            "Epoch 10: val_loss improved from 0.47104 to 0.45425, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 8s 207ms/step - loss: 0.6188 - accuracy: 0.8050 - val_loss: 0.4542 - val_accuracy: 0.8567\n",
            "Epoch 11/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5774 - accuracy: 0.7942\n",
            "Epoch 11: val_loss improved from 0.45425 to 0.44580, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.5774 - accuracy: 0.7942 - val_loss: 0.4458 - val_accuracy: 0.8567\n",
            "Epoch 12/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5330 - accuracy: 0.8074\n",
            "Epoch 12: val_loss improved from 0.44580 to 0.43397, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.5361 - accuracy: 0.8067 - val_loss: 0.4340 - val_accuracy: 0.8633\n",
            "Epoch 13/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5165 - accuracy: 0.8220\n",
            "Epoch 13: val_loss improved from 0.43397 to 0.42430, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.5068 - accuracy: 0.8267 - val_loss: 0.4243 - val_accuracy: 0.8633\n",
            "Epoch 14/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.5355 - accuracy: 0.8281\n",
            "Epoch 14: val_loss improved from 0.42430 to 0.41498, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.5407 - accuracy: 0.8233 - val_loss: 0.4150 - val_accuracy: 0.8633\n",
            "Epoch 15/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5153 - accuracy: 0.8193\n",
            "Epoch 15: val_loss improved from 0.41498 to 0.40588, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.5236 - accuracy: 0.8167 - val_loss: 0.4059 - val_accuracy: 0.8667\n",
            "Epoch 16/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.8192\n",
            "Epoch 16: val_loss improved from 0.40588 to 0.39815, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.5221 - accuracy: 0.8192 - val_loss: 0.3981 - val_accuracy: 0.8700\n",
            "Epoch 17/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.8225\n",
            "Epoch 17: val_loss improved from 0.39815 to 0.38850, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.5005 - accuracy: 0.8225 - val_loss: 0.3885 - val_accuracy: 0.8767\n",
            "Epoch 18/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.8294\n",
            "Epoch 18: val_loss improved from 0.38850 to 0.38119, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.4804 - accuracy: 0.8283 - val_loss: 0.3812 - val_accuracy: 0.8767\n",
            "Epoch 19/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.8450\n",
            "Epoch 19: val_loss improved from 0.38119 to 0.37587, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4452 - accuracy: 0.8450 - val_loss: 0.3759 - val_accuracy: 0.8800\n",
            "Epoch 20/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.8395\n",
            "Epoch 20: val_loss improved from 0.37587 to 0.36882, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.4647 - accuracy: 0.8400 - val_loss: 0.3688 - val_accuracy: 0.8800\n",
            "Epoch 21/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.8442\n",
            "Epoch 21: val_loss improved from 0.36882 to 0.36255, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.4421 - accuracy: 0.8442 - val_loss: 0.3626 - val_accuracy: 0.8833\n",
            "Epoch 22/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8458\n",
            "Epoch 22: val_loss improved from 0.36255 to 0.35521, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.4158 - accuracy: 0.8458 - val_loss: 0.3552 - val_accuracy: 0.8833\n",
            "Epoch 23/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4540 - accuracy: 0.8300\n",
            "Epoch 23: val_loss improved from 0.35521 to 0.35488, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.4540 - accuracy: 0.8300 - val_loss: 0.3549 - val_accuracy: 0.8800\n",
            "Epoch 24/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4314 - accuracy: 0.8385\n",
            "Epoch 24: val_loss improved from 0.35488 to 0.34622, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.4337 - accuracy: 0.8375 - val_loss: 0.3462 - val_accuracy: 0.8833\n",
            "Epoch 25/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8542\n",
            "Epoch 25: val_loss improved from 0.34622 to 0.34235, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.4136 - accuracy: 0.8542 - val_loss: 0.3423 - val_accuracy: 0.8833\n",
            "Epoch 26/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4071 - accuracy: 0.8585\n",
            "Epoch 26: val_loss improved from 0.34235 to 0.33845, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.4011 - accuracy: 0.8600 - val_loss: 0.3385 - val_accuracy: 0.8833\n",
            "Epoch 27/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3829 - accuracy: 0.8611\n",
            "Epoch 27: val_loss improved from 0.33845 to 0.33452, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.3830 - accuracy: 0.8617 - val_loss: 0.3345 - val_accuracy: 0.8833\n",
            "Epoch 28/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.8708\n",
            "Epoch 28: val_loss improved from 0.33452 to 0.32767, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.3719 - accuracy: 0.8708 - val_loss: 0.3277 - val_accuracy: 0.8800\n",
            "Epoch 29/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3696 - accuracy: 0.8585\n",
            "Epoch 29: val_loss improved from 0.32767 to 0.32274, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.3654 - accuracy: 0.8583 - val_loss: 0.3227 - val_accuracy: 0.8867\n",
            "Epoch 30/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8667\n",
            "Epoch 30: val_loss improved from 0.32274 to 0.32051, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3684 - accuracy: 0.8667 - val_loss: 0.3205 - val_accuracy: 0.8933\n",
            "Epoch 31/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3713 - accuracy: 0.8741\n",
            "Epoch 31: val_loss improved from 0.32051 to 0.31882, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3682 - accuracy: 0.8733 - val_loss: 0.3188 - val_accuracy: 0.8933\n",
            "Epoch 32/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3563 - accuracy: 0.8724\n",
            "Epoch 32: val_loss improved from 0.31882 to 0.31331, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.3536 - accuracy: 0.8742 - val_loss: 0.3133 - val_accuracy: 0.8933\n",
            "Epoch 33/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.8725\n",
            "Epoch 33: val_loss improved from 0.31331 to 0.30935, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3539 - accuracy: 0.8725 - val_loss: 0.3094 - val_accuracy: 0.9000\n",
            "Epoch 34/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3413 - accuracy: 0.8828\n",
            "Epoch 34: val_loss improved from 0.30935 to 0.30603, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.3325 - accuracy: 0.8858 - val_loss: 0.3060 - val_accuracy: 0.9000\n",
            "Epoch 35/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.8826\n",
            "Epoch 35: val_loss improved from 0.30603 to 0.30370, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.3301 - accuracy: 0.8817 - val_loss: 0.3037 - val_accuracy: 0.9100\n",
            "Epoch 36/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8742\n",
            "Epoch 36: val_loss improved from 0.30370 to 0.30024, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.3395 - accuracy: 0.8742 - val_loss: 0.3002 - val_accuracy: 0.9100\n",
            "Epoch 37/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3185 - accuracy: 0.8828\n",
            "Epoch 37: val_loss improved from 0.30024 to 0.29608, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3187 - accuracy: 0.8833 - val_loss: 0.2961 - val_accuracy: 0.9067\n",
            "Epoch 38/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8792\n",
            "Epoch 38: val_loss improved from 0.29608 to 0.29195, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.3195 - accuracy: 0.8792 - val_loss: 0.2919 - val_accuracy: 0.9033\n",
            "Epoch 39/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8801\n",
            "Epoch 39: val_loss did not improve from 0.29195\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.3180 - accuracy: 0.8800 - val_loss: 0.2958 - val_accuracy: 0.9067\n",
            "Epoch 40/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3320 - accuracy: 0.8750\n",
            "Epoch 40: val_loss did not improve from 0.29195\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.3297 - accuracy: 0.8758 - val_loss: 0.2930 - val_accuracy: 0.9000\n",
            "Epoch 41/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.8875\n",
            "Epoch 41: val_loss improved from 0.29195 to 0.28770, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 146ms/step - loss: 0.3088 - accuracy: 0.8875 - val_loss: 0.2877 - val_accuracy: 0.9000\n",
            "Epoch 42/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.8933\n",
            "Epoch 42: val_loss improved from 0.28770 to 0.28335, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3038 - accuracy: 0.8933 - val_loss: 0.2833 - val_accuracy: 0.9033\n",
            "Epoch 43/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.8902\n",
            "Epoch 43: val_loss did not improve from 0.28335\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.2987 - accuracy: 0.8883 - val_loss: 0.2840 - val_accuracy: 0.9033\n",
            "Epoch 44/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.8961\n",
            "Epoch 44: val_loss did not improve from 0.28335\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.2889 - accuracy: 0.8942 - val_loss: 0.2845 - val_accuracy: 0.9100\n",
            "Epoch 45/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.8950\n",
            "Epoch 45: val_loss improved from 0.28335 to 0.28224, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 128ms/step - loss: 0.2926 - accuracy: 0.8950 - val_loss: 0.2822 - val_accuracy: 0.9067\n",
            "Epoch 46/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8967\n",
            "Epoch 46: val_loss improved from 0.28224 to 0.28155, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2940 - accuracy: 0.8967 - val_loss: 0.2816 - val_accuracy: 0.9100\n",
            "Epoch 47/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2719 - accuracy: 0.9054\n",
            "Epoch 47: val_loss improved from 0.28155 to 0.27826, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2716 - accuracy: 0.9067 - val_loss: 0.2783 - val_accuracy: 0.9133\n",
            "Epoch 48/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2912 - accuracy: 0.8906\n",
            "Epoch 48: val_loss improved from 0.27826 to 0.27609, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.2868 - accuracy: 0.8925 - val_loss: 0.2761 - val_accuracy: 0.9100\n",
            "Epoch 49/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2802 - accuracy: 0.8984\n",
            "Epoch 49: val_loss improved from 0.27609 to 0.27336, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2873 - accuracy: 0.8967 - val_loss: 0.2734 - val_accuracy: 0.9133\n",
            "Epoch 50/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.8933\n",
            "Epoch 50: val_loss improved from 0.27336 to 0.27188, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2893 - accuracy: 0.8933 - val_loss: 0.2719 - val_accuracy: 0.9100\n",
            "Epoch 51/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9008\n",
            "Epoch 51: val_loss improved from 0.27188 to 0.26992, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.2674 - accuracy: 0.9008 - val_loss: 0.2699 - val_accuracy: 0.9133\n",
            "Epoch 52/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.9037\n",
            "Epoch 52: val_loss improved from 0.26992 to 0.26850, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.2649 - accuracy: 0.9050 - val_loss: 0.2685 - val_accuracy: 0.9133\n",
            "Epoch 53/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2842 - accuracy: 0.8958\n",
            "Epoch 53: val_loss improved from 0.26850 to 0.26569, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2832 - accuracy: 0.8950 - val_loss: 0.2657 - val_accuracy: 0.9133\n",
            "Epoch 54/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9117\n",
            "Epoch 54: val_loss improved from 0.26569 to 0.26554, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.2515 - accuracy: 0.9117 - val_loss: 0.2655 - val_accuracy: 0.9133\n",
            "Epoch 55/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9033\n",
            "Epoch 55: val_loss improved from 0.26554 to 0.26311, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.2491 - accuracy: 0.9033 - val_loss: 0.2631 - val_accuracy: 0.9133\n",
            "Epoch 56/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9029\n",
            "Epoch 56: val_loss improved from 0.26311 to 0.26185, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2476 - accuracy: 0.9033 - val_loss: 0.2619 - val_accuracy: 0.9167\n",
            "Epoch 57/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2267 - accuracy: 0.9227\n",
            "Epoch 57: val_loss improved from 0.26185 to 0.25986, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2248 - accuracy: 0.9233 - val_loss: 0.2599 - val_accuracy: 0.9167\n",
            "Epoch 58/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9046\n",
            "Epoch 58: val_loss improved from 0.25986 to 0.25771, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2592 - accuracy: 0.9050 - val_loss: 0.2577 - val_accuracy: 0.9100\n",
            "Epoch 59/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9079\n",
            "Epoch 59: val_loss improved from 0.25771 to 0.25719, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.2521 - accuracy: 0.9092 - val_loss: 0.2572 - val_accuracy: 0.9133\n",
            "Epoch 60/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9150\n",
            "Epoch 60: val_loss improved from 0.25719 to 0.25639, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2514 - accuracy: 0.9150 - val_loss: 0.2564 - val_accuracy: 0.9133\n",
            "Epoch 61/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9217\n",
            "Epoch 61: val_loss did not improve from 0.25639\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2228 - accuracy: 0.9217 - val_loss: 0.2568 - val_accuracy: 0.9167\n",
            "Epoch 62/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9092\n",
            "Epoch 62: val_loss improved from 0.25639 to 0.25229, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2326 - accuracy: 0.9092 - val_loss: 0.2523 - val_accuracy: 0.9167\n",
            "Epoch 63/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9130\n",
            "Epoch 63: val_loss did not improve from 0.25229\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.2493 - accuracy: 0.9133 - val_loss: 0.2543 - val_accuracy: 0.9167\n",
            "Epoch 64/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9258\n",
            "Epoch 64: val_loss improved from 0.25229 to 0.24574, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.2278 - accuracy: 0.9258 - val_loss: 0.2457 - val_accuracy: 0.9167\n",
            "Epoch 65/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2380 - accuracy: 0.9219\n",
            "Epoch 65: val_loss did not improve from 0.24574\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2354 - accuracy: 0.9217 - val_loss: 0.2459 - val_accuracy: 0.9167\n",
            "Epoch 66/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9192\n",
            "Epoch 66: val_loss improved from 0.24574 to 0.24534, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.2495 - accuracy: 0.9192 - val_loss: 0.2453 - val_accuracy: 0.9200\n",
            "Epoch 67/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9183\n",
            "Epoch 67: val_loss did not improve from 0.24534\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2358 - accuracy: 0.9183 - val_loss: 0.2477 - val_accuracy: 0.9167\n",
            "Epoch 68/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2356 - accuracy: 0.9175\n",
            "Epoch 68: val_loss improved from 0.24534 to 0.24464, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2379 - accuracy: 0.9150 - val_loss: 0.2446 - val_accuracy: 0.9167\n",
            "Epoch 69/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9267\n",
            "Epoch 69: val_loss improved from 0.24464 to 0.24130, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.2008 - accuracy: 0.9267 - val_loss: 0.2413 - val_accuracy: 0.9133\n",
            "Epoch 70/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9225\n",
            "Epoch 70: val_loss improved from 0.24130 to 0.24026, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2173 - accuracy: 0.9225 - val_loss: 0.2403 - val_accuracy: 0.9133\n",
            "Epoch 71/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9267\n",
            "Epoch 71: val_loss improved from 0.24026 to 0.24018, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2048 - accuracy: 0.9267 - val_loss: 0.2402 - val_accuracy: 0.9200\n",
            "Epoch 72/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9240\n",
            "Epoch 72: val_loss improved from 0.24018 to 0.23405, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 119ms/step - loss: 0.2147 - accuracy: 0.9250 - val_loss: 0.2340 - val_accuracy: 0.9233\n",
            "Epoch 73/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9200\n",
            "Epoch 73: val_loss did not improve from 0.23405\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2236 - accuracy: 0.9200 - val_loss: 0.2348 - val_accuracy: 0.9233\n",
            "Epoch 74/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2321 - accuracy: 0.9105\n",
            "Epoch 74: val_loss improved from 0.23405 to 0.23213, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2344 - accuracy: 0.9108 - val_loss: 0.2321 - val_accuracy: 0.9233\n",
            "Epoch 75/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9324\n",
            "Epoch 75: val_loss did not improve from 0.23213\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2010 - accuracy: 0.9325 - val_loss: 0.2339 - val_accuracy: 0.9167\n",
            "Epoch 76/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9308\n",
            "Epoch 76: val_loss improved from 0.23213 to 0.23109, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.1948 - accuracy: 0.9308 - val_loss: 0.2311 - val_accuracy: 0.9233\n",
            "Epoch 77/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1851 - accuracy: 0.9227\n",
            "Epoch 77: val_loss improved from 0.23109 to 0.22907, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1834 - accuracy: 0.9242 - val_loss: 0.2291 - val_accuracy: 0.9233\n",
            "Epoch 78/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1864 - accuracy: 0.9292\n",
            "Epoch 78: val_loss improved from 0.22907 to 0.22697, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1864 - accuracy: 0.9292 - val_loss: 0.2270 - val_accuracy: 0.9233\n",
            "Epoch 79/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1900 - accuracy: 0.9307\n",
            "Epoch 79: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1909 - accuracy: 0.9292 - val_loss: 0.2323 - val_accuracy: 0.9233\n",
            "Epoch 80/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9358\n",
            "Epoch 80: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1783 - accuracy: 0.9358 - val_loss: 0.2283 - val_accuracy: 0.9233\n",
            "Epoch 81/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9265\n",
            "Epoch 81: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 3s 73ms/step - loss: 0.1946 - accuracy: 0.9258 - val_loss: 0.2283 - val_accuracy: 0.9233\n",
            "Epoch 82/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9333\n",
            "Epoch 82: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1979 - accuracy: 0.9333 - val_loss: 0.2301 - val_accuracy: 0.9233\n",
            "Epoch 83/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9400\n",
            "Epoch 83: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 3s 73ms/step - loss: 0.1707 - accuracy: 0.9400 - val_loss: 0.2299 - val_accuracy: 0.9267\n",
            "Epoch 84/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9383\n",
            "Epoch 84: val_loss did not improve from 0.22697\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1829 - accuracy: 0.9392 - val_loss: 0.2286 - val_accuracy: 0.9200\n",
            "Epoch 85/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1857 - accuracy: 0.9409\n",
            "Epoch 85: val_loss improved from 0.22697 to 0.22619, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.1852 - accuracy: 0.9400 - val_loss: 0.2262 - val_accuracy: 0.9267\n",
            "Epoch 86/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9392\n",
            "Epoch 86: val_loss did not improve from 0.22619\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1782 - accuracy: 0.9392 - val_loss: 0.2304 - val_accuracy: 0.9233\n",
            "Epoch 87/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9300\n",
            "Epoch 87: val_loss improved from 0.22619 to 0.22306, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.1842 - accuracy: 0.9300 - val_loss: 0.2231 - val_accuracy: 0.9233\n",
            "Epoch 88/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1697 - accuracy: 0.9350\n",
            "Epoch 88: val_loss improved from 0.22306 to 0.22206, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.1704 - accuracy: 0.9342 - val_loss: 0.2221 - val_accuracy: 0.9233\n",
            "Epoch 89/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9291\n",
            "Epoch 89: val_loss did not improve from 0.22206\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1775 - accuracy: 0.9283 - val_loss: 0.2294 - val_accuracy: 0.9267\n",
            "Epoch 90/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1650 - accuracy: 0.9418\n",
            "Epoch 90: val_loss did not improve from 0.22206\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1676 - accuracy: 0.9408 - val_loss: 0.2253 - val_accuracy: 0.9267\n",
            "Epoch 91/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9257\n",
            "Epoch 91: val_loss improved from 0.22206 to 0.21641, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.1871 - accuracy: 0.9258 - val_loss: 0.2164 - val_accuracy: 0.9267\n",
            "Epoch 92/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9383\n",
            "Epoch 92: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1674 - accuracy: 0.9383 - val_loss: 0.2214 - val_accuracy: 0.9300\n",
            "Epoch 93/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9400\n",
            "Epoch 93: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1658 - accuracy: 0.9400 - val_loss: 0.2225 - val_accuracy: 0.9267\n",
            "Epoch 94/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1541 - accuracy: 0.9544\n",
            "Epoch 94: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.1551 - accuracy: 0.9542 - val_loss: 0.2243 - val_accuracy: 0.9267\n",
            "Epoch 95/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9592\n",
            "Epoch 95: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.1395 - accuracy: 0.9592 - val_loss: 0.2187 - val_accuracy: 0.9267\n",
            "Epoch 96/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9409\n",
            "Epoch 96: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1641 - accuracy: 0.9417 - val_loss: 0.2240 - val_accuracy: 0.9267\n",
            "Epoch 97/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9467\n",
            "Epoch 97: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1647 - accuracy: 0.9467 - val_loss: 0.2240 - val_accuracy: 0.9300\n",
            "Epoch 98/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9358\n",
            "Epoch 98: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1657 - accuracy: 0.9367 - val_loss: 0.2265 - val_accuracy: 0.9267\n",
            "Epoch 99/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9475\n",
            "Epoch 99: val_loss did not improve from 0.21641\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1478 - accuracy: 0.9475 - val_loss: 0.2182 - val_accuracy: 0.9267\n",
            "Epoch 100/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9475\n",
            "Epoch 100: val_loss improved from 0.21641 to 0.21403, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 137ms/step - loss: 0.1498 - accuracy: 0.9475 - val_loss: 0.2140 - val_accuracy: 0.9267\n",
            "Epoch 101/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9409\n",
            "Epoch 101: val_loss did not improve from 0.21403\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1613 - accuracy: 0.9417 - val_loss: 0.2153 - val_accuracy: 0.9267\n",
            "Epoch 102/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9375\n",
            "Epoch 102: val_loss did not improve from 0.21403\n",
            "38/38 [==============================] - 2s 64ms/step - loss: 0.1652 - accuracy: 0.9375 - val_loss: 0.2148 - val_accuracy: 0.9267\n",
            "Epoch 103/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9475\n",
            "Epoch 103: val_loss did not improve from 0.21403\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1486 - accuracy: 0.9475 - val_loss: 0.2147 - val_accuracy: 0.9233\n",
            "Epoch 104/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.9470\n",
            "Epoch 104: val_loss improved from 0.21403 to 0.20852, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1431 - accuracy: 0.9458 - val_loss: 0.2085 - val_accuracy: 0.9267\n",
            "Epoch 105/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9492\n",
            "Epoch 105: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1323 - accuracy: 0.9492 - val_loss: 0.2112 - val_accuracy: 0.9267\n",
            "Epoch 106/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9459\n",
            "Epoch 106: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1471 - accuracy: 0.9467 - val_loss: 0.2125 - val_accuracy: 0.9267\n",
            "Epoch 107/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9535\n",
            "Epoch 107: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1250 - accuracy: 0.9542 - val_loss: 0.2141 - val_accuracy: 0.9300\n",
            "Epoch 108/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9458\n",
            "Epoch 108: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1407 - accuracy: 0.9458 - val_loss: 0.2156 - val_accuracy: 0.9300\n",
            "Epoch 109/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.9497\n",
            "Epoch 109: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1351 - accuracy: 0.9483 - val_loss: 0.2102 - val_accuracy: 0.9267\n",
            "Epoch 110/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9586\n",
            "Epoch 110: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1316 - accuracy: 0.9592 - val_loss: 0.2098 - val_accuracy: 0.9300\n",
            "Epoch 111/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9508\n",
            "Epoch 111: val_loss did not improve from 0.20852\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1411 - accuracy: 0.9508 - val_loss: 0.2145 - val_accuracy: 0.9333\n",
            "Epoch 112/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9468\n",
            "Epoch 112: val_loss improved from 0.20852 to 0.20510, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 138ms/step - loss: 0.1396 - accuracy: 0.9458 - val_loss: 0.2051 - val_accuracy: 0.9300\n",
            "Epoch 113/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.9493\n",
            "Epoch 113: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1378 - accuracy: 0.9483 - val_loss: 0.2089 - val_accuracy: 0.9267\n",
            "Epoch 114/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9475\n",
            "Epoch 114: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1422 - accuracy: 0.9475 - val_loss: 0.2172 - val_accuracy: 0.9300\n",
            "Epoch 115/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9508\n",
            "Epoch 115: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1403 - accuracy: 0.9508 - val_loss: 0.2095 - val_accuracy: 0.9300\n",
            "Epoch 116/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1257 - accuracy: 0.9531\n",
            "Epoch 116: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1247 - accuracy: 0.9542 - val_loss: 0.2084 - val_accuracy: 0.9300\n",
            "Epoch 117/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9567\n",
            "Epoch 117: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1304 - accuracy: 0.9567 - val_loss: 0.2109 - val_accuracy: 0.9300\n",
            "Epoch 118/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9578\n",
            "Epoch 118: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.1255 - accuracy: 0.9583 - val_loss: 0.2099 - val_accuracy: 0.9300\n",
            "Epoch 119/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9426\n",
            "Epoch 119: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1451 - accuracy: 0.9417 - val_loss: 0.2137 - val_accuracy: 0.9267\n",
            "Epoch 120/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1127 - accuracy: 0.9557\n",
            "Epoch 120: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.1130 - accuracy: 0.9550 - val_loss: 0.2111 - val_accuracy: 0.9300\n",
            "Epoch 121/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9533\n",
            "Epoch 121: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1323 - accuracy: 0.9533 - val_loss: 0.2061 - val_accuracy: 0.9267\n",
            "Epoch 122/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9705\n",
            "Epoch 122: val_loss did not improve from 0.20510\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.1073 - accuracy: 0.9700 - val_loss: 0.2060 - val_accuracy: 0.9267\n",
            "Epoch 123/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9540\n",
            "Epoch 123: val_loss improved from 0.20510 to 0.20496, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 130ms/step - loss: 0.1318 - accuracy: 0.9542 - val_loss: 0.2050 - val_accuracy: 0.9300\n",
            "Epoch 124/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9578\n",
            "Epoch 124: val_loss improved from 0.20496 to 0.19988, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.1184 - accuracy: 0.9567 - val_loss: 0.1999 - val_accuracy: 0.9333\n",
            "Epoch 125/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9517\n",
            "Epoch 125: val_loss did not improve from 0.19988\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1207 - accuracy: 0.9517 - val_loss: 0.2046 - val_accuracy: 0.9300\n",
            "Epoch 126/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9552\n",
            "Epoch 126: val_loss did not improve from 0.19988\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1191 - accuracy: 0.9542 - val_loss: 0.2044 - val_accuracy: 0.9333\n",
            "Epoch 127/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9583\n",
            "Epoch 127: val_loss did not improve from 0.19988\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.2096 - val_accuracy: 0.9300\n",
            "Epoch 128/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9603\n",
            "Epoch 128: val_loss did not improve from 0.19988\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1070 - accuracy: 0.9608 - val_loss: 0.2036 - val_accuracy: 0.9267\n",
            "Epoch 129/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9642\n",
            "Epoch 129: val_loss improved from 0.19988 to 0.19670, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 123ms/step - loss: 0.1023 - accuracy: 0.9642 - val_loss: 0.1967 - val_accuracy: 0.9267\n",
            "Epoch 130/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9600\n",
            "Epoch 130: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.1098 - accuracy: 0.9600 - val_loss: 0.2015 - val_accuracy: 0.9300\n",
            "Epoch 131/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9642\n",
            "Epoch 131: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0937 - accuracy: 0.9642 - val_loss: 0.2080 - val_accuracy: 0.9300\n",
            "Epoch 132/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9595\n",
            "Epoch 132: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1129 - accuracy: 0.9600 - val_loss: 0.2032 - val_accuracy: 0.9333\n",
            "Epoch 133/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9650\n",
            "Epoch 133: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1172 - accuracy: 0.9650 - val_loss: 0.2059 - val_accuracy: 0.9333\n",
            "Epoch 134/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9544\n",
            "Epoch 134: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 3s 67ms/step - loss: 0.1202 - accuracy: 0.9550 - val_loss: 0.2015 - val_accuracy: 0.9333\n",
            "Epoch 135/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9569\n",
            "Epoch 135: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1093 - accuracy: 0.9575 - val_loss: 0.2034 - val_accuracy: 0.9300\n",
            "Epoch 136/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9662\n",
            "Epoch 136: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.1006 - accuracy: 0.9650 - val_loss: 0.2029 - val_accuracy: 0.9300\n",
            "Epoch 137/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1077 - accuracy: 0.9627\n",
            "Epoch 137: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1067 - accuracy: 0.9642 - val_loss: 0.1998 - val_accuracy: 0.9300\n",
            "Epoch 138/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9662\n",
            "Epoch 138: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0963 - accuracy: 0.9650 - val_loss: 0.2026 - val_accuracy: 0.9300\n",
            "Epoch 139/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9683\n",
            "Epoch 139: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0909 - accuracy: 0.9683 - val_loss: 0.1975 - val_accuracy: 0.9300\n",
            "Epoch 140/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9608\n",
            "Epoch 140: val_loss did not improve from 0.19670\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1019 - accuracy: 0.9608 - val_loss: 0.2048 - val_accuracy: 0.9233\n",
            "Epoch 141/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9567\n",
            "Epoch 141: val_loss improved from 0.19670 to 0.19397, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.1142 - accuracy: 0.9567 - val_loss: 0.1940 - val_accuracy: 0.9333\n",
            "Epoch 142/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1066 - accuracy: 0.9618\n",
            "Epoch 142: val_loss did not improve from 0.19397\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1096 - accuracy: 0.9600 - val_loss: 0.2020 - val_accuracy: 0.9333\n",
            "Epoch 143/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0927 - accuracy: 0.9661\n",
            "Epoch 143: val_loss did not improve from 0.19397\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.0951 - accuracy: 0.9642 - val_loss: 0.1967 - val_accuracy: 0.9333\n",
            "Epoch 144/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9650\n",
            "Epoch 144: val_loss did not improve from 0.19397\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.0951 - accuracy: 0.9650 - val_loss: 0.1995 - val_accuracy: 0.9300\n",
            "Epoch 145/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9633\n",
            "Epoch 145: val_loss did not improve from 0.19397\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1023 - accuracy: 0.9633 - val_loss: 0.2008 - val_accuracy: 0.9267\n",
            "Epoch 146/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9617\n",
            "Epoch 146: val_loss improved from 0.19397 to 0.19377, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 146ms/step - loss: 0.0919 - accuracy: 0.9617 - val_loss: 0.1938 - val_accuracy: 0.9300\n",
            "Epoch 147/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9721\n",
            "Epoch 147: val_loss did not improve from 0.19377\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0865 - accuracy: 0.9725 - val_loss: 0.1981 - val_accuracy: 0.9333\n",
            "Epoch 148/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9696\n",
            "Epoch 148: val_loss did not improve from 0.19377\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0922 - accuracy: 0.9692 - val_loss: 0.1997 - val_accuracy: 0.9333\n",
            "Epoch 149/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9713\n",
            "Epoch 149: val_loss improved from 0.19377 to 0.19360, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 134ms/step - loss: 0.0929 - accuracy: 0.9717 - val_loss: 0.1936 - val_accuracy: 0.9300\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9654\n",
            "Epoch 150: val_loss did not improve from 0.19360\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0873 - accuracy: 0.9650 - val_loss: 0.1945 - val_accuracy: 0.9333\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9654\n",
            "Epoch 151: val_loss did not improve from 0.19360\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1002 - accuracy: 0.9658 - val_loss: 0.1990 - val_accuracy: 0.9333\n",
            "Epoch 152/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9767\n",
            "Epoch 152: val_loss improved from 0.19360 to 0.19209, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0759 - accuracy: 0.9767 - val_loss: 0.1921 - val_accuracy: 0.9300\n",
            "Epoch 153/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9683\n",
            "Epoch 153: val_loss improved from 0.19209 to 0.18807, saving model to /content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 132ms/step - loss: 0.0832 - accuracy: 0.9683 - val_loss: 0.1881 - val_accuracy: 0.9300\n",
            "Epoch 154/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9708\n",
            "Epoch 154: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0798 - accuracy: 0.9708 - val_loss: 0.2014 - val_accuracy: 0.9300\n",
            "Epoch 155/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9742\n",
            "Epoch 155: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0757 - accuracy: 0.9742 - val_loss: 0.1918 - val_accuracy: 0.9300\n",
            "Epoch 156/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9708\n",
            "Epoch 156: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0778 - accuracy: 0.9708 - val_loss: 0.1922 - val_accuracy: 0.9333\n",
            "Epoch 157/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9772\n",
            "Epoch 157: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0704 - accuracy: 0.9775 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
            "Epoch 158/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9667\n",
            "Epoch 158: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.1018 - accuracy: 0.9667 - val_loss: 0.2009 - val_accuracy: 0.9300\n",
            "Epoch 159/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9713\n",
            "Epoch 159: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0842 - accuracy: 0.9708 - val_loss: 0.1960 - val_accuracy: 0.9333\n",
            "Epoch 160/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9692\n",
            "Epoch 160: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0810 - accuracy: 0.9692 - val_loss: 0.1932 - val_accuracy: 0.9333\n",
            "Epoch 161/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9658\n",
            "Epoch 161: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.0903 - accuracy: 0.9658 - val_loss: 0.1894 - val_accuracy: 0.9367\n",
            "Epoch 162/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9696\n",
            "Epoch 162: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.0870 - accuracy: 0.9700 - val_loss: 0.1969 - val_accuracy: 0.9300\n",
            "Epoch 163/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9767\n",
            "Epoch 163: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0767 - accuracy: 0.9767 - val_loss: 0.1946 - val_accuracy: 0.9300\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9730\n",
            "Epoch 164: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0930 - accuracy: 0.9733 - val_loss: 0.1965 - val_accuracy: 0.9300\n",
            "Epoch 165/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9764\n",
            "Epoch 165: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0843 - accuracy: 0.9742 - val_loss: 0.1938 - val_accuracy: 0.9300\n",
            "Epoch 166/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9767\n",
            "Epoch 166: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0693 - accuracy: 0.9767 - val_loss: 0.1957 - val_accuracy: 0.9300\n",
            "Epoch 167/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9704\n",
            "Epoch 167: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.0705 - accuracy: 0.9700 - val_loss: 0.1969 - val_accuracy: 0.9300\n",
            "Epoch 168/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9721\n",
            "Epoch 168: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.0793 - accuracy: 0.9725 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
            "Epoch 169/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9747\n",
            "Epoch 169: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0637 - accuracy: 0.9750 - val_loss: 0.1951 - val_accuracy: 0.9333\n",
            "Epoch 170/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9700\n",
            "Epoch 170: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0755 - accuracy: 0.9700 - val_loss: 0.1927 - val_accuracy: 0.9300\n",
            "Epoch 171/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9683\n",
            "Epoch 171: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0895 - accuracy: 0.9683 - val_loss: 0.1979 - val_accuracy: 0.9300\n",
            "Epoch 172/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9721\n",
            "Epoch 172: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0700 - accuracy: 0.9725 - val_loss: 0.2006 - val_accuracy: 0.9300\n",
            "Epoch 173/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9783\n",
            "Epoch 173: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0650 - accuracy: 0.9783 - val_loss: 0.1984 - val_accuracy: 0.9300\n",
            "Epoch 174/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9738\n",
            "Epoch 174: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0722 - accuracy: 0.9742 - val_loss: 0.1964 - val_accuracy: 0.9300\n",
            "Epoch 175/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9775\n",
            "Epoch 175: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 56ms/step - loss: 0.0701 - accuracy: 0.9775 - val_loss: 0.1973 - val_accuracy: 0.9367\n",
            "Epoch 176/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0757 - accuracy: 0.9714\n",
            "Epoch 176: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0740 - accuracy: 0.9725 - val_loss: 0.2086 - val_accuracy: 0.9300\n",
            "Epoch 177/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9780\n",
            "Epoch 177: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0643 - accuracy: 0.9783 - val_loss: 0.1897 - val_accuracy: 0.9367\n",
            "Epoch 178/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9696\n",
            "Epoch 178: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0818 - accuracy: 0.9700 - val_loss: 0.1975 - val_accuracy: 0.9333\n",
            "Epoch 179/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9738\n",
            "Epoch 179: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0775 - accuracy: 0.9742 - val_loss: 0.2024 - val_accuracy: 0.9267\n",
            "Epoch 180/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9780\n",
            "Epoch 180: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0665 - accuracy: 0.9783 - val_loss: 0.1946 - val_accuracy: 0.9333\n",
            "Epoch 181/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9823\n",
            "Epoch 181: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0575 - accuracy: 0.9825 - val_loss: 0.1920 - val_accuracy: 0.9333\n",
            "Epoch 182/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9730\n",
            "Epoch 182: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0749 - accuracy: 0.9725 - val_loss: 0.2045 - val_accuracy: 0.9300\n",
            "Epoch 183/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9783\n",
            "Epoch 183: val_loss did not improve from 0.18807\n",
            "38/38 [==============================] - 2s 54ms/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 0.2033 - val_accuracy: 0.9267\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.3804 - accuracy: 0.8907\n",
            "Test accuracy, 18 run, after finetuning: 0.890666663646698\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.2569 - accuracy: 0.6542\n",
            "Epoch 1: val_loss improved from inf to 1.74385, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 45s 550ms/step - loss: 2.2569 - accuracy: 0.6542 - val_loss: 1.7439 - val_accuracy: 0.7000\n",
            "Epoch 2/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 1.5205 - accuracy: 0.7133\n",
            "Epoch 2: val_loss improved from 1.74385 to 1.19987, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 1.5205 - accuracy: 0.7133 - val_loss: 1.1999 - val_accuracy: 0.7300\n",
            "Epoch 3/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 1.0489 - accuracy: 0.7417\n",
            "Epoch 3: val_loss improved from 1.19987 to 0.89643, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 1.0489 - accuracy: 0.7417 - val_loss: 0.8964 - val_accuracy: 0.7600\n",
            "Epoch 4/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.9239 - accuracy: 0.7677\n",
            "Epoch 4: val_loss improved from 0.89643 to 0.76141, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.9157 - accuracy: 0.7700 - val_loss: 0.7614 - val_accuracy: 0.7800\n",
            "Epoch 5/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7780 - accuracy: 0.7821\n",
            "Epoch 5: val_loss improved from 0.76141 to 0.67100, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.7966 - accuracy: 0.7808 - val_loss: 0.6710 - val_accuracy: 0.7967\n",
            "Epoch 6/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7898 - accuracy: 0.7956\n",
            "Epoch 6: val_loss improved from 0.67100 to 0.61913, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.7872 - accuracy: 0.7958 - val_loss: 0.6191 - val_accuracy: 0.8000\n",
            "Epoch 7/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.7983\n",
            "Epoch 7: val_loss improved from 0.61913 to 0.58605, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.6844 - accuracy: 0.7983 - val_loss: 0.5861 - val_accuracy: 0.8033\n",
            "Epoch 8/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6641 - accuracy: 0.8167\n",
            "Epoch 8: val_loss improved from 0.58605 to 0.55555, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.6716 - accuracy: 0.8150 - val_loss: 0.5556 - val_accuracy: 0.8100\n",
            "Epoch 9/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6775 - accuracy: 0.8116\n",
            "Epoch 9: val_loss improved from 0.55555 to 0.52833, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.6729 - accuracy: 0.8125 - val_loss: 0.5283 - val_accuracy: 0.8167\n",
            "Epoch 10/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.8217\n",
            "Epoch 10: val_loss improved from 0.52833 to 0.50805, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 7s 197ms/step - loss: 0.6028 - accuracy: 0.8217 - val_loss: 0.5080 - val_accuracy: 0.8267\n",
            "Epoch 11/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6063 - accuracy: 0.8167\n",
            "Epoch 11: val_loss improved from 0.50805 to 0.48763, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 6s 170ms/step - loss: 0.6063 - accuracy: 0.8167 - val_loss: 0.4876 - val_accuracy: 0.8333\n",
            "Epoch 12/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5696 - accuracy: 0.8226\n",
            "Epoch 12: val_loss improved from 0.48763 to 0.47415, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.5656 - accuracy: 0.8242 - val_loss: 0.4742 - val_accuracy: 0.8333\n",
            "Epoch 13/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5609 - accuracy: 0.8218\n",
            "Epoch 13: val_loss improved from 0.47415 to 0.46462, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.5587 - accuracy: 0.8217 - val_loss: 0.4646 - val_accuracy: 0.8333\n",
            "Epoch 14/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5388 - accuracy: 0.8342\n",
            "Epoch 14: val_loss improved from 0.46462 to 0.45891, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.5388 - accuracy: 0.8342 - val_loss: 0.4589 - val_accuracy: 0.8400\n",
            "Epoch 15/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5287 - accuracy: 0.8328\n",
            "Epoch 15: val_loss improved from 0.45891 to 0.44659, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.5260 - accuracy: 0.8325 - val_loss: 0.4466 - val_accuracy: 0.8400\n",
            "Epoch 16/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.8433\n",
            "Epoch 16: val_loss improved from 0.44659 to 0.43424, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.5034 - accuracy: 0.8433 - val_loss: 0.4342 - val_accuracy: 0.8433\n",
            "Epoch 17/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.8325\n",
            "Epoch 17: val_loss improved from 0.43424 to 0.42409, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.4908 - accuracy: 0.8325 - val_loss: 0.4241 - val_accuracy: 0.8467\n",
            "Epoch 18/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4625 - accuracy: 0.8455\n",
            "Epoch 18: val_loss improved from 0.42409 to 0.40997, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.4680 - accuracy: 0.8450 - val_loss: 0.4100 - val_accuracy: 0.8533\n",
            "Epoch 19/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4718 - accuracy: 0.8497\n",
            "Epoch 19: val_loss improved from 0.40997 to 0.40117, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.4675 - accuracy: 0.8500 - val_loss: 0.4012 - val_accuracy: 0.8500\n",
            "Epoch 20/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4766 - accuracy: 0.8403\n",
            "Epoch 20: val_loss improved from 0.40117 to 0.40010, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.4754 - accuracy: 0.8417 - val_loss: 0.4001 - val_accuracy: 0.8500\n",
            "Epoch 21/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8575\n",
            "Epoch 21: val_loss improved from 0.40010 to 0.38646, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.4339 - accuracy: 0.8575 - val_loss: 0.3865 - val_accuracy: 0.8567\n",
            "Epoch 22/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.8590\n",
            "Epoch 22: val_loss improved from 0.38646 to 0.37705, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.4372 - accuracy: 0.8592 - val_loss: 0.3771 - val_accuracy: 0.8567\n",
            "Epoch 23/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4501 - accuracy: 0.8516\n",
            "Epoch 23: val_loss improved from 0.37705 to 0.37258, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.4534 - accuracy: 0.8467 - val_loss: 0.3726 - val_accuracy: 0.8567\n",
            "Epoch 24/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4528 - accuracy: 0.8429\n",
            "Epoch 24: val_loss improved from 0.37258 to 0.36058, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.4552 - accuracy: 0.8417 - val_loss: 0.3606 - val_accuracy: 0.8633\n",
            "Epoch 25/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4150 - accuracy: 0.8550\n",
            "Epoch 25: val_loss did not improve from 0.36058\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.4049 - accuracy: 0.8575 - val_loss: 0.3655 - val_accuracy: 0.8600\n",
            "Epoch 26/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4173 - accuracy: 0.8585\n",
            "Epoch 26: val_loss improved from 0.36058 to 0.35732, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 123ms/step - loss: 0.4147 - accuracy: 0.8575 - val_loss: 0.3573 - val_accuracy: 0.8600\n",
            "Epoch 27/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3966 - accuracy: 0.8606\n",
            "Epoch 27: val_loss improved from 0.35732 to 0.34822, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.4037 - accuracy: 0.8600 - val_loss: 0.3482 - val_accuracy: 0.8633\n",
            "Epoch 28/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3600 - accuracy: 0.8801\n",
            "Epoch 28: val_loss improved from 0.34822 to 0.34439, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.3602 - accuracy: 0.8783 - val_loss: 0.3444 - val_accuracy: 0.8633\n",
            "Epoch 29/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8750\n",
            "Epoch 29: val_loss improved from 0.34439 to 0.34110, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3775 - accuracy: 0.8750 - val_loss: 0.3411 - val_accuracy: 0.8667\n",
            "Epoch 30/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3721 - accuracy: 0.8733\n",
            "Epoch 30: val_loss improved from 0.34110 to 0.33768, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3707 - accuracy: 0.8733 - val_loss: 0.3377 - val_accuracy: 0.8633\n",
            "Epoch 31/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3723 - accuracy: 0.8800\n",
            "Epoch 31: val_loss improved from 0.33768 to 0.33668, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.3723 - accuracy: 0.8800 - val_loss: 0.3367 - val_accuracy: 0.8700\n",
            "Epoch 32/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8725\n",
            "Epoch 32: val_loss improved from 0.33668 to 0.32931, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.3775 - accuracy: 0.8717 - val_loss: 0.3293 - val_accuracy: 0.8667\n",
            "Epoch 33/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.8885\n",
            "Epoch 33: val_loss improved from 0.32931 to 0.32490, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.3400 - accuracy: 0.8892 - val_loss: 0.3249 - val_accuracy: 0.8633\n",
            "Epoch 34/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.8725\n",
            "Epoch 34: val_loss did not improve from 0.32490\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.3733 - accuracy: 0.8725 - val_loss: 0.3253 - val_accuracy: 0.8733\n",
            "Epoch 35/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3491 - accuracy: 0.8811\n",
            "Epoch 35: val_loss improved from 0.32490 to 0.32130, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.3473 - accuracy: 0.8817 - val_loss: 0.3213 - val_accuracy: 0.8733\n",
            "Epoch 36/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3295 - accuracy: 0.8976\n",
            "Epoch 36: val_loss improved from 0.32130 to 0.31405, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.3333 - accuracy: 0.8967 - val_loss: 0.3140 - val_accuracy: 0.8733\n",
            "Epoch 37/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.8725\n",
            "Epoch 37: val_loss did not improve from 0.31405\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.3521 - accuracy: 0.8725 - val_loss: 0.3156 - val_accuracy: 0.8767\n",
            "Epoch 38/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3368 - accuracy: 0.8845\n",
            "Epoch 38: val_loss improved from 0.31405 to 0.30682, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.3397 - accuracy: 0.8850 - val_loss: 0.3068 - val_accuracy: 0.8767\n",
            "Epoch 39/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8767\n",
            "Epoch 39: val_loss improved from 0.30682 to 0.29791, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.3213 - accuracy: 0.8767 - val_loss: 0.2979 - val_accuracy: 0.8767\n",
            "Epoch 40/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3259 - accuracy: 0.8880\n",
            "Epoch 40: val_loss did not improve from 0.29791\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.3309 - accuracy: 0.8875 - val_loss: 0.3007 - val_accuracy: 0.8767\n",
            "Epoch 41/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3218 - accuracy: 0.8850\n",
            "Epoch 41: val_loss improved from 0.29791 to 0.29245, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.3218 - accuracy: 0.8850 - val_loss: 0.2924 - val_accuracy: 0.8767\n",
            "Epoch 42/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.8919\n",
            "Epoch 42: val_loss did not improve from 0.29245\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.3157 - accuracy: 0.8900 - val_loss: 0.2925 - val_accuracy: 0.8767\n",
            "Epoch 43/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8933\n",
            "Epoch 43: val_loss improved from 0.29245 to 0.29227, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2964 - accuracy: 0.8933 - val_loss: 0.2923 - val_accuracy: 0.8833\n",
            "Epoch 44/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2835 - accuracy: 0.8950\n",
            "Epoch 44: val_loss improved from 0.29227 to 0.29104, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.2846 - accuracy: 0.8942 - val_loss: 0.2910 - val_accuracy: 0.8833\n",
            "Epoch 45/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2842 - accuracy: 0.9054\n",
            "Epoch 45: val_loss improved from 0.29104 to 0.28711, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.2864 - accuracy: 0.9042 - val_loss: 0.2871 - val_accuracy: 0.8833\n",
            "Epoch 46/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8927\n",
            "Epoch 46: val_loss improved from 0.28711 to 0.27966, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3028 - accuracy: 0.8933 - val_loss: 0.2797 - val_accuracy: 0.8800\n",
            "Epoch 47/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.9020\n",
            "Epoch 47: val_loss improved from 0.27966 to 0.27548, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2795 - accuracy: 0.9017 - val_loss: 0.2755 - val_accuracy: 0.8900\n",
            "Epoch 48/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.9033\n",
            "Epoch 48: val_loss improved from 0.27548 to 0.27423, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.2819 - accuracy: 0.9033 - val_loss: 0.2742 - val_accuracy: 0.8900\n",
            "Epoch 49/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2743 - accuracy: 0.8993\n",
            "Epoch 49: val_loss did not improve from 0.27423\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2760 - accuracy: 0.9000 - val_loss: 0.2748 - val_accuracy: 0.8933\n",
            "Epoch 50/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8995\n",
            "Epoch 50: val_loss improved from 0.27423 to 0.26860, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2924 - accuracy: 0.8992 - val_loss: 0.2686 - val_accuracy: 0.8867\n",
            "Epoch 51/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2714 - accuracy: 0.8978\n",
            "Epoch 51: val_loss improved from 0.26860 to 0.26342, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2723 - accuracy: 0.8983 - val_loss: 0.2634 - val_accuracy: 0.8933\n",
            "Epoch 52/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2777 - accuracy: 0.9123\n",
            "Epoch 52: val_loss improved from 0.26342 to 0.26014, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 129ms/step - loss: 0.2741 - accuracy: 0.9125 - val_loss: 0.2601 - val_accuracy: 0.8967\n",
            "Epoch 53/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8995\n",
            "Epoch 53: val_loss improved from 0.26014 to 0.25804, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2839 - accuracy: 0.8992 - val_loss: 0.2580 - val_accuracy: 0.9000\n",
            "Epoch 54/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2558 - accuracy: 0.9122\n",
            "Epoch 54: val_loss did not improve from 0.25804\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.2556 - accuracy: 0.9125 - val_loss: 0.2603 - val_accuracy: 0.9067\n",
            "Epoch 55/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2618 - accuracy: 0.9097\n",
            "Epoch 55: val_loss improved from 0.25804 to 0.25675, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2581 - accuracy: 0.9100 - val_loss: 0.2567 - val_accuracy: 0.9067\n",
            "Epoch 56/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2485 - accuracy: 0.9184\n",
            "Epoch 56: val_loss did not improve from 0.25675\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.2502 - accuracy: 0.9192 - val_loss: 0.2568 - val_accuracy: 0.9067\n",
            "Epoch 57/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2467 - accuracy: 0.9175\n",
            "Epoch 57: val_loss improved from 0.25675 to 0.25548, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.2505 - accuracy: 0.9175 - val_loss: 0.2555 - val_accuracy: 0.9067\n",
            "Epoch 58/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9175\n",
            "Epoch 58: val_loss did not improve from 0.25548\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2293 - accuracy: 0.9175 - val_loss: 0.2568 - val_accuracy: 0.9067\n",
            "Epoch 59/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9088\n",
            "Epoch 59: val_loss improved from 0.25548 to 0.24929, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2480 - accuracy: 0.9083 - val_loss: 0.2493 - val_accuracy: 0.9067\n",
            "Epoch 60/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9092\n",
            "Epoch 60: val_loss improved from 0.24929 to 0.24720, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2414 - accuracy: 0.9092 - val_loss: 0.2472 - val_accuracy: 0.9067\n",
            "Epoch 61/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9029\n",
            "Epoch 61: val_loss did not improve from 0.24720\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.2578 - accuracy: 0.9033 - val_loss: 0.2473 - val_accuracy: 0.9067\n",
            "Epoch 62/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9164\n",
            "Epoch 62: val_loss improved from 0.24720 to 0.24502, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2538 - accuracy: 0.9158 - val_loss: 0.2450 - val_accuracy: 0.9067\n",
            "Epoch 63/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9142\n",
            "Epoch 63: val_loss improved from 0.24502 to 0.24375, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2365 - accuracy: 0.9142 - val_loss: 0.2438 - val_accuracy: 0.9100\n",
            "Epoch 64/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.9267\n",
            "Epoch 64: val_loss improved from 0.24375 to 0.24144, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 126ms/step - loss: 0.2246 - accuracy: 0.9267 - val_loss: 0.2414 - val_accuracy: 0.9067\n",
            "Epoch 65/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9189\n",
            "Epoch 65: val_loss improved from 0.24144 to 0.23785, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.2210 - accuracy: 0.9192 - val_loss: 0.2378 - val_accuracy: 0.9067\n",
            "Epoch 66/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9267\n",
            "Epoch 66: val_loss improved from 0.23785 to 0.23611, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2016 - accuracy: 0.9267 - val_loss: 0.2361 - val_accuracy: 0.9100\n",
            "Epoch 67/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9142\n",
            "Epoch 67: val_loss did not improve from 0.23611\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2261 - accuracy: 0.9142 - val_loss: 0.2363 - val_accuracy: 0.9133\n",
            "Epoch 68/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9283\n",
            "Epoch 68: val_loss improved from 0.23611 to 0.23270, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2050 - accuracy: 0.9283 - val_loss: 0.2327 - val_accuracy: 0.9100\n",
            "Epoch 69/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9125\n",
            "Epoch 69: val_loss did not improve from 0.23270\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.2365 - accuracy: 0.9125 - val_loss: 0.2339 - val_accuracy: 0.9067\n",
            "Epoch 70/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2230 - accuracy: 0.9201\n",
            "Epoch 70: val_loss improved from 0.23270 to 0.23028, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.2256 - accuracy: 0.9183 - val_loss: 0.2303 - val_accuracy: 0.9133\n",
            "Epoch 71/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9198\n",
            "Epoch 71: val_loss did not improve from 0.23028\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2324 - accuracy: 0.9183 - val_loss: 0.2327 - val_accuracy: 0.9100\n",
            "Epoch 72/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9292\n",
            "Epoch 72: val_loss improved from 0.23028 to 0.22520, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.2200 - accuracy: 0.9292 - val_loss: 0.2252 - val_accuracy: 0.9100\n",
            "Epoch 73/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2260 - accuracy: 0.9236\n",
            "Epoch 73: val_loss improved from 0.22520 to 0.22323, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.2274 - accuracy: 0.9225 - val_loss: 0.2232 - val_accuracy: 0.9067\n",
            "Epoch 74/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9258\n",
            "Epoch 74: val_loss improved from 0.22323 to 0.22235, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.2081 - accuracy: 0.9258 - val_loss: 0.2224 - val_accuracy: 0.9100\n",
            "Epoch 75/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9317\n",
            "Epoch 75: val_loss did not improve from 0.22235\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.2022 - accuracy: 0.9317 - val_loss: 0.2247 - val_accuracy: 0.9100\n",
            "Epoch 76/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2068 - accuracy: 0.9280\n",
            "Epoch 76: val_loss did not improve from 0.22235\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.2077 - accuracy: 0.9267 - val_loss: 0.2238 - val_accuracy: 0.9133\n",
            "Epoch 77/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9307\n",
            "Epoch 77: val_loss improved from 0.22235 to 0.22086, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.1992 - accuracy: 0.9317 - val_loss: 0.2209 - val_accuracy: 0.9100\n",
            "Epoch 78/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9358\n",
            "Epoch 78: val_loss improved from 0.22086 to 0.21686, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1838 - accuracy: 0.9358 - val_loss: 0.2169 - val_accuracy: 0.9100\n",
            "Epoch 79/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9316\n",
            "Epoch 79: val_loss did not improve from 0.21686\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.1956 - accuracy: 0.9325 - val_loss: 0.2180 - val_accuracy: 0.9167\n",
            "Epoch 80/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9333\n",
            "Epoch 80: val_loss improved from 0.21686 to 0.21622, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1814 - accuracy: 0.9333 - val_loss: 0.2162 - val_accuracy: 0.9133\n",
            "Epoch 81/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9383\n",
            "Epoch 81: val_loss improved from 0.21622 to 0.21495, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.1775 - accuracy: 0.9383 - val_loss: 0.2149 - val_accuracy: 0.9133\n",
            "Epoch 82/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9375\n",
            "Epoch 82: val_loss did not improve from 0.21495\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1782 - accuracy: 0.9375 - val_loss: 0.2188 - val_accuracy: 0.9167\n",
            "Epoch 83/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9383\n",
            "Epoch 83: val_loss did not improve from 0.21495\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1844 - accuracy: 0.9383 - val_loss: 0.2155 - val_accuracy: 0.9067\n",
            "Epoch 84/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9325\n",
            "Epoch 84: val_loss improved from 0.21495 to 0.21456, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1801 - accuracy: 0.9325 - val_loss: 0.2146 - val_accuracy: 0.9167\n",
            "Epoch 85/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9291\n",
            "Epoch 85: val_loss improved from 0.21456 to 0.21193, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1807 - accuracy: 0.9292 - val_loss: 0.2119 - val_accuracy: 0.9167\n",
            "Epoch 86/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9324\n",
            "Epoch 86: val_loss improved from 0.21193 to 0.20941, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1891 - accuracy: 0.9325 - val_loss: 0.2094 - val_accuracy: 0.9233\n",
            "Epoch 87/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9308\n",
            "Epoch 87: val_loss did not improve from 0.20941\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1852 - accuracy: 0.9308 - val_loss: 0.2116 - val_accuracy: 0.9200\n",
            "Epoch 88/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1770 - accuracy: 0.9349\n",
            "Epoch 88: val_loss did not improve from 0.20941\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1778 - accuracy: 0.9358 - val_loss: 0.2106 - val_accuracy: 0.9133\n",
            "Epoch 89/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9317\n",
            "Epoch 89: val_loss improved from 0.20941 to 0.20808, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1840 - accuracy: 0.9317 - val_loss: 0.2081 - val_accuracy: 0.9167\n",
            "Epoch 90/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.9324\n",
            "Epoch 90: val_loss did not improve from 0.20808\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1720 - accuracy: 0.9333 - val_loss: 0.2085 - val_accuracy: 0.9200\n",
            "Epoch 91/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9458\n",
            "Epoch 91: val_loss improved from 0.20808 to 0.20615, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1621 - accuracy: 0.9458 - val_loss: 0.2061 - val_accuracy: 0.9200\n",
            "Epoch 92/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1579 - accuracy: 0.9462\n",
            "Epoch 92: val_loss did not improve from 0.20615\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1604 - accuracy: 0.9450 - val_loss: 0.2088 - val_accuracy: 0.9200\n",
            "Epoch 93/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1619 - accuracy: 0.9462\n",
            "Epoch 93: val_loss improved from 0.20615 to 0.20444, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.1629 - accuracy: 0.9450 - val_loss: 0.2044 - val_accuracy: 0.9133\n",
            "Epoch 94/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9333\n",
            "Epoch 94: val_loss improved from 0.20444 to 0.20383, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.1756 - accuracy: 0.9333 - val_loss: 0.2038 - val_accuracy: 0.9233\n",
            "Epoch 95/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9350\n",
            "Epoch 95: val_loss did not improve from 0.20383\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1724 - accuracy: 0.9350 - val_loss: 0.2041 - val_accuracy: 0.9200\n",
            "Epoch 96/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9451\n",
            "Epoch 96: val_loss improved from 0.20383 to 0.20176, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.1571 - accuracy: 0.9442 - val_loss: 0.2018 - val_accuracy: 0.9233\n",
            "Epoch 97/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1599 - accuracy: 0.9417\n",
            "Epoch 97: val_loss did not improve from 0.20176\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1610 - accuracy: 0.9417 - val_loss: 0.2048 - val_accuracy: 0.9233\n",
            "Epoch 98/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.9470\n",
            "Epoch 98: val_loss did not improve from 0.20176\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1586 - accuracy: 0.9475 - val_loss: 0.2026 - val_accuracy: 0.9300\n",
            "Epoch 99/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1642 - accuracy: 0.9418\n",
            "Epoch 99: val_loss did not improve from 0.20176\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1646 - accuracy: 0.9408 - val_loss: 0.2018 - val_accuracy: 0.9200\n",
            "Epoch 100/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9417\n",
            "Epoch 100: val_loss improved from 0.20176 to 0.19932, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.1596 - accuracy: 0.9425 - val_loss: 0.1993 - val_accuracy: 0.9233\n",
            "Epoch 101/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1686 - accuracy: 0.9436\n",
            "Epoch 101: val_loss did not improve from 0.19932\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1655 - accuracy: 0.9450 - val_loss: 0.2002 - val_accuracy: 0.9200\n",
            "Epoch 102/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9417\n",
            "Epoch 102: val_loss did not improve from 0.19932\n",
            "38/38 [==============================] - 2s 66ms/step - loss: 0.1570 - accuracy: 0.9417 - val_loss: 0.2028 - val_accuracy: 0.9200\n",
            "Epoch 103/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9508\n",
            "Epoch 103: val_loss did not improve from 0.19932\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1423 - accuracy: 0.9508 - val_loss: 0.2002 - val_accuracy: 0.9267\n",
            "Epoch 104/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9476\n",
            "Epoch 104: val_loss improved from 0.19932 to 0.19512, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.1557 - accuracy: 0.9475 - val_loss: 0.1951 - val_accuracy: 0.9267\n",
            "Epoch 105/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9519\n",
            "Epoch 105: val_loss did not improve from 0.19512\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1381 - accuracy: 0.9525 - val_loss: 0.1981 - val_accuracy: 0.9267\n",
            "Epoch 106/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9467\n",
            "Epoch 106: val_loss did not improve from 0.19512\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1395 - accuracy: 0.9467 - val_loss: 0.1973 - val_accuracy: 0.9233\n",
            "Epoch 107/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1589 - accuracy: 0.9479\n",
            "Epoch 107: val_loss improved from 0.19512 to 0.19353, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.1552 - accuracy: 0.9492 - val_loss: 0.1935 - val_accuracy: 0.9300\n",
            "Epoch 108/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9493\n",
            "Epoch 108: val_loss improved from 0.19353 to 0.19051, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.1416 - accuracy: 0.9492 - val_loss: 0.1905 - val_accuracy: 0.9367\n",
            "Epoch 109/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9608\n",
            "Epoch 109: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.1336 - accuracy: 0.9608 - val_loss: 0.1948 - val_accuracy: 0.9233\n",
            "Epoch 110/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9550\n",
            "Epoch 110: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.1375 - accuracy: 0.9550 - val_loss: 0.1962 - val_accuracy: 0.9267\n",
            "Epoch 111/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1428 - accuracy: 0.9603\n",
            "Epoch 111: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1413 - accuracy: 0.9608 - val_loss: 0.1934 - val_accuracy: 0.9267\n",
            "Epoch 112/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9476\n",
            "Epoch 112: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1432 - accuracy: 0.9475 - val_loss: 0.1926 - val_accuracy: 0.9267\n",
            "Epoch 113/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1425 - accuracy: 0.9459\n",
            "Epoch 113: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 59ms/step - loss: 0.1421 - accuracy: 0.9467 - val_loss: 0.1906 - val_accuracy: 0.9300\n",
            "Epoch 114/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9508\n",
            "Epoch 114: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1277 - accuracy: 0.9508 - val_loss: 0.1946 - val_accuracy: 0.9233\n",
            "Epoch 115/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9542\n",
            "Epoch 115: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1316 - accuracy: 0.9542 - val_loss: 0.1936 - val_accuracy: 0.9300\n",
            "Epoch 116/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9502\n",
            "Epoch 116: val_loss did not improve from 0.19051\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1347 - accuracy: 0.9508 - val_loss: 0.1918 - val_accuracy: 0.9300\n",
            "Epoch 117/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1332 - accuracy: 0.9557\n",
            "Epoch 117: val_loss improved from 0.19051 to 0.19012, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 132ms/step - loss: 0.1337 - accuracy: 0.9550 - val_loss: 0.1901 - val_accuracy: 0.9367\n",
            "Epoch 118/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9583\n",
            "Epoch 118: val_loss did not improve from 0.19012\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1178 - accuracy: 0.9592 - val_loss: 0.1923 - val_accuracy: 0.9300\n",
            "Epoch 119/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9578\n",
            "Epoch 119: val_loss did not improve from 0.19012\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1182 - accuracy: 0.9583 - val_loss: 0.1917 - val_accuracy: 0.9300\n",
            "Epoch 120/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9552\n",
            "Epoch 120: val_loss improved from 0.19012 to 0.18964, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.1218 - accuracy: 0.9558 - val_loss: 0.1896 - val_accuracy: 0.9333\n",
            "Epoch 121/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9542\n",
            "Epoch 121: val_loss did not improve from 0.18964\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1363 - accuracy: 0.9542 - val_loss: 0.1898 - val_accuracy: 0.9300\n",
            "Epoch 122/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9544\n",
            "Epoch 122: val_loss did not improve from 0.18964\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.1259 - accuracy: 0.9550 - val_loss: 0.1918 - val_accuracy: 0.9267\n",
            "Epoch 123/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9550\n",
            "Epoch 123: val_loss did not improve from 0.18964\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.1270 - accuracy: 0.9550 - val_loss: 0.1899 - val_accuracy: 0.9267\n",
            "Epoch 124/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9567\n",
            "Epoch 124: val_loss improved from 0.18964 to 0.18754, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.1189 - accuracy: 0.9567 - val_loss: 0.1875 - val_accuracy: 0.9300\n",
            "Epoch 125/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9470\n",
            "Epoch 125: val_loss did not improve from 0.18754\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1331 - accuracy: 0.9492 - val_loss: 0.1918 - val_accuracy: 0.9267\n",
            "Epoch 126/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9586\n",
            "Epoch 126: val_loss improved from 0.18754 to 0.18366, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1180 - accuracy: 0.9592 - val_loss: 0.1837 - val_accuracy: 0.9333\n",
            "Epoch 127/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9658\n",
            "Epoch 127: val_loss did not improve from 0.18366\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1126 - accuracy: 0.9658 - val_loss: 0.1865 - val_accuracy: 0.9333\n",
            "Epoch 128/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9679\n",
            "Epoch 128: val_loss did not improve from 0.18366\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1021 - accuracy: 0.9667 - val_loss: 0.1845 - val_accuracy: 0.9367\n",
            "Epoch 129/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9617\n",
            "Epoch 129: val_loss improved from 0.18366 to 0.18201, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.1086 - accuracy: 0.9617 - val_loss: 0.1820 - val_accuracy: 0.9333\n",
            "Epoch 130/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9527\n",
            "Epoch 130: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1265 - accuracy: 0.9517 - val_loss: 0.1832 - val_accuracy: 0.9300\n",
            "Epoch 131/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9617\n",
            "Epoch 131: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1128 - accuracy: 0.9617 - val_loss: 0.1854 - val_accuracy: 0.9233\n",
            "Epoch 132/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9692\n",
            "Epoch 132: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0914 - accuracy: 0.9692 - val_loss: 0.1851 - val_accuracy: 0.9267\n",
            "Epoch 133/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9704\n",
            "Epoch 133: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0934 - accuracy: 0.9708 - val_loss: 0.1836 - val_accuracy: 0.9267\n",
            "Epoch 134/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9575\n",
            "Epoch 134: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.1233 - accuracy: 0.9575 - val_loss: 0.1841 - val_accuracy: 0.9267\n",
            "Epoch 135/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1037 - accuracy: 0.9644\n",
            "Epoch 135: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 3s 76ms/step - loss: 0.1042 - accuracy: 0.9633 - val_loss: 0.1844 - val_accuracy: 0.9267\n",
            "Epoch 136/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9569\n",
            "Epoch 136: val_loss did not improve from 0.18201\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1181 - accuracy: 0.9567 - val_loss: 0.1824 - val_accuracy: 0.9267\n",
            "Epoch 137/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9683\n",
            "Epoch 137: val_loss improved from 0.18201 to 0.18174, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 130ms/step - loss: 0.1003 - accuracy: 0.9683 - val_loss: 0.1817 - val_accuracy: 0.9300\n",
            "Epoch 138/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9620\n",
            "Epoch 138: val_loss improved from 0.18174 to 0.17909, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.1016 - accuracy: 0.9617 - val_loss: 0.1791 - val_accuracy: 0.9300\n",
            "Epoch 139/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9642\n",
            "Epoch 139: val_loss improved from 0.17909 to 0.17861, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.1065 - accuracy: 0.9642 - val_loss: 0.1786 - val_accuracy: 0.9333\n",
            "Epoch 140/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9683\n",
            "Epoch 140: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0998 - accuracy: 0.9683 - val_loss: 0.1828 - val_accuracy: 0.9267\n",
            "Epoch 141/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9675\n",
            "Epoch 141: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 3s 75ms/step - loss: 0.0939 - accuracy: 0.9675 - val_loss: 0.1813 - val_accuracy: 0.9300\n",
            "Epoch 142/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0971 - accuracy: 0.9654\n",
            "Epoch 142: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0988 - accuracy: 0.9642 - val_loss: 0.1789 - val_accuracy: 0.9300\n",
            "Epoch 143/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9617\n",
            "Epoch 143: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.1045 - accuracy: 0.9617 - val_loss: 0.1841 - val_accuracy: 0.9233\n",
            "Epoch 144/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9662\n",
            "Epoch 144: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 3s 71ms/step - loss: 0.1136 - accuracy: 0.9667 - val_loss: 0.1796 - val_accuracy: 0.9267\n",
            "Epoch 145/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0938 - accuracy: 0.9662\n",
            "Epoch 145: val_loss did not improve from 0.17861\n",
            "38/38 [==============================] - 2s 58ms/step - loss: 0.0927 - accuracy: 0.9667 - val_loss: 0.1795 - val_accuracy: 0.9267\n",
            "Epoch 146/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9700\n",
            "Epoch 146: val_loss improved from 0.17861 to 0.17860, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 127ms/step - loss: 0.0940 - accuracy: 0.9700 - val_loss: 0.1786 - val_accuracy: 0.9333\n",
            "Epoch 147/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9671\n",
            "Epoch 147: val_loss did not improve from 0.17860\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0937 - accuracy: 0.9667 - val_loss: 0.1791 - val_accuracy: 0.9267\n",
            "Epoch 148/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9683\n",
            "Epoch 148: val_loss improved from 0.17860 to 0.17847, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.0900 - accuracy: 0.9683 - val_loss: 0.1785 - val_accuracy: 0.9300\n",
            "Epoch 149/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9611\n",
            "Epoch 149: val_loss improved from 0.17847 to 0.17644, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 132ms/step - loss: 0.1062 - accuracy: 0.9617 - val_loss: 0.1764 - val_accuracy: 0.9400\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9679\n",
            "Epoch 150: val_loss improved from 0.17644 to 0.17506, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0947 - accuracy: 0.9683 - val_loss: 0.1751 - val_accuracy: 0.9300\n",
            "Epoch 151/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9704\n",
            "Epoch 151: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0847 - accuracy: 0.9700 - val_loss: 0.1765 - val_accuracy: 0.9267\n",
            "Epoch 152/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9767\n",
            "Epoch 152: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0857 - accuracy: 0.9767 - val_loss: 0.1792 - val_accuracy: 0.9267\n",
            "Epoch 153/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9747\n",
            "Epoch 153: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 61ms/step - loss: 0.0902 - accuracy: 0.9742 - val_loss: 0.1796 - val_accuracy: 0.9233\n",
            "Epoch 154/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9692\n",
            "Epoch 154: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.0868 - accuracy: 0.9692 - val_loss: 0.1815 - val_accuracy: 0.9233\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9713\n",
            "Epoch 155: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0864 - accuracy: 0.9708 - val_loss: 0.1780 - val_accuracy: 0.9300\n",
            "Epoch 156/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9717\n",
            "Epoch 156: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0856 - accuracy: 0.9717 - val_loss: 0.1773 - val_accuracy: 0.9333\n",
            "Epoch 157/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9608\n",
            "Epoch 157: val_loss did not improve from 0.17506\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0967 - accuracy: 0.9608 - val_loss: 0.1772 - val_accuracy: 0.9233\n",
            "Epoch 158/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9725\n",
            "Epoch 158: val_loss improved from 0.17506 to 0.17505, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 143ms/step - loss: 0.0794 - accuracy: 0.9725 - val_loss: 0.1750 - val_accuracy: 0.9300\n",
            "Epoch 159/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9730\n",
            "Epoch 159: val_loss improved from 0.17505 to 0.17299, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.0763 - accuracy: 0.9733 - val_loss: 0.1730 - val_accuracy: 0.9333\n",
            "Epoch 160/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9742\n",
            "Epoch 160: val_loss improved from 0.17299 to 0.17258, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0786 - accuracy: 0.9742 - val_loss: 0.1726 - val_accuracy: 0.9367\n",
            "Epoch 161/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.9797\n",
            "Epoch 161: val_loss improved from 0.17258 to 0.17203, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.0698 - accuracy: 0.9792 - val_loss: 0.1720 - val_accuracy: 0.9433\n",
            "Epoch 162/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9750\n",
            "Epoch 162: val_loss improved from 0.17203 to 0.17200, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 130ms/step - loss: 0.0741 - accuracy: 0.9750 - val_loss: 0.1720 - val_accuracy: 0.9333\n",
            "Epoch 163/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9679\n",
            "Epoch 163: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0905 - accuracy: 0.9683 - val_loss: 0.1764 - val_accuracy: 0.9267\n",
            "Epoch 164/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9730\n",
            "Epoch 164: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0822 - accuracy: 0.9733 - val_loss: 0.1739 - val_accuracy: 0.9300\n",
            "Epoch 165/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.9755\n",
            "Epoch 165: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0770 - accuracy: 0.9750 - val_loss: 0.1739 - val_accuracy: 0.9333\n",
            "Epoch 166/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9772\n",
            "Epoch 166: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0792 - accuracy: 0.9758 - val_loss: 0.1740 - val_accuracy: 0.9333\n",
            "Epoch 167/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9764\n",
            "Epoch 167: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 3s 76ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.1721 - val_accuracy: 0.9367\n",
            "Epoch 168/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9747\n",
            "Epoch 168: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 60ms/step - loss: 0.0811 - accuracy: 0.9742 - val_loss: 0.1751 - val_accuracy: 0.9333\n",
            "Epoch 169/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9750\n",
            "Epoch 169: val_loss did not improve from 0.17200\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0755 - accuracy: 0.9750 - val_loss: 0.1746 - val_accuracy: 0.9333\n",
            "Epoch 170/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9755\n",
            "Epoch 170: val_loss improved from 0.17200 to 0.17118, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.1712 - val_accuracy: 0.9333\n",
            "Epoch 171/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0685 - accuracy: 0.9748\n",
            "Epoch 171: val_loss improved from 0.17118 to 0.16654, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.0678 - accuracy: 0.9750 - val_loss: 0.1665 - val_accuracy: 0.9367\n",
            "Epoch 172/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9758\n",
            "Epoch 172: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0719 - accuracy: 0.9758 - val_loss: 0.1714 - val_accuracy: 0.9333\n",
            "Epoch 173/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9738\n",
            "Epoch 173: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.1702 - val_accuracy: 0.9333\n",
            "Epoch 174/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9742\n",
            "Epoch 174: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0762 - accuracy: 0.9742 - val_loss: 0.1740 - val_accuracy: 0.9300\n",
            "Epoch 175/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9775\n",
            "Epoch 175: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0703 - accuracy: 0.9775 - val_loss: 0.1706 - val_accuracy: 0.9400\n",
            "Epoch 176/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9806\n",
            "Epoch 176: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 62ms/step - loss: 0.0679 - accuracy: 0.9808 - val_loss: 0.1751 - val_accuracy: 0.9300\n",
            "Epoch 177/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9858\n",
            "Epoch 177: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0569 - accuracy: 0.9858 - val_loss: 0.1727 - val_accuracy: 0.9300\n",
            "Epoch 178/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9688\n",
            "Epoch 178: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0834 - accuracy: 0.9692 - val_loss: 0.1754 - val_accuracy: 0.9333\n",
            "Epoch 179/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9750\n",
            "Epoch 179: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 3s 77ms/step - loss: 0.0733 - accuracy: 0.9750 - val_loss: 0.1699 - val_accuracy: 0.9400\n",
            "Epoch 180/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0632 - accuracy: 0.9774\n",
            "Epoch 180: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 52ms/step - loss: 0.0639 - accuracy: 0.9767 - val_loss: 0.1690 - val_accuracy: 0.9333\n",
            "Epoch 181/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0677 - accuracy: 0.9783\n",
            "Epoch 181: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0694 - accuracy: 0.9775 - val_loss: 0.1709 - val_accuracy: 0.9333\n",
            "Epoch 182/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9808\n",
            "Epoch 182: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0611 - accuracy: 0.9808 - val_loss: 0.1694 - val_accuracy: 0.9333\n",
            "Epoch 183/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9738\n",
            "Epoch 183: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0756 - accuracy: 0.9742 - val_loss: 0.1740 - val_accuracy: 0.9300\n",
            "Epoch 184/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9848\n",
            "Epoch 184: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0604 - accuracy: 0.9850 - val_loss: 0.1716 - val_accuracy: 0.9367\n",
            "Epoch 185/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9775\n",
            "Epoch 185: val_loss did not improve from 0.16654\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0737 - accuracy: 0.9775 - val_loss: 0.1697 - val_accuracy: 0.9367\n",
            "Epoch 186/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0751 - accuracy: 0.9722\n",
            "Epoch 186: val_loss improved from 0.16654 to 0.16610, saving model to /content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 136ms/step - loss: 0.0756 - accuracy: 0.9717 - val_loss: 0.1661 - val_accuracy: 0.9367\n",
            "Epoch 187/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9764\n",
            "Epoch 187: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0615 - accuracy: 0.9767 - val_loss: 0.1664 - val_accuracy: 0.9367\n",
            "Epoch 188/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9850\n",
            "Epoch 188: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 0.1677 - val_accuracy: 0.9333\n",
            "Epoch 189/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9783\n",
            "Epoch 189: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0649 - accuracy: 0.9783 - val_loss: 0.1717 - val_accuracy: 0.9300\n",
            "Epoch 190/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9750\n",
            "Epoch 190: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0702 - accuracy: 0.9750 - val_loss: 0.1724 - val_accuracy: 0.9367\n",
            "Epoch 191/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9775\n",
            "Epoch 191: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 66ms/step - loss: 0.0676 - accuracy: 0.9775 - val_loss: 0.1708 - val_accuracy: 0.9367\n",
            "Epoch 192/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9806\n",
            "Epoch 192: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0601 - accuracy: 0.9808 - val_loss: 0.1739 - val_accuracy: 0.9333\n",
            "Epoch 193/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0600 - accuracy: 0.9783\n",
            "Epoch 193: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0605 - accuracy: 0.9783 - val_loss: 0.1746 - val_accuracy: 0.9333\n",
            "Epoch 194/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0639 - accuracy: 0.9766\n",
            "Epoch 194: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 53ms/step - loss: 0.0633 - accuracy: 0.9775 - val_loss: 0.1714 - val_accuracy: 0.9367\n",
            "Epoch 195/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9856\n",
            "Epoch 195: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0514 - accuracy: 0.9858 - val_loss: 0.1740 - val_accuracy: 0.9367\n",
            "Epoch 196/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0543 - accuracy: 0.9823\n",
            "Epoch 196: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0550 - accuracy: 0.9817 - val_loss: 0.1717 - val_accuracy: 0.9367\n",
            "Epoch 197/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0530 - accuracy: 0.9809\n",
            "Epoch 197: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0529 - accuracy: 0.9808 - val_loss: 0.1744 - val_accuracy: 0.9367\n",
            "Epoch 198/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9842\n",
            "Epoch 198: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0620 - accuracy: 0.9842 - val_loss: 0.1689 - val_accuracy: 0.9400\n",
            "Epoch 199/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9772\n",
            "Epoch 199: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0569 - accuracy: 0.9767 - val_loss: 0.1690 - val_accuracy: 0.9400\n",
            "Epoch 200/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9823\n",
            "Epoch 200: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.0584 - accuracy: 0.9825 - val_loss: 0.1699 - val_accuracy: 0.9400\n",
            "Epoch 201/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9882\n",
            "Epoch 201: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0510 - accuracy: 0.9875 - val_loss: 0.1691 - val_accuracy: 0.9333\n",
            "Epoch 202/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9792\n",
            "Epoch 202: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0545 - accuracy: 0.9792 - val_loss: 0.1669 - val_accuracy: 0.9367\n",
            "Epoch 203/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0532 - accuracy: 0.9852\n",
            "Epoch 203: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0548 - accuracy: 0.9850 - val_loss: 0.1687 - val_accuracy: 0.9400\n",
            "Epoch 204/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9831\n",
            "Epoch 204: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0530 - accuracy: 0.9825 - val_loss: 0.1707 - val_accuracy: 0.9333\n",
            "Epoch 205/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9808\n",
            "Epoch 205: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0566 - accuracy: 0.9808 - val_loss: 0.1700 - val_accuracy: 0.9367\n",
            "Epoch 206/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0467 - accuracy: 0.9887\n",
            "Epoch 206: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0464 - accuracy: 0.9892 - val_loss: 0.1717 - val_accuracy: 0.9333\n",
            "Epoch 207/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9767\n",
            "Epoch 207: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0595 - accuracy: 0.9767 - val_loss: 0.1672 - val_accuracy: 0.9367\n",
            "Epoch 208/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9806\n",
            "Epoch 208: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0477 - accuracy: 0.9808 - val_loss: 0.1664 - val_accuracy: 0.9367\n",
            "Epoch 209/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9806\n",
            "Epoch 209: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0618 - accuracy: 0.9800 - val_loss: 0.1735 - val_accuracy: 0.9300\n",
            "Epoch 210/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0583 - accuracy: 0.9757\n",
            "Epoch 210: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0607 - accuracy: 0.9733 - val_loss: 0.1688 - val_accuracy: 0.9333\n",
            "Epoch 211/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9840\n",
            "Epoch 211: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0465 - accuracy: 0.9842 - val_loss: 0.1669 - val_accuracy: 0.9367\n",
            "Epoch 212/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0463 - accuracy: 0.9852\n",
            "Epoch 212: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0468 - accuracy: 0.9850 - val_loss: 0.1676 - val_accuracy: 0.9367\n",
            "Epoch 213/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9867\n",
            "Epoch 213: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0460 - accuracy: 0.9867 - val_loss: 0.1668 - val_accuracy: 0.9367\n",
            "Epoch 214/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9783\n",
            "Epoch 214: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 3s 68ms/step - loss: 0.0655 - accuracy: 0.9783 - val_loss: 0.1683 - val_accuracy: 0.9367\n",
            "Epoch 215/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9867\n",
            "Epoch 215: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0485 - accuracy: 0.9867 - val_loss: 0.1667 - val_accuracy: 0.9400\n",
            "Epoch 216/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9865\n",
            "Epoch 216: val_loss did not improve from 0.16610\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0403 - accuracy: 0.9867 - val_loss: 0.1688 - val_accuracy: 0.9367\n",
            "47/47 [==============================] - 2s 23ms/step - loss: 0.3915 - accuracy: 0.8953\n",
            "Test accuracy, 19 run, after finetuning: 0.8953333497047424\n",
            "Epoch 1/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 2.6108 - accuracy: 0.6208\n",
            "Epoch 1: val_loss improved from inf to 1.69475, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 43s 543ms/step - loss: 2.6108 - accuracy: 0.6208 - val_loss: 1.6947 - val_accuracy: 0.6867\n",
            "Epoch 2/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 1.6733 - accuracy: 0.6824\n",
            "Epoch 2: val_loss improved from 1.69475 to 1.07622, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 1.6652 - accuracy: 0.6842 - val_loss: 1.0762 - val_accuracy: 0.7433\n",
            "Epoch 3/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 1.1822 - accuracy: 0.7217\n",
            "Epoch 3: val_loss improved from 1.07622 to 0.77522, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 123ms/step - loss: 1.1822 - accuracy: 0.7217 - val_loss: 0.7752 - val_accuracy: 0.7700\n",
            "Epoch 4/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.9733 - accuracy: 0.7433\n",
            "Epoch 4: val_loss improved from 0.77522 to 0.64197, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.9733 - accuracy: 0.7433 - val_loss: 0.6420 - val_accuracy: 0.8133\n",
            "Epoch 5/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.8973 - accuracy: 0.7466\n",
            "Epoch 5: val_loss improved from 0.64197 to 0.56647, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.8962 - accuracy: 0.7475 - val_loss: 0.5665 - val_accuracy: 0.8333\n",
            "Epoch 6/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7619 - accuracy: 0.7804\n",
            "Epoch 6: val_loss improved from 0.56647 to 0.52606, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.7573 - accuracy: 0.7817 - val_loss: 0.5261 - val_accuracy: 0.8467\n",
            "Epoch 7/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.7266 - accuracy: 0.7838\n",
            "Epoch 7: val_loss improved from 0.52606 to 0.49550, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.7195 - accuracy: 0.7858 - val_loss: 0.4955 - val_accuracy: 0.8500\n",
            "Epoch 8/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.6682 - accuracy: 0.7863\n",
            "Epoch 8: val_loss improved from 0.49550 to 0.46433, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 137ms/step - loss: 0.6680 - accuracy: 0.7858 - val_loss: 0.4643 - val_accuracy: 0.8600\n",
            "Epoch 9/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.7942\n",
            "Epoch 9: val_loss improved from 0.46433 to 0.45002, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.6705 - accuracy: 0.7942 - val_loss: 0.4500 - val_accuracy: 0.8500\n",
            "Epoch 10/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.6795 - accuracy: 0.7995\n",
            "Epoch 10: val_loss improved from 0.45002 to 0.43246, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 121ms/step - loss: 0.6667 - accuracy: 0.8025 - val_loss: 0.4325 - val_accuracy: 0.8533\n",
            "Epoch 11/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.6239 - accuracy: 0.8058\n",
            "Epoch 11: val_loss improved from 0.43246 to 0.41817, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.6239 - accuracy: 0.8058 - val_loss: 0.4182 - val_accuracy: 0.8533\n",
            "Epoch 12/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5626 - accuracy: 0.8235\n",
            "Epoch 12: val_loss improved from 0.41817 to 0.40905, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.5554 - accuracy: 0.8258 - val_loss: 0.4091 - val_accuracy: 0.8500\n",
            "Epoch 13/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5797 - accuracy: 0.8260\n",
            "Epoch 13: val_loss improved from 0.40905 to 0.39833, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 134ms/step - loss: 0.5780 - accuracy: 0.8258 - val_loss: 0.3983 - val_accuracy: 0.8600\n",
            "Epoch 14/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.8117\n",
            "Epoch 14: val_loss improved from 0.39833 to 0.39143, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.5584 - accuracy: 0.8125 - val_loss: 0.3914 - val_accuracy: 0.8667\n",
            "Epoch 15/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.5125 - accuracy: 0.8167\n",
            "Epoch 15: val_loss improved from 0.39143 to 0.38058, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.5132 - accuracy: 0.8167 - val_loss: 0.3806 - val_accuracy: 0.8667\n",
            "Epoch 16/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8308\n",
            "Epoch 16: val_loss improved from 0.38058 to 0.37126, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.5173 - accuracy: 0.8308 - val_loss: 0.3713 - val_accuracy: 0.8533\n",
            "Epoch 17/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4749 - accuracy: 0.8342\n",
            "Epoch 17: val_loss improved from 0.37126 to 0.36470, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.4830 - accuracy: 0.8317 - val_loss: 0.3647 - val_accuracy: 0.8633\n",
            "Epoch 18/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4953 - accuracy: 0.8319\n",
            "Epoch 18: val_loss improved from 0.36470 to 0.36133, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.4923 - accuracy: 0.8333 - val_loss: 0.3613 - val_accuracy: 0.8733\n",
            "Epoch 19/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4671 - accuracy: 0.8353\n",
            "Epoch 19: val_loss improved from 0.36133 to 0.35343, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 120ms/step - loss: 0.4682 - accuracy: 0.8350 - val_loss: 0.3534 - val_accuracy: 0.8700\n",
            "Epoch 20/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.8395\n",
            "Epoch 20: val_loss improved from 0.35343 to 0.34916, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.4541 - accuracy: 0.8408 - val_loss: 0.3492 - val_accuracy: 0.8667\n",
            "Epoch 21/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.8475\n",
            "Epoch 21: val_loss improved from 0.34916 to 0.34092, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.4569 - accuracy: 0.8475 - val_loss: 0.3409 - val_accuracy: 0.8700\n",
            "Epoch 22/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4486 - accuracy: 0.8463\n",
            "Epoch 22: val_loss improved from 0.34092 to 0.33584, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.4445 - accuracy: 0.8475 - val_loss: 0.3358 - val_accuracy: 0.8733\n",
            "Epoch 23/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8421\n",
            "Epoch 23: val_loss improved from 0.33584 to 0.33489, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 124ms/step - loss: 0.4402 - accuracy: 0.8425 - val_loss: 0.3349 - val_accuracy: 0.8733\n",
            "Epoch 24/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8583\n",
            "Epoch 24: val_loss improved from 0.33489 to 0.32665, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.4089 - accuracy: 0.8583 - val_loss: 0.3266 - val_accuracy: 0.8767\n",
            "Epoch 25/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8608\n",
            "Epoch 25: val_loss improved from 0.32665 to 0.32358, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.3949 - accuracy: 0.8608 - val_loss: 0.3236 - val_accuracy: 0.8767\n",
            "Epoch 26/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3888 - accuracy: 0.8699\n",
            "Epoch 26: val_loss improved from 0.32358 to 0.31936, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 134ms/step - loss: 0.3935 - accuracy: 0.8700 - val_loss: 0.3194 - val_accuracy: 0.8800\n",
            "Epoch 27/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.4063 - accuracy: 0.8594\n",
            "Epoch 27: val_loss improved from 0.31936 to 0.31588, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.4070 - accuracy: 0.8583 - val_loss: 0.3159 - val_accuracy: 0.8800\n",
            "Epoch 28/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8708\n",
            "Epoch 28: val_loss improved from 0.31588 to 0.31183, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.3867 - accuracy: 0.8717 - val_loss: 0.3118 - val_accuracy: 0.8867\n",
            "Epoch 29/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3675 - accuracy: 0.8843\n",
            "Epoch 29: val_loss improved from 0.31183 to 0.30888, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 127ms/step - loss: 0.3642 - accuracy: 0.8850 - val_loss: 0.3089 - val_accuracy: 0.8867\n",
            "Epoch 30/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8640\n",
            "Epoch 30: val_loss improved from 0.30888 to 0.30702, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 122ms/step - loss: 0.3881 - accuracy: 0.8642 - val_loss: 0.3070 - val_accuracy: 0.8833\n",
            "Epoch 31/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8658\n",
            "Epoch 31: val_loss improved from 0.30702 to 0.30402, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.3811 - accuracy: 0.8658 - val_loss: 0.3040 - val_accuracy: 0.8800\n",
            "Epoch 32/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.8600\n",
            "Epoch 32: val_loss improved from 0.30402 to 0.30054, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.3839 - accuracy: 0.8600 - val_loss: 0.3005 - val_accuracy: 0.8833\n",
            "Epoch 33/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8667\n",
            "Epoch 33: val_loss improved from 0.30054 to 0.29686, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.3603 - accuracy: 0.8667 - val_loss: 0.2969 - val_accuracy: 0.8900\n",
            "Epoch 34/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.8784\n",
            "Epoch 34: val_loss improved from 0.29686 to 0.29276, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.3552 - accuracy: 0.8783 - val_loss: 0.2928 - val_accuracy: 0.8867\n",
            "Epoch 35/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.8783\n",
            "Epoch 35: val_loss improved from 0.29276 to 0.28775, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.3341 - accuracy: 0.8783 - val_loss: 0.2877 - val_accuracy: 0.8900\n",
            "Epoch 36/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3559 - accuracy: 0.8776\n",
            "Epoch 36: val_loss improved from 0.28775 to 0.28737, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 117ms/step - loss: 0.3523 - accuracy: 0.8783 - val_loss: 0.2874 - val_accuracy: 0.8867\n",
            "Epoch 37/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8826\n",
            "Epoch 37: val_loss improved from 0.28737 to 0.28491, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.3297 - accuracy: 0.8808 - val_loss: 0.2849 - val_accuracy: 0.8867\n",
            "Epoch 38/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8925\n",
            "Epoch 38: val_loss improved from 0.28491 to 0.28097, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.3083 - accuracy: 0.8925 - val_loss: 0.2810 - val_accuracy: 0.8867\n",
            "Epoch 39/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.8850\n",
            "Epoch 39: val_loss improved from 0.28097 to 0.27976, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.3223 - accuracy: 0.8850 - val_loss: 0.2798 - val_accuracy: 0.8900\n",
            "Epoch 40/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8883\n",
            "Epoch 40: val_loss improved from 0.27976 to 0.27633, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.3226 - accuracy: 0.8883 - val_loss: 0.2763 - val_accuracy: 0.8867\n",
            "Epoch 41/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3148 - accuracy: 0.8863\n",
            "Epoch 41: val_loss improved from 0.27633 to 0.27485, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 101ms/step - loss: 0.3239 - accuracy: 0.8850 - val_loss: 0.2749 - val_accuracy: 0.8867\n",
            "Epoch 42/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3054 - accuracy: 0.8932\n",
            "Epoch 42: val_loss improved from 0.27485 to 0.27112, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.3045 - accuracy: 0.8950 - val_loss: 0.2711 - val_accuracy: 0.8900\n",
            "Epoch 43/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.3252 - accuracy: 0.8808\n",
            "Epoch 43: val_loss improved from 0.27112 to 0.26984, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.3252 - accuracy: 0.8808 - val_loss: 0.2698 - val_accuracy: 0.8867\n",
            "Epoch 44/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.3029 - accuracy: 0.8906\n",
            "Epoch 44: val_loss improved from 0.26984 to 0.26784, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.3002 - accuracy: 0.8900 - val_loss: 0.2678 - val_accuracy: 0.8967\n",
            "Epoch 45/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9000\n",
            "Epoch 45: val_loss improved from 0.26784 to 0.26396, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2883 - accuracy: 0.9000 - val_loss: 0.2640 - val_accuracy: 0.8967\n",
            "Epoch 46/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2816 - accuracy: 0.9010\n",
            "Epoch 46: val_loss did not improve from 0.26396\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.2805 - accuracy: 0.9025 - val_loss: 0.2655 - val_accuracy: 0.8933\n",
            "Epoch 47/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2882 - accuracy: 0.8984\n",
            "Epoch 47: val_loss improved from 0.26396 to 0.26369, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2818 - accuracy: 0.9008 - val_loss: 0.2637 - val_accuracy: 0.8933\n",
            "Epoch 48/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2912 - accuracy: 0.9010\n",
            "Epoch 48: val_loss improved from 0.26369 to 0.25711, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2922 - accuracy: 0.9017 - val_loss: 0.2571 - val_accuracy: 0.9033\n",
            "Epoch 49/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9017\n",
            "Epoch 49: val_loss did not improve from 0.25711\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2787 - accuracy: 0.9017 - val_loss: 0.2575 - val_accuracy: 0.8933\n",
            "Epoch 50/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2687 - accuracy: 0.9046\n",
            "Epoch 50: val_loss improved from 0.25711 to 0.25207, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.2713 - accuracy: 0.9042 - val_loss: 0.2521 - val_accuracy: 0.9067\n",
            "Epoch 51/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.9003\n",
            "Epoch 51: val_loss improved from 0.25207 to 0.24943, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.2816 - accuracy: 0.9000 - val_loss: 0.2494 - val_accuracy: 0.9100\n",
            "Epoch 52/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2885 - accuracy: 0.9019\n",
            "Epoch 52: val_loss improved from 0.24943 to 0.24754, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2836 - accuracy: 0.9025 - val_loss: 0.2475 - val_accuracy: 0.9067\n",
            "Epoch 53/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.9096\n",
            "Epoch 53: val_loss did not improve from 0.24754\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2649 - accuracy: 0.9100 - val_loss: 0.2480 - val_accuracy: 0.9067\n",
            "Epoch 54/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.9071\n",
            "Epoch 54: val_loss improved from 0.24754 to 0.24648, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2789 - accuracy: 0.9067 - val_loss: 0.2465 - val_accuracy: 0.9133\n",
            "Epoch 55/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9198\n",
            "Epoch 55: val_loss improved from 0.24648 to 0.24485, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.2429 - accuracy: 0.9200 - val_loss: 0.2449 - val_accuracy: 0.9033\n",
            "Epoch 56/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9200\n",
            "Epoch 56: val_loss improved from 0.24485 to 0.24167, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2487 - accuracy: 0.9200 - val_loss: 0.2417 - val_accuracy: 0.9133\n",
            "Epoch 57/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2592 - accuracy: 0.9175\n",
            "Epoch 57: val_loss improved from 0.24167 to 0.24049, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2524 - accuracy: 0.9200 - val_loss: 0.2405 - val_accuracy: 0.9133\n",
            "Epoch 58/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2658 - accuracy: 0.9097\n",
            "Epoch 58: val_loss improved from 0.24049 to 0.23704, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 103ms/step - loss: 0.2654 - accuracy: 0.9100 - val_loss: 0.2370 - val_accuracy: 0.9167\n",
            "Epoch 59/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2458 - accuracy: 0.9141\n",
            "Epoch 59: val_loss did not improve from 0.23704\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.2474 - accuracy: 0.9150 - val_loss: 0.2391 - val_accuracy: 0.9133\n",
            "Epoch 60/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2361 - accuracy: 0.9092\n",
            "Epoch 60: val_loss improved from 0.23704 to 0.23594, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.2361 - accuracy: 0.9092 - val_loss: 0.2359 - val_accuracy: 0.9133\n",
            "Epoch 61/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9155\n",
            "Epoch 61: val_loss improved from 0.23594 to 0.23370, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.2471 - accuracy: 0.9142 - val_loss: 0.2337 - val_accuracy: 0.9200\n",
            "Epoch 62/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9092\n",
            "Epoch 62: val_loss did not improve from 0.23370\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2458 - accuracy: 0.9092 - val_loss: 0.2353 - val_accuracy: 0.9133\n",
            "Epoch 63/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9096\n",
            "Epoch 63: val_loss improved from 0.23370 to 0.23250, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.2373 - accuracy: 0.9100 - val_loss: 0.2325 - val_accuracy: 0.9200\n",
            "Epoch 64/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2284 - accuracy: 0.9141\n",
            "Epoch 64: val_loss improved from 0.23250 to 0.23111, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.2378 - accuracy: 0.9117 - val_loss: 0.2311 - val_accuracy: 0.9200\n",
            "Epoch 65/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2335 - accuracy: 0.9201\n",
            "Epoch 65: val_loss did not improve from 0.23111\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.2326 - accuracy: 0.9225 - val_loss: 0.2320 - val_accuracy: 0.9133\n",
            "Epoch 66/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.9105\n",
            "Epoch 66: val_loss improved from 0.23111 to 0.22987, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.2427 - accuracy: 0.9100 - val_loss: 0.2299 - val_accuracy: 0.9167\n",
            "Epoch 67/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9233\n",
            "Epoch 67: val_loss improved from 0.22987 to 0.22673, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 102ms/step - loss: 0.2157 - accuracy: 0.9233 - val_loss: 0.2267 - val_accuracy: 0.9233\n",
            "Epoch 68/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9215\n",
            "Epoch 68: val_loss improved from 0.22673 to 0.22562, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.2145 - accuracy: 0.9217 - val_loss: 0.2256 - val_accuracy: 0.9233\n",
            "Epoch 69/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9274\n",
            "Epoch 69: val_loss did not improve from 0.22562\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2123 - accuracy: 0.9283 - val_loss: 0.2277 - val_accuracy: 0.9267\n",
            "Epoch 70/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2011 - accuracy: 0.9223\n",
            "Epoch 70: val_loss did not improve from 0.22562\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2011 - accuracy: 0.9225 - val_loss: 0.2280 - val_accuracy: 0.9133\n",
            "Epoch 71/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2204 - accuracy: 0.9262\n",
            "Epoch 71: val_loss did not improve from 0.22562\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.2154 - accuracy: 0.9267 - val_loss: 0.2283 - val_accuracy: 0.9067\n",
            "Epoch 72/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9231\n",
            "Epoch 72: val_loss improved from 0.22562 to 0.22143, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.2200 - accuracy: 0.9233 - val_loss: 0.2214 - val_accuracy: 0.9200\n",
            "Epoch 73/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1987 - accuracy: 0.9248\n",
            "Epoch 73: val_loss improved from 0.22143 to 0.21849, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.1994 - accuracy: 0.9242 - val_loss: 0.2185 - val_accuracy: 0.9200\n",
            "Epoch 74/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1990 - accuracy: 0.9282\n",
            "Epoch 74: val_loss did not improve from 0.21849\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1996 - accuracy: 0.9275 - val_loss: 0.2188 - val_accuracy: 0.9233\n",
            "Epoch 75/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2137 - accuracy: 0.9262\n",
            "Epoch 75: val_loss did not improve from 0.21849\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.2118 - accuracy: 0.9267 - val_loss: 0.2210 - val_accuracy: 0.9200\n",
            "Epoch 76/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.9349\n",
            "Epoch 76: val_loss improved from 0.21849 to 0.21779, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.1992 - accuracy: 0.9333 - val_loss: 0.2178 - val_accuracy: 0.9167\n",
            "Epoch 77/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1779 - accuracy: 0.9366\n",
            "Epoch 77: val_loss improved from 0.21779 to 0.21196, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 105ms/step - loss: 0.1776 - accuracy: 0.9367 - val_loss: 0.2120 - val_accuracy: 0.9300\n",
            "Epoch 78/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9316\n",
            "Epoch 78: val_loss did not improve from 0.21196\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.2003 - accuracy: 0.9325 - val_loss: 0.2142 - val_accuracy: 0.9233\n",
            "Epoch 79/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.2043 - accuracy: 0.9280\n",
            "Epoch 79: val_loss did not improve from 0.21196\n",
            "38/38 [==============================] - 3s 75ms/step - loss: 0.1991 - accuracy: 0.9300 - val_loss: 0.2128 - val_accuracy: 0.9233\n",
            "Epoch 80/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9208\n",
            "Epoch 80: val_loss did not improve from 0.21196\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1976 - accuracy: 0.9208 - val_loss: 0.2121 - val_accuracy: 0.9233\n",
            "Epoch 81/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9358\n",
            "Epoch 81: val_loss improved from 0.21196 to 0.21132, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 95ms/step - loss: 0.1880 - accuracy: 0.9350 - val_loss: 0.2113 - val_accuracy: 0.9233\n",
            "Epoch 82/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9324\n",
            "Epoch 82: val_loss improved from 0.21132 to 0.20937, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1862 - accuracy: 0.9333 - val_loss: 0.2094 - val_accuracy: 0.9267\n",
            "Epoch 83/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9350\n",
            "Epoch 83: val_loss did not improve from 0.20937\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1877 - accuracy: 0.9358 - val_loss: 0.2094 - val_accuracy: 0.9233\n",
            "Epoch 84/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1968 - accuracy: 0.9265\n",
            "Epoch 84: val_loss improved from 0.20937 to 0.20606, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.1949 - accuracy: 0.9275 - val_loss: 0.2061 - val_accuracy: 0.9267\n",
            "Epoch 85/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9316\n",
            "Epoch 85: val_loss did not improve from 0.20606\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1729 - accuracy: 0.9325 - val_loss: 0.2077 - val_accuracy: 0.9300\n",
            "Epoch 86/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9291\n",
            "Epoch 86: val_loss did not improve from 0.20606\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1996 - accuracy: 0.9292 - val_loss: 0.2074 - val_accuracy: 0.9300\n",
            "Epoch 87/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9458\n",
            "Epoch 87: val_loss did not improve from 0.20606\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1669 - accuracy: 0.9458 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
            "Epoch 88/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9458\n",
            "Epoch 88: val_loss did not improve from 0.20606\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1719 - accuracy: 0.9458 - val_loss: 0.2103 - val_accuracy: 0.9267\n",
            "Epoch 89/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9307\n",
            "Epoch 89: val_loss improved from 0.20606 to 0.20371, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.1799 - accuracy: 0.9317 - val_loss: 0.2037 - val_accuracy: 0.9200\n",
            "Epoch 90/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9425\n",
            "Epoch 90: val_loss did not improve from 0.20371\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.1706 - accuracy: 0.9425 - val_loss: 0.2066 - val_accuracy: 0.9233\n",
            "Epoch 91/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9400\n",
            "Epoch 91: val_loss did not improve from 0.20371\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1705 - accuracy: 0.9400 - val_loss: 0.2065 - val_accuracy: 0.9267\n",
            "Epoch 92/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9483\n",
            "Epoch 92: val_loss improved from 0.20371 to 0.20241, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.1642 - accuracy: 0.9483 - val_loss: 0.2024 - val_accuracy: 0.9300\n",
            "Epoch 93/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9350\n",
            "Epoch 93: val_loss did not improve from 0.20241\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1707 - accuracy: 0.9342 - val_loss: 0.2055 - val_accuracy: 0.9233\n",
            "Epoch 94/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9383\n",
            "Epoch 94: val_loss did not improve from 0.20241\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1621 - accuracy: 0.9392 - val_loss: 0.2050 - val_accuracy: 0.9233\n",
            "Epoch 95/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9426\n",
            "Epoch 95: val_loss improved from 0.20241 to 0.19850, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.1642 - accuracy: 0.9433 - val_loss: 0.1985 - val_accuracy: 0.9200\n",
            "Epoch 96/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.9426\n",
            "Epoch 96: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1573 - accuracy: 0.9425 - val_loss: 0.2010 - val_accuracy: 0.9300\n",
            "Epoch 97/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9517\n",
            "Epoch 97: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 41ms/step - loss: 0.1484 - accuracy: 0.9517 - val_loss: 0.2018 - val_accuracy: 0.9267\n",
            "Epoch 98/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9500\n",
            "Epoch 98: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1397 - accuracy: 0.9500 - val_loss: 0.1991 - val_accuracy: 0.9267\n",
            "Epoch 99/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9442\n",
            "Epoch 99: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1639 - accuracy: 0.9442 - val_loss: 0.1999 - val_accuracy: 0.9300\n",
            "Epoch 100/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9417\n",
            "Epoch 100: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1508 - accuracy: 0.9425 - val_loss: 0.1991 - val_accuracy: 0.9300\n",
            "Epoch 101/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9525\n",
            "Epoch 101: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1441 - accuracy: 0.9525 - val_loss: 0.1994 - val_accuracy: 0.9233\n",
            "Epoch 102/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1611 - accuracy: 0.9436\n",
            "Epoch 102: val_loss did not improve from 0.19850\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1601 - accuracy: 0.9442 - val_loss: 0.1988 - val_accuracy: 0.9233\n",
            "Epoch 103/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1479 - accuracy: 0.9476\n",
            "Epoch 103: val_loss improved from 0.19850 to 0.19720, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 125ms/step - loss: 0.1476 - accuracy: 0.9475 - val_loss: 0.1972 - val_accuracy: 0.9233\n",
            "Epoch 104/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9485\n",
            "Epoch 104: val_loss did not improve from 0.19720\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1428 - accuracy: 0.9483 - val_loss: 0.1992 - val_accuracy: 0.9233\n",
            "Epoch 105/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9535\n",
            "Epoch 105: val_loss improved from 0.19720 to 0.19645, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 99ms/step - loss: 0.1364 - accuracy: 0.9517 - val_loss: 0.1965 - val_accuracy: 0.9267\n",
            "Epoch 106/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9510\n",
            "Epoch 106: val_loss improved from 0.19645 to 0.19576, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.1394 - accuracy: 0.9508 - val_loss: 0.1958 - val_accuracy: 0.9267\n",
            "Epoch 107/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9459\n",
            "Epoch 107: val_loss improved from 0.19576 to 0.19565, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.1375 - accuracy: 0.9458 - val_loss: 0.1957 - val_accuracy: 0.9233\n",
            "Epoch 108/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9635\n",
            "Epoch 108: val_loss improved from 0.19565 to 0.19560, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1363 - accuracy: 0.9608 - val_loss: 0.1956 - val_accuracy: 0.9233\n",
            "Epoch 109/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9458\n",
            "Epoch 109: val_loss improved from 0.19560 to 0.19290, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 98ms/step - loss: 0.1460 - accuracy: 0.9458 - val_loss: 0.1929 - val_accuracy: 0.9200\n",
            "Epoch 110/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1152 - accuracy: 0.9653\n",
            "Epoch 110: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1173 - accuracy: 0.9642 - val_loss: 0.1974 - val_accuracy: 0.9300\n",
            "Epoch 111/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9561\n",
            "Epoch 111: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.1338 - accuracy: 0.9558 - val_loss: 0.1944 - val_accuracy: 0.9300\n",
            "Epoch 112/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1376 - accuracy: 0.9497\n",
            "Epoch 112: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.1427 - accuracy: 0.9475 - val_loss: 0.1950 - val_accuracy: 0.9300\n",
            "Epoch 113/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9492\n",
            "Epoch 113: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 55ms/step - loss: 0.1440 - accuracy: 0.9492 - val_loss: 0.1987 - val_accuracy: 0.9267\n",
            "Epoch 114/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9542\n",
            "Epoch 114: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1433 - accuracy: 0.9542 - val_loss: 0.1974 - val_accuracy: 0.9300\n",
            "Epoch 115/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1280 - accuracy: 0.9583\n",
            "Epoch 115: val_loss did not improve from 0.19290\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1257 - accuracy: 0.9592 - val_loss: 0.1943 - val_accuracy: 0.9267\n",
            "Epoch 116/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1392 - accuracy: 0.9392\n",
            "Epoch 116: val_loss improved from 0.19290 to 0.19167, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 147ms/step - loss: 0.1408 - accuracy: 0.9392 - val_loss: 0.1917 - val_accuracy: 0.9267\n",
            "Epoch 117/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9575\n",
            "Epoch 117: val_loss improved from 0.19167 to 0.18860, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.1270 - accuracy: 0.9575 - val_loss: 0.1886 - val_accuracy: 0.9333\n",
            "Epoch 118/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9525\n",
            "Epoch 118: val_loss did not improve from 0.18860\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1309 - accuracy: 0.9525 - val_loss: 0.1948 - val_accuracy: 0.9267\n",
            "Epoch 119/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9558\n",
            "Epoch 119: val_loss did not improve from 0.18860\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.1223 - accuracy: 0.9558 - val_loss: 0.1891 - val_accuracy: 0.9300\n",
            "Epoch 120/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9583\n",
            "Epoch 120: val_loss improved from 0.18860 to 0.18506, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 4s 100ms/step - loss: 0.1163 - accuracy: 0.9583 - val_loss: 0.1851 - val_accuracy: 0.9200\n",
            "Epoch 121/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9523\n",
            "Epoch 121: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1263 - accuracy: 0.9517 - val_loss: 0.1884 - val_accuracy: 0.9267\n",
            "Epoch 122/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9549\n",
            "Epoch 122: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1236 - accuracy: 0.9550 - val_loss: 0.1951 - val_accuracy: 0.9267\n",
            "Epoch 123/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9586\n",
            "Epoch 123: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1235 - accuracy: 0.9592 - val_loss: 0.1902 - val_accuracy: 0.9367\n",
            "Epoch 124/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9544\n",
            "Epoch 124: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 65ms/step - loss: 0.1300 - accuracy: 0.9550 - val_loss: 0.1923 - val_accuracy: 0.9333\n",
            "Epoch 125/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1157 - accuracy: 0.9609\n",
            "Epoch 125: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1132 - accuracy: 0.9608 - val_loss: 0.1905 - val_accuracy: 0.9333\n",
            "Epoch 126/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9535\n",
            "Epoch 126: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1378 - accuracy: 0.9542 - val_loss: 0.1885 - val_accuracy: 0.9300\n",
            "Epoch 127/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9608\n",
            "Epoch 127: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1075 - accuracy: 0.9608 - val_loss: 0.1907 - val_accuracy: 0.9300\n",
            "Epoch 128/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9540\n",
            "Epoch 128: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1185 - accuracy: 0.9558 - val_loss: 0.1878 - val_accuracy: 0.9300\n",
            "Epoch 129/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9519\n",
            "Epoch 129: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.1304 - accuracy: 0.9525 - val_loss: 0.1869 - val_accuracy: 0.9267\n",
            "Epoch 130/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1228 - accuracy: 0.9531\n",
            "Epoch 130: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.1231 - accuracy: 0.9525 - val_loss: 0.1858 - val_accuracy: 0.9233\n",
            "Epoch 131/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9633\n",
            "Epoch 131: val_loss did not improve from 0.18506\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1059 - accuracy: 0.9633 - val_loss: 0.1867 - val_accuracy: 0.9267\n",
            "Epoch 132/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9592\n",
            "Epoch 132: val_loss improved from 0.18506 to 0.18301, saving model to /content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3.h5\n",
            "38/38 [==============================] - 5s 133ms/step - loss: 0.1204 - accuracy: 0.9592 - val_loss: 0.1830 - val_accuracy: 0.9233\n",
            "Epoch 133/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9617\n",
            "Epoch 133: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1040 - accuracy: 0.9617 - val_loss: 0.1870 - val_accuracy: 0.9267\n",
            "Epoch 134/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0986 - accuracy: 0.9618\n",
            "Epoch 134: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0972 - accuracy: 0.9625 - val_loss: 0.1843 - val_accuracy: 0.9267\n",
            "Epoch 135/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9625\n",
            "Epoch 135: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1083 - accuracy: 0.9625 - val_loss: 0.1871 - val_accuracy: 0.9233\n",
            "Epoch 136/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9671\n",
            "Epoch 136: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0991 - accuracy: 0.9667 - val_loss: 0.1874 - val_accuracy: 0.9267\n",
            "Epoch 137/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9595\n",
            "Epoch 137: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 3s 67ms/step - loss: 0.1128 - accuracy: 0.9558 - val_loss: 0.1886 - val_accuracy: 0.9267\n",
            "Epoch 138/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9671\n",
            "Epoch 138: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0957 - accuracy: 0.9667 - val_loss: 0.1909 - val_accuracy: 0.9267\n",
            "Epoch 139/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9611\n",
            "Epoch 139: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.1204 - accuracy: 0.9608 - val_loss: 0.1880 - val_accuracy: 0.9200\n",
            "Epoch 140/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9679\n",
            "Epoch 140: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0978 - accuracy: 0.9675 - val_loss: 0.1893 - val_accuracy: 0.9233\n",
            "Epoch 141/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.1105 - accuracy: 0.9583\n",
            "Epoch 141: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.1082 - accuracy: 0.9592 - val_loss: 0.1866 - val_accuracy: 0.9267\n",
            "Epoch 142/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9658\n",
            "Epoch 142: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0970 - accuracy: 0.9658 - val_loss: 0.1930 - val_accuracy: 0.9233\n",
            "Epoch 143/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0846 - accuracy: 0.9696\n",
            "Epoch 143: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0849 - accuracy: 0.9692 - val_loss: 0.1897 - val_accuracy: 0.9233\n",
            "Epoch 144/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9628\n",
            "Epoch 144: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0887 - accuracy: 0.9633 - val_loss: 0.1898 - val_accuracy: 0.9233\n",
            "Epoch 145/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9569\n",
            "Epoch 145: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 48ms/step - loss: 0.1099 - accuracy: 0.9575 - val_loss: 0.1897 - val_accuracy: 0.9233\n",
            "Epoch 146/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9696\n",
            "Epoch 146: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0992 - accuracy: 0.9692 - val_loss: 0.1897 - val_accuracy: 0.9267\n",
            "Epoch 147/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0826 - accuracy: 0.9731\n",
            "Epoch 147: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0882 - accuracy: 0.9708 - val_loss: 0.1896 - val_accuracy: 0.9267\n",
            "Epoch 148/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9717\n",
            "Epoch 148: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0852 - accuracy: 0.9717 - val_loss: 0.1868 - val_accuracy: 0.9267\n",
            "Epoch 149/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9662\n",
            "Epoch 149: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0913 - accuracy: 0.9667 - val_loss: 0.1859 - val_accuracy: 0.9267\n",
            "Epoch 150/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9755\n",
            "Epoch 150: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0797 - accuracy: 0.9742 - val_loss: 0.1887 - val_accuracy: 0.9267\n",
            "Epoch 151/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9658\n",
            "Epoch 151: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0988 - accuracy: 0.9658 - val_loss: 0.1899 - val_accuracy: 0.9300\n",
            "Epoch 152/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9628\n",
            "Epoch 152: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 42ms/step - loss: 0.0968 - accuracy: 0.9625 - val_loss: 0.1894 - val_accuracy: 0.9267\n",
            "Epoch 153/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9696\n",
            "Epoch 153: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 47ms/step - loss: 0.0871 - accuracy: 0.9692 - val_loss: 0.1847 - val_accuracy: 0.9267\n",
            "Epoch 154/1500\n",
            "38/38 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9717\n",
            "Epoch 154: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 51ms/step - loss: 0.0872 - accuracy: 0.9717 - val_loss: 0.1895 - val_accuracy: 0.9267\n",
            "Epoch 155/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9696\n",
            "Epoch 155: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 3s 69ms/step - loss: 0.0850 - accuracy: 0.9700 - val_loss: 0.1856 - val_accuracy: 0.9267\n",
            "Epoch 156/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9738\n",
            "Epoch 156: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 45ms/step - loss: 0.0824 - accuracy: 0.9742 - val_loss: 0.1898 - val_accuracy: 0.9267\n",
            "Epoch 157/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9696\n",
            "Epoch 157: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0863 - accuracy: 0.9700 - val_loss: 0.1884 - val_accuracy: 0.9267\n",
            "Epoch 158/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9764\n",
            "Epoch 158: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0679 - accuracy: 0.9767 - val_loss: 0.1840 - val_accuracy: 0.9267\n",
            "Epoch 159/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9645\n",
            "Epoch 159: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 44ms/step - loss: 0.0831 - accuracy: 0.9650 - val_loss: 0.1846 - val_accuracy: 0.9333\n",
            "Epoch 160/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0696 - accuracy: 0.9766\n",
            "Epoch 160: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 43ms/step - loss: 0.0711 - accuracy: 0.9767 - val_loss: 0.1858 - val_accuracy: 0.9267\n",
            "Epoch 161/1500\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9713\n",
            "Epoch 161: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 46ms/step - loss: 0.0800 - accuracy: 0.9717 - val_loss: 0.1876 - val_accuracy: 0.9300\n",
            "Epoch 162/1500\n",
            "36/38 [===========================>..] - ETA: 0s - loss: 0.0821 - accuracy: 0.9714\n",
            "Epoch 162: val_loss did not improve from 0.18301\n",
            "38/38 [==============================] - 2s 50ms/step - loss: 0.0811 - accuracy: 0.9717 - val_loss: 0.1890 - val_accuracy: 0.9267\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.3620 - accuracy: 0.8900\n",
            "Test accuracy, 20 run, after finetuning: 0.8899999856948853\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  res_net = create_model()\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=res_net)\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=30, mode='min'), ModelCheckpoint('/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2_FTm3.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
        "  res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "  res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2.h5\")\n",
        "\n",
        "  for layer in res_net.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  Known_data_X_finetune, Known_data_X_finetune_label = shuffle(Known_data_X_finetune, Known_data_X_finetune_label)\n",
        "\n",
        "  history = res_net.fit(Known_data_X_finetune, Known_data_X_finetune_label, epochs=1500, batch_size=32, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)\n",
        "\n",
        "  res_net.save_weights('/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2_FTm3_PP.h5')\n",
        "\n",
        "  test_loss, test_acc = res_net.evaluate(Known_data_X_test, Known_data_X_test_label)\n",
        "  print('Test accuracy, 01 run, after finetuning:', test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpFwucALz6ab",
        "outputId": "b09c16e5-dc71-4430-8600-a0cb52ffb678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 46s 949ms/step - loss: 0.3741 - accuracy: 0.8993\n",
            "Test accuracy, 01 run: 0.8993333578109741\n",
            "47/47 [==============================] - 32s 628ms/step - loss: 0.4438 - accuracy: 0.8920\n",
            "Test accuracy, 02 run: 0.8920000195503235\n",
            "47/47 [==============================] - 42s 852ms/step - loss: 0.4224 - accuracy: 0.8833\n",
            "Test accuracy, 03 run: 0.8833333253860474\n",
            "47/47 [==============================] - 44s 834ms/step - loss: 0.4682 - accuracy: 0.8840\n",
            "Test accuracy, 04 run: 0.8840000033378601\n",
            "47/47 [==============================] - 32s 617ms/step - loss: 0.4369 - accuracy: 0.8900\n",
            "Test accuracy, 05 run: 0.8899999856948853\n",
            "47/47 [==============================] - 32s 634ms/step - loss: 0.4382 - accuracy: 0.8813\n",
            "Test accuracy, 06 run: 0.8813333511352539\n",
            "47/47 [==============================] - 31s 625ms/step - loss: 0.4258 - accuracy: 0.8867\n",
            "Test accuracy, 07 run: 0.8866666555404663\n",
            "47/47 [==============================] - 32s 620ms/step - loss: 0.3980 - accuracy: 0.8887\n",
            "Test accuracy, 08 run: 0.8886666893959045\n",
            "47/47 [==============================] - 32s 638ms/step - loss: 0.3999 - accuracy: 0.8940\n",
            "Test accuracy, 09 run: 0.8939999938011169\n",
            "47/47 [==============================] - 32s 633ms/step - loss: 0.3921 - accuracy: 0.8847\n",
            "Test accuracy, 10 run: 0.8846666812896729\n",
            "47/47 [==============================] - 35s 711ms/step - loss: 0.3971 - accuracy: 0.8940\n",
            "Test accuracy, 11 run: 0.8939999938011169\n",
            "47/47 [==============================] - 31s 619ms/step - loss: 0.4226 - accuracy: 0.8900\n",
            "Test accuracy, 12 run: 0.8899999856948853\n",
            "47/47 [==============================] - 31s 611ms/step - loss: 0.4038 - accuracy: 0.8860\n",
            "Test accuracy, 13 run: 0.8859999775886536\n",
            "47/47 [==============================] - 31s 613ms/step - loss: 0.4257 - accuracy: 0.8887\n",
            "Test accuracy, 14 run: 0.8886666893959045\n",
            "47/47 [==============================] - 31s 602ms/step - loss: 0.3994 - accuracy: 0.8900\n",
            "Test accuracy, 15 run: 0.8899999856948853\n",
            "47/47 [==============================] - 31s 617ms/step - loss: 0.4642 - accuracy: 0.8780\n",
            "Test accuracy, 16 run: 0.878000020980835\n",
            "47/47 [==============================] - 32s 627ms/step - loss: 0.4181 - accuracy: 0.8920\n",
            "Test accuracy, 17 run: 0.8920000195503235\n",
            "47/47 [==============================] - 32s 628ms/step - loss: 0.3807 - accuracy: 0.8900\n",
            "Test accuracy, 18 run: 0.8899999856948853\n",
            "47/47 [==============================] - 32s 625ms/step - loss: 0.3910 - accuracy: 0.8960\n",
            "Test accuracy, 19 run: 0.8960000276565552\n",
            "47/47 [==============================] - 31s 615ms/step - loss: 0.3617 - accuracy: 0.8900\n",
            "Test accuracy, 20 run: 0.8899999856948853\n"
          ]
        }
      ],
      "source": [
        "# Now run and see the models with the best validation accuracy\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 01 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/02_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 02 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/03_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 03 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/04_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 04 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/05_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 05 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/06_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 06 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/07_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 07 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/08_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 08 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/09_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 09 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/10_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 10 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/11_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 11 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/12_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 12 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/13_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 13 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 14 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 15 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 16 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 17 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 18 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 19 run:', test_acc)\n",
        "\n",
        "res_net = create_model()\n",
        "res_net.load_weights(\"/content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(learning_rate=.00001/10),\n",
        "            metrics=['accuracy'])\n",
        "test_loss, test_acc = res_net.evaluate(Known_data_X_test,Known_data_X_test_label)\n",
        "print('Test accuracy, 20 run:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on the known dataset $\\mathcal{K}$\n"
      ],
      "metadata": {
        "id": "5YpaRDBPHgWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ii_oKevQh2",
        "outputId": "14a83fd3-b63c-494b-ee16-1e62197ed975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 11s 202ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 11s 201ms/step\n",
            "47/47 [==============================] - 11s 204ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 11s 194ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 10s 193ms/step\n",
            "47/47 [==============================] - 10s 190ms/step\n",
            "47/47 [==============================] - 10s 189ms/step\n",
            "47/47 [==============================] - 10s 188ms/step\n",
            "47/47 [==============================] - 10s 184ms/step\n",
            "47/47 [==============================] - 10s 188ms/step\n",
            "47/47 [==============================] - 10s 197ms/step\n",
            "47/47 [==============================] - 10s 198ms/step\n",
            "47/47 [==============================] - 10s 188ms/step\n",
            "47/47 [==============================] - 10s 189ms/step\n",
            "47/47 [==============================] - 10s 193ms/step\n",
            "Accuracy of the first ensemble on the knowns 0.894\n",
            "Accuracy of the second ensemble on the knowns 0.8973333333333333\n",
            "Accuracy of the third ensemble on the knowns 0.8933333333333333\n",
            "Accuracy of the fourth ensemble on the knowns 0.896\n"
          ]
        }
      ],
      "source": [
        "res_net01 = create_model()\n",
        "res_net02 = create_model()\n",
        "res_net03 = create_model()\n",
        "res_net04 = create_model()\n",
        "res_net05 = create_model()\n",
        "res_net06 = create_model()\n",
        "res_net07 = create_model()\n",
        "res_net08 = create_model()\n",
        "res_net09 = create_model()\n",
        "res_net10 = create_model()\n",
        "res_net11 = create_model()\n",
        "res_net12 = create_model()\n",
        "res_net13 = create_model()\n",
        "res_net14 = create_model()\n",
        "res_net15 = create_model()\n",
        "res_net16 = create_model()\n",
        "res_net17 = create_model()\n",
        "res_net18 = create_model()\n",
        "res_net19 = create_model()\n",
        "res_net20 = create_model()\n",
        "\n",
        "res_net01.load_weights(\"/content/gdrive/MyDrive/Stanford_data/01_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net02.load_weights(\"/content/gdrive/MyDrive/Stanford_data/02_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net03.load_weights(\"/content/gdrive/MyDrive/Stanford_data/03_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net04.load_weights(\"/content/gdrive/MyDrive/Stanford_data/04_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net05.load_weights(\"/content/gdrive/MyDrive/Stanford_data/05_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net06.load_weights(\"/content/gdrive/MyDrive/Stanford_data/06_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net07.load_weights(\"/content/gdrive/MyDrive/Stanford_data/07_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net08.load_weights(\"/content/gdrive/MyDrive/Stanford_data/08_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net09.load_weights(\"/content/gdrive/MyDrive/Stanford_data/09_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net10.load_weights(\"/content/gdrive/MyDrive/Stanford_data/10_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net11.load_weights(\"/content/gdrive/MyDrive/Stanford_data/11_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net12.load_weights(\"/content/gdrive/MyDrive/Stanford_data/12_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net13.load_weights(\"/content/gdrive/MyDrive/Stanford_data/13_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net14.load_weights(\"/content/gdrive/MyDrive/Stanford_data/14_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net15.load_weights(\"/content/gdrive/MyDrive/Stanford_data/15_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net16.load_weights(\"/content/gdrive/MyDrive/Stanford_data/16_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net17.load_weights(\"/content/gdrive/MyDrive/Stanford_data/17_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net18.load_weights(\"/content/gdrive/MyDrive/Stanford_data/18_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net19.load_weights(\"/content/gdrive/MyDrive/Stanford_data/19_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "res_net20.load_weights(\"/content/gdrive/MyDrive/Stanford_data/20_Naive_K_p1_p2_FTm3_PP.h5\")\n",
        "\n",
        "\n",
        "\n",
        "prediction01_known = res_net01.predict(Known_data_X_test)\n",
        "prediction02_known = res_net02.predict(Known_data_X_test)\n",
        "prediction03_known = res_net03.predict(Known_data_X_test)\n",
        "prediction04_known = res_net04.predict(Known_data_X_test)\n",
        "prediction05_known = res_net05.predict(Known_data_X_test)\n",
        "prediction06_known = res_net06.predict(Known_data_X_test)\n",
        "prediction07_known = res_net07.predict(Known_data_X_test)\n",
        "prediction08_known = res_net08.predict(Known_data_X_test)\n",
        "prediction09_known = res_net09.predict(Known_data_X_test)\n",
        "prediction10_known = res_net10.predict(Known_data_X_test)\n",
        "\n",
        "prediction11_known = res_net11.predict(Known_data_X_test)\n",
        "prediction12_known = res_net12.predict(Known_data_X_test)\n",
        "prediction13_known = res_net13.predict(Known_data_X_test)\n",
        "prediction14_known = res_net14.predict(Known_data_X_test)\n",
        "prediction15_known = res_net15.predict(Known_data_X_test)\n",
        "prediction16_known = res_net16.predict(Known_data_X_test)\n",
        "prediction17_known = res_net17.predict(Known_data_X_test)\n",
        "prediction18_known = res_net18.predict(Known_data_X_test)\n",
        "prediction19_known = res_net19.predict(Known_data_X_test)\n",
        "prediction20_known = res_net20.predict(Known_data_X_test)\n",
        "\n",
        "\n",
        "prediction_known_ensemble_1 = (prediction01_known + prediction02_known + prediction03_known + prediction04_known + prediction05_known)/5\n",
        "\n",
        "true = 0\n",
        "for i in range(Known_data_X_test_label.shape[0]):\n",
        "  if prediction_known_ensemble_1.argmax(axis=1)[i] == Known_data_X_test_label_int[i]:\n",
        "    true += 1\n",
        "print(\"Accuracy of the first ensemble on the knowns\", true/(Known_data_X_test_label.shape[0]))\n",
        "\n",
        "prediction_known_ensemble_2 = (prediction06_known + prediction07_known + prediction08_known + prediction09_known + prediction10_known)/5\n",
        "\n",
        "true = 0\n",
        "for i in range(Known_data_X_test_label.shape[0]):\n",
        "  if prediction_known_ensemble_2.argmax(axis=1)[i] == Known_data_X_test_label_int[i]:\n",
        "    true += 1\n",
        "print(\"Accuracy of the second ensemble on the knowns\", true/(Known_data_X_test_label.shape[0]))\n",
        "\n",
        "prediction_known_ensemble_3 = (prediction11_known + prediction12_known + prediction13_known + prediction14_known + prediction15_known)/5\n",
        "\n",
        "true = 0\n",
        "for i in range(Known_data_X_test_label.shape[0]):\n",
        "  if prediction_known_ensemble_3.argmax(axis=1)[i] == Known_data_X_test_label_int[i]:\n",
        "    true += 1\n",
        "print(\"Accuracy of the third ensemble on the knowns\", true/(Known_data_X_test_label.shape[0]))\n",
        "\n",
        "prediction_known_ensemble_4 = (prediction16_known + prediction17_known + prediction18_known + prediction19_known + prediction20_known)/5\n",
        "\n",
        "true = 0\n",
        "for i in range(Known_data_X_test_label.shape[0]):\n",
        "  if prediction_known_ensemble_4.argmax(axis=1)[i] == Known_data_X_test_label_int[i]:\n",
        "    true += 1\n",
        "print(\"Accuracy of the fourth ensemble on the knowns\", true/(Known_data_X_test_label.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9A0xE9BvQjy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPYicXZtvQlh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYAiLq1VvQnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction on the unknown dataset $\\mathcal{N}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1qjUJ4wfHXTY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTcCkbqgP_nC",
        "outputId": "6a8ce519-4e05-4042-c4ff-dea79ed4c7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 6s 189ms/step\n",
            "32/32 [==============================] - 6s 187ms/step\n",
            "32/32 [==============================] - 6s 194ms/step\n",
            "32/32 [==============================] - 6s 184ms/step\n",
            "32/32 [==============================] - 6s 193ms/step\n",
            "32/32 [==============================] - 6s 177ms/step\n",
            "32/32 [==============================] - 6s 188ms/step\n",
            "32/32 [==============================] - 6s 198ms/step\n",
            "32/32 [==============================] - 6s 193ms/step\n",
            "32/32 [==============================] - 6s 201ms/step\n",
            "32/32 [==============================] - 6s 192ms/step\n",
            "32/32 [==============================] - 6s 188ms/step\n",
            "32/32 [==============================] - 6s 192ms/step\n",
            "32/32 [==============================] - 6s 184ms/step\n",
            "32/32 [==============================] - 6s 187ms/step\n",
            "32/32 [==============================] - 6s 196ms/step\n",
            "32/32 [==============================] - 6s 192ms/step\n",
            "32/32 [==============================] - 6s 193ms/step\n",
            "32/32 [==============================] - 6s 199ms/step\n",
            "32/32 [==============================] - 6s 192ms/step\n"
          ]
        }
      ],
      "source": [
        "prediction01_unknown = res_net01.predict(NeverSeen_data_X_test)\n",
        "prediction02_unknown = res_net02.predict(NeverSeen_data_X_test)\n",
        "prediction03_unknown = res_net03.predict(NeverSeen_data_X_test)\n",
        "prediction04_unknown = res_net04.predict(NeverSeen_data_X_test)\n",
        "prediction05_unknown = res_net05.predict(NeverSeen_data_X_test)\n",
        "prediction06_unknown = res_net06.predict(NeverSeen_data_X_test)\n",
        "prediction07_unknown = res_net07.predict(NeverSeen_data_X_test)\n",
        "prediction08_unknown = res_net08.predict(NeverSeen_data_X_test)\n",
        "prediction09_unknown = res_net09.predict(NeverSeen_data_X_test)\n",
        "prediction10_unknown = res_net10.predict(NeverSeen_data_X_test)\n",
        "prediction11_unknown = res_net11.predict(NeverSeen_data_X_test)\n",
        "prediction12_unknown = res_net12.predict(NeverSeen_data_X_test)\n",
        "prediction13_unknown = res_net13.predict(NeverSeen_data_X_test)\n",
        "prediction14_unknown = res_net14.predict(NeverSeen_data_X_test)\n",
        "prediction15_unknown = res_net15.predict(NeverSeen_data_X_test)\n",
        "prediction16_unknown = res_net16.predict(NeverSeen_data_X_test)\n",
        "prediction17_unknown = res_net17.predict(NeverSeen_data_X_test)\n",
        "prediction18_unknown = res_net18.predict(NeverSeen_data_X_test)\n",
        "prediction19_unknown = res_net19.predict(NeverSeen_data_X_test)\n",
        "prediction20_unknown = res_net20.predict(NeverSeen_data_X_test)\n",
        "\n",
        "\n",
        "prediction_unknown_ensemble_1 = (prediction01_unknown + prediction02_unknown + prediction03_unknown + prediction04_unknown + prediction05_unknown)/5\n",
        "prediction_unknown_ensemble_2 = (prediction06_unknown + prediction07_unknown + prediction08_unknown + prediction09_unknown + prediction10_unknown)/5\n",
        "prediction_unknown_ensemble_3 = (prediction11_unknown + prediction12_unknown + prediction13_unknown + prediction14_unknown + prediction15_unknown)/5\n",
        "prediction_unknown_ensemble_4 = (prediction16_unknown + prediction17_unknown + prediction18_unknown + prediction19_unknown + prediction20_unknown)/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN0HLi5D7fDE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing ODIN scores"
      ],
      "metadata": {
        "id": "EBoiEsxmHR0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ftq4x5Yt7fFg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temperature = 1000\n",
        "magnitude = 0.001\n",
        "batch_size = 32\n",
        "threshold = 0.1\n",
        "\n",
        "loaded_models = [res_net01, res_net02, res_net03, res_net04, res_net05, res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "                 res_net11, res_net12, res_net13, res_net14, res_net15, res_net16, res_net17, res_net18, res_net19, res_net20]\n",
        "\n",
        "\n",
        "UN_Known_data_X_test_as_tensor = tf.convert_to_tensor(NeverSeen_data_X_test)\n",
        "\n",
        "odin_scores_for_models = [np.array([]) for _ in loaded_models]\n",
        "\n",
        "def compute_odin_scores_for_model(model, images, threshold):\n",
        "    logits_layer = model.layers[-2].output\n",
        "    logits_model = tf.keras.Model(inputs=model.input, outputs=logits_layer)\n",
        "\n",
        "    odin_scores_UN_KNOWN = []\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i + batch_size]\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            logits = logits_model(batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(batch)\n",
        "            output = logits_model(batch)\n",
        "        grads = tape.gradient(output, batch)\n",
        "\n",
        "        signed_grads = tf.sign(grads)\n",
        "\n",
        "        perturbed_spectra = batch + magnitude * signed_grads\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            perturbed_logits = logits_model(perturbed_spectra)\n",
        "\n",
        "        scaled_perturbed_logits = perturbed_logits / temperature\n",
        "\n",
        "        perturbed_softmax_output = tf.nn.softmax(scaled_perturbed_logits)\n",
        "\n",
        "        max_perturbed_softmax_scores = tf.reduce_max(perturbed_softmax_output, axis=1)\n",
        "\n",
        "        max_logits = tf.reduce_max(tf.nn.softmax(logits), axis=1)\n",
        "        odin_scores_batch = max_logits - max_perturbed_softmax_scores\n",
        "\n",
        "        odin_scores_UN_KNOWN.extend(odin_scores_batch)\n",
        "\n",
        "    return np.array(odin_scores_UN_KNOWN)\n",
        "\n",
        "for model_index, model in enumerate(loaded_models):\n",
        "    odin_scores = compute_odin_scores_for_model(model, UN_Known_data_X_test_as_tensor, threshold)\n",
        "    odin_scores_for_models[model_index] = odin_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0hnqThCW7fIy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "all_odin_scores = []\n",
        "for scores in odin_scores_for_models:\n",
        "    all_odin_scores.extend(scores)\n",
        "\n",
        "folder_path = \"/content/gdrive/MyDrive/Stanford_data\"\n",
        "\n",
        "file_name = \"all_odin_scores_NAIVEKp1p2.txt\"\n",
        "\n",
        "file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for score in all_odin_scores:\n",
        "        file.write(f\"{score}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Drcnjr6C7fLN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temperature = 1000\n",
        "magnitude = 0.001\n",
        "batch_size = 32\n",
        "threshold = 0.1\n",
        "\n",
        "loaded_models = [res_net01, res_net02, res_net03, res_net04, res_net05, res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "                 res_net11, res_net12, res_net13, res_net14, res_net15, res_net16, res_net17, res_net18, res_net19, res_net20]\n",
        "\n",
        "\n",
        "Known_data_X_test_as_tensor = tf.convert_to_tensor(Known_data_X_test)\n",
        "\n",
        "odin_scores_for_models = [np.array([]) for _ in loaded_models]\n",
        "\n",
        "def compute_odin_scores_for_model(model, images, threshold):\n",
        "    logits_layer = model.layers[-2].output\n",
        "    logits_model = tf.keras.Model(inputs=model.input, outputs=logits_layer)\n",
        "\n",
        "    odin_scores_KNOWN = []\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i + batch_size]\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            logits = logits_model(batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(batch)\n",
        "            output = logits_model(batch)\n",
        "        grads = tape.gradient(output, batch)\n",
        "\n",
        "        signed_grads = tf.sign(grads)\n",
        "\n",
        "        perturbed_spectra = batch + magnitude * signed_grads\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            perturbed_logits = logits_model(perturbed_spectra)\n",
        "\n",
        "        scaled_perturbed_logits = perturbed_logits / temperature\n",
        "\n",
        "        perturbed_softmax_output = tf.nn.softmax(scaled_perturbed_logits)\n",
        "\n",
        "        max_perturbed_softmax_scores = tf.reduce_max(perturbed_softmax_output, axis=1)\n",
        "\n",
        "        max_logits = tf.reduce_max(tf.nn.softmax(logits), axis=1)\n",
        "        odin_scores_batch = max_logits - max_perturbed_softmax_scores\n",
        "\n",
        "        odin_scores_KNOWN.extend(odin_scores_batch)\n",
        "\n",
        "    return np.array(odin_scores_KNOWN)\n",
        "\n",
        "for model_index, model in enumerate(loaded_models):\n",
        "    odin_scores = compute_odin_scores_for_model(model, Known_data_X_test_as_tensor, threshold)\n",
        "    odin_scores_for_models[model_index] = odin_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFDMk3A2k1V7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvFOPBS8k1ZU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temperature = 1000\n",
        "magnitude = 0.001\n",
        "batch_size = 32\n",
        "threshold = 0.1\n",
        "\n",
        "loaded_models = [res_net01, res_net02, res_net03, res_net04, res_net05, res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "                 res_net11, res_net12, res_net13, res_net14, res_net15, res_net16, res_net17, res_net18, res_net19, res_net20]\n",
        "\n",
        "\n",
        "UN_Known_data_X_test_as_tensor = tf.convert_to_tensor(NeverSeen_data_X_test)\n",
        "\n",
        "odin_scores_for_models = [np.array([]) for _ in loaded_models]\n",
        "\n",
        "def compute_odin_scores_for_model(model, images, threshold):\n",
        "    logits_layer = model.layers[-2].output\n",
        "    logits_model = tf.keras.Model(inputs=model.input, outputs=logits_layer)\n",
        "\n",
        "    odin_scores_UN_KNOWN = []\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch = images[i:i + batch_size]\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            logits = logits_model(batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(batch)\n",
        "            output = logits_model(batch)\n",
        "        grads = tape.gradient(output, batch)\n",
        "\n",
        "        signed_grads = tf.sign(grads)\n",
        "\n",
        "        perturbed_spectra = batch + magnitude * signed_grads\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            perturbed_logits = logits_model(perturbed_spectra)\n",
        "\n",
        "        scaled_perturbed_logits = perturbed_logits / temperature\n",
        "\n",
        "        perturbed_softmax_output = tf.nn.softmax(scaled_perturbed_logits)\n",
        "\n",
        "        max_perturbed_softmax_scores = tf.reduce_max(perturbed_softmax_output, axis=1)\n",
        "\n",
        "        max_logits = tf.reduce_max(tf.nn.softmax(logits), axis=1)\n",
        "        odin_scores_batch = max_logits - max_perturbed_softmax_scores\n",
        "\n",
        "        odin_scores_UN_KNOWN.extend(odin_scores_batch)\n",
        "\n",
        "    return np.array(odin_scores_UN_KNOWN)\n",
        "\n",
        "for model_index, model in enumerate(loaded_models):\n",
        "    odin_scores = compute_odin_scores_for_model(model, UN_Known_data_X_test_as_tensor, threshold)\n",
        "    odin_scores_for_models[model_index] = odin_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqoTGbiIk1cG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "all_odin_scores = []\n",
        "for scores in odin_scores_for_models:\n",
        "    all_odin_scores.extend(scores)\n",
        "\n",
        "folder_path = \"/content/gdrive/MyDrive/Stanford_data\"\n",
        "\n",
        "file_name = \"all_odin_scores_NAIVEKp1p2.txt\"\n",
        "\n",
        "file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for score in all_odin_scores:\n",
        "        file.write(f\"{score}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a32VwP7hk1ep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "e84ded2b-0046-4808-923a-0ad27d3ec3a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 32\n",
        "magnitude = 0.001\n",
        "temperature = 1000\n",
        "num_models = 20\n",
        "\n",
        "loaded_models = [\n",
        "    res_net01, res_net02, res_net03, res_net04, res_net05,\n",
        "    res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "    res_net11, res_net12, res_net13, res_net14, res_net15,\n",
        "    res_net16, res_net17, res_net18, res_net19, res_net20\n",
        "]\n",
        "\n",
        "odin_scores_all_models = []\n",
        "\n",
        "for i in range(num_models):\n",
        "    logits_layer = loaded_models[i].layers[-2].output\n",
        "\n",
        "    logits_model = tf.keras.Model(inputs=loaded_models[i].input, outputs=logits_layer)\n",
        "\n",
        "    Known_data_X_test_as_tensor = tf.convert_to_tensor(Known_data_X_test)\n",
        "\n",
        "    odin_scores_KNOWN = []\n",
        "\n",
        "    for j in range(0, len(Known_data_X_test), batch_size):\n",
        "        batch = Known_data_X_test_as_tensor[j:j+batch_size]\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            logits = logits_model(batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(batch)\n",
        "            logits = logits_model(batch)\n",
        "        grads = tape.gradient(logits, batch)\n",
        "\n",
        "        signed_grads = tf.sign(grads)\n",
        "\n",
        "        perturbed_spectra = batch + magnitude * signed_grads\n",
        "\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            perturbed_logits = logits_model(perturbed_spectra)\n",
        "\n",
        "        scaled_perturbed_logits = perturbed_logits / temperature\n",
        "\n",
        "        perturbed_softmax_output = tf.nn.softmax(scaled_perturbed_logits)\n",
        "\n",
        "        max_perturbed_softmax_scores = tf.reduce_max(perturbed_softmax_output, axis=1)\n",
        "\n",
        "        original_softmax_output = tf.nn.softmax(logits / temperature)\n",
        "        max_softmax_scores = tf.reduce_max(original_softmax_output, axis=1)\n",
        "\n",
        "        odin_scores_batch = max_softmax_scores - max_perturbed_softmax_scores\n",
        "\n",
        "        odin_scores_KNOWN.extend(odin_scores_batch)\n",
        "\n",
        "    odin_scores_KNOWN = np.array(odin_scores_KNOWN)\n",
        "    odin_scores_all_models.append(odin_scores_KNOWN)\n",
        "\n",
        "combined_odin_scores = np.concatenate(odin_scores_all_models)\n",
        "\n",
        "plt.hist(combined_odin_scores, bins=10)\n",
        "\n",
        "plt.xlabel('ODIN Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of ODIN Scores for Known Data (All Models)')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/gdrive/MyDrive/Stanford_data\"\n",
        "\n",
        "file_name = \"KNOWN_all_odin_scores_NAIVEKp1p2.txt\"\n",
        "\n",
        "file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for score in combined_odin_scores:\n",
        "        file.write(f\"{score}\\n\")"
      ],
      "metadata": {
        "id": "hO5_Z5t4EmeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnL9aFy6k1kF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVbEY8z3FgTy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_G5WNyYCohc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "7f730668-d881-4168-b79a-21924253a2cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temperature = 1000\n",
        "magnitude = 0.001\n",
        "\n",
        "UN_Known_data_X_test_as_tensor = tf.convert_to_tensor(NeverSeen_data_X_test)\n",
        "\n",
        "logits_layer = res_net01.layers[-2].output\n",
        "logits_model = tf.keras.Model(inputs=res_net01.input, outputs=logits_layer)\n",
        "\n",
        "odin_scores_UN_KNOWN = []\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "for i in range(0, len(NeverSeen_data_X_test), batch_size):\n",
        "    batch = UN_Known_data_X_test_as_tensor[i:i+batch_size]\n",
        "\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        logits = logits_model(batch)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(batch)\n",
        "        output = logits_model(batch)\n",
        "    grads = tape.gradient(output, batch)\n",
        "\n",
        "    signed_grads = tf.sign(grads)\n",
        "\n",
        "    perturbed_spectra = batch + magnitude * signed_grads\n",
        "\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        perturbed_logits = logits_model(perturbed_spectra)\n",
        "\n",
        "    scaled_perturbed_logits = perturbed_logits / temperature\n",
        "\n",
        "    perturbed_softmax_output = tf.nn.softmax(scaled_perturbed_logits)\n",
        "\n",
        "    max_perturbed_softmax_scores = tf.reduce_max(perturbed_softmax_output, axis=1)\n",
        "\n",
        "    max_logits = tf.reduce_max(tf.nn.softmax(logits), axis=1)\n",
        "    odin_scores_batch = max_logits - max_perturbed_softmax_scores\n",
        "\n",
        "    odin_scores_UN_KNOWN.extend(odin_scores_batch)\n",
        "\n",
        "odin_scores_UN_KNOWN = np.array(odin_scores_UN_KNOWN)\n",
        "\n",
        "plt.hist(odin_scores_UN_KNOWN, bins=10)\n",
        "\n",
        "plt.xlabel('ODIN Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of ODIN UNKNOWN Scores')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9HCTKIgCojz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenMax"
      ],
      "metadata": {
        "id": "HPaX-s3MRwa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alpha = 1.0\n",
        "num_models = 20\n",
        "\n",
        "loaded_models = [res_net01, res_net02, res_net03, res_net04, res_net05, res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "                 res_net11, res_net12, res_net13, res_net14, res_net15, res_net16, res_net17, res_net18, res_net19, res_net20]\n",
        "\n",
        "openmax_scores_known = []\n",
        "openmax_scores_unknown = []\n",
        "\n",
        "Known_data_X_test_as_tensor = tf.convert_to_tensor(Known_data_X_test)\n",
        "NeverSeen_data_X_test_as_tensor = tf.convert_to_tensor(NeverSeen_data_X_test)\n",
        "\n",
        "for model in loaded_models:\n",
        "    logits_layer = model.layers[-2].output\n",
        "\n",
        "    logits_model = tf.keras.Model(inputs=model.input, outputs=logits_layer)\n",
        "\n",
        "    known_logits = logits_model(Known_data_X_test_as_tensor)\n",
        "    unknown_logits = logits_model(NeverSeen_data_X_test_as_tensor)\n",
        "\n",
        "    max_known_logits = tf.reduce_max(known_logits, axis=1)\n",
        "    max_unknown_logits = tf.reduce_max(unknown_logits, axis=1)\n",
        "\n",
        "    scores_known = tf.exp(alpha * max_known_logits) / tf.reduce_sum(tf.exp(alpha * max_known_logits))\n",
        "\n",
        "    scores_unknown = []\n",
        "    for max_known, max_unknown in zip(max_known_logits, max_unknown_logits):\n",
        "        unknown_score = tf.exp(alpha * max_known) / (tf.exp(alpha * max_known) + tf.exp(alpha * max_unknown))\n",
        "        scores_unknown.append(unknown_score)\n",
        "\n",
        "    openmax_scores_known.append(scores_known.numpy())\n",
        "    openmax_scores_unknown.append(scores_unknown)\n",
        "\n",
        "merged_openmax_scores_known = np.stack(openmax_scores_known, axis=1)\n",
        "merged_openmax_scores_unknown = np.stack(openmax_scores_unknown, axis=1)\n",
        "\n",
        "combined_openmax_scores_known = merged_openmax_scores_known.flatten()\n",
        "combined_openmax_scores_unknown = merged_openmax_scores_unknown.flatten()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(combined_openmax_scores_known, bins=10, alpha=0.5, label=[f'Model {i+1}' for i in range(num_models)])\n",
        "plt.xlabel('OpenMax Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of OpenMax Scores for Known Data')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(combined_openmax_scores_unknown, bins=10, alpha=0.5, label=[f'Model {i+1}' for i in range(num_models)])\n",
        "plt.xlabel('OpenMax Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of OpenMax Scores for Unknown Data')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "UcZt2HEs_Y-f",
        "outputId": "11d2dc39-307f-4d36-b7da-8d5146541623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/gdrive/MyDrive/Stanford_data/NaiveKp1p2_combined_openmax_scores_known.txt', combined_openmax_scores_known)\n",
        "np.savetxt('/content/gdrive/MyDrive/Stanford_data/NaiveKp1p2_combined_openmax_scores_unknown.txt', combined_openmax_scores_unknown)"
      ],
      "metadata": {
        "id": "aXkTRzCe_ZBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quf7550w_ZEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mahalanobis"
      ],
      "metadata": {
        "id": "C1_joNSsSZlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    preprocessed_data = data\n",
        "    return preprocessed_data\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_deep_features(model, data):\n",
        "\n",
        "    preprocessed_data = preprocess_data(data)\n",
        "\n",
        "    deep_features = model.layers[-2].output\n",
        "    deep_feature_model = tf.keras.Model(inputs=model.input, outputs=deep_features)\n",
        "    extracted_features = deep_feature_model.predict(preprocessed_data)\n",
        "\n",
        "    return extracted_features\n",
        "\n",
        "def preprocess_data(data):\n",
        "    preprocessed_data = data\n",
        "    return preprocessed_data\n",
        "\n",
        "loaded_models = [res_net01, res_net02, res_net03, res_net04, res_net05, res_net06, res_net07, res_net08, res_net09, res_net10,\n",
        "                 res_net11, res_net12, res_net13, res_net14, res_net15, res_net16, res_net17, res_net18, res_net19, res_net20]\n",
        "\n",
        "deep_features_known_all = []\n",
        "mahalanobis_distances_known_all = []\n",
        "deep_features_unknown_all = []\n",
        "mahalanobis_distances_unknown_all = []\n",
        "\n",
        "n_components = min(deep_features_known.shape[0], deep_features_known.shape[1]) - 1\n",
        "\n",
        "for model in loaded_models:\n",
        "\n",
        "    deep_features_known = extract_deep_features(model, Known_data_X_test)\n",
        "    deep_features_unknown = extract_deep_features(model, NeverSeen_data_X_test)\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    deep_features_known_pca = pca.fit_transform(deep_features_known)\n",
        "    deep_features_unknown_pca = pca.transform(deep_features_unknown)\n",
        "\n",
        "    mean_known = np.mean(deep_features_known_pca, axis=0)\n",
        "    cov_known = np.cov(deep_features_known_pca, rowvar=False)\n",
        "\n",
        "    cond_number = np.linalg.cond(cov_known)\n",
        "    print(\"Condition Number of Covariance Matrix (Known Data):\", cond_number)\n",
        "\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    if cond_number > 1 / epsilon:\n",
        "        cov_known_reg = cov_known + epsilon * np.eye(cov_known.shape[0])\n",
        "    else:\n",
        "        cov_known_reg = cov_known\n",
        "\n",
        "    mahalanobis_distances_known = []\n",
        "    for feature in deep_features_known_pca:\n",
        "        mahalanobis_distance = distance.mahalanobis(feature, mean_known, np.linalg.inv(cov_known_reg))\n",
        "        mahalanobis_distances_known.append(mahalanobis_distance)\n",
        "\n",
        "    mahalanobis_distances_unknown = []\n",
        "    for feature in deep_features_unknown_pca:\n",
        "        mahalanobis_distance = distance.mahalanobis(feature, mean_known, np.linalg.inv(cov_known_reg))\n",
        "        mahalanobis_distances_unknown.append(mahalanobis_distance)\n",
        "\n",
        "    deep_features_known_all.append(deep_features_known_pca)\n",
        "    mahalanobis_distances_known_all.append(mahalanobis_distances_known)\n",
        "    deep_features_unknown_all.append(deep_features_unknown_pca)\n",
        "    mahalanobis_distances_unknown_all.append(mahalanobis_distances_unknown)\n",
        "\n",
        "deep_features_known_combined = np.concatenate(deep_features_known_all, axis=-1)\n",
        "mahalanobis_distances_known_combined = np.mean(mahalanobis_distances_known_all, axis=0)\n",
        "\n",
        "deep_features_unknown_combined = np.concatenate(deep_features_unknown_all, axis=-1)\n",
        "mahalanobis_distances_unknown_combined = np.mean(mahalanobis_distances_unknown_all, axis=0)\n",
        "\n",
        "threshold_known = 3.0\n",
        "threshold_unknown = 4.0\n",
        "\n",
        "ood_samples_known = [i for i, distance in enumerate(mahalanobis_distances_known_combined) if distance > threshold_known]\n",
        "ood_samples_unknown = [i for i, distance in enumerate(mahalanobis_distances_unknown_combined) if distance > threshold_unknown]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(mahalanobis_distances_known_combined, bins=10)\n",
        "plt.xlabel('Mahalanobis Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Mahalanobis Scores for Known Data (Combined)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(mahalanobis_distances_unknown_combined, bins=10)\n",
        "plt.xlabel('Mahalanobis Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Mahalanobis Scores for Unknown Data (Combined)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IgYvRhe5SXqZ",
        "outputId": "ef312e9f-8a5d-4d77-aa91-f92b46c5c164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47/47 [==============================] - 12s 227ms/step\n",
            "32/32 [==============================] - 7s 190ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 458549.9710960553\n",
            "47/47 [==============================] - 9s 179ms/step\n",
            "32/32 [==============================] - 6s 169ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 406462.45885768835\n",
            "47/47 [==============================] - 9s 178ms/step\n",
            "32/32 [==============================] - 6s 169ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 476691.78225533554\n",
            "47/47 [==============================] - 9s 171ms/step\n",
            "32/32 [==============================] - 6s 169ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 457231.230886524\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 174ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 510651.17131877254\n",
            "47/47 [==============================] - 10s 183ms/step\n",
            "32/32 [==============================] - 7s 174ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 485731.5866390541\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 172ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 363482.3454994885\n",
            "47/47 [==============================] - 10s 184ms/step\n",
            "32/32 [==============================] - 7s 175ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 545725.129891769\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 176ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 569648.481706772\n",
            "47/47 [==============================] - 10s 181ms/step\n",
            "32/32 [==============================] - 7s 178ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 672874.670255768\n",
            "47/47 [==============================] - 11s 203ms/step\n",
            "32/32 [==============================] - 8s 195ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 557003.6537038041\n",
            "47/47 [==============================] - 9s 178ms/step\n",
            "32/32 [==============================] - 7s 170ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 502663.14709124045\n",
            "47/47 [==============================] - 10s 186ms/step\n",
            "32/32 [==============================] - 7s 186ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 460630.6889531926\n",
            "47/47 [==============================] - 10s 181ms/step\n",
            "32/32 [==============================] - 7s 172ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 562837.6286831906\n",
            "47/47 [==============================] - 10s 181ms/step\n",
            "32/32 [==============================] - 7s 169ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 645086.1969530901\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 172ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 455434.89240447944\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 170ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 461210.48948568734\n",
            "47/47 [==============================] - 9s 175ms/step\n",
            "32/32 [==============================] - 7s 180ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 490127.6324982873\n",
            "47/47 [==============================] - 10s 180ms/step\n",
            "32/32 [==============================] - 7s 174ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 504448.9608228999\n",
            "47/47 [==============================] - 10s 181ms/step\n",
            "32/32 [==============================] - 7s 170ms/step\n",
            "Condition Number of Covariance Matrix (Known Data): 455586.8807310498\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(f'{folder_path}/mahalanobis_distances_known_combined_Naivep1p2.txt', mahalanobis_distances_known_combined)\n",
        "np.savetxt(f'{folder_path}/mahalanobis_distances_unknown_combined_Naivep1p2.txt', mahalanobis_distances_unknown_combined)"
      ],
      "metadata": {
        "id": "XUfpJb9UR2Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCiMJfcc5PW5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWXxOYcXP_uk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Pq2KqHYYgM"
      },
      "source": [
        "# Separating $\\mathcal{K}$ and $\\mathcal{N}$ by a fixed (class-independendent) threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZfHJkznZ4Lq"
      },
      "outputs": [],
      "source": [
        "prediction_known = prediction_known_ensemble_1\n",
        "prediction_unknown = prediction_unknown_ensemble_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HA75xLuUGNca"
      },
      "outputs": [],
      "source": [
        "def thresholding(threshold):\n",
        "  true = 0\n",
        "  for i in range(prediction_known.shape[0]):\n",
        "    if prediction_known.argmax(axis=1)[i] == Known_data_X_test_label_int[i] and max(prediction_known[i]) > threshold:\n",
        "      true += 1\n",
        "  return true/(prediction_known.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW6PGNX72bky",
        "outputId": "8698fd28-1d22-41bd-d400-b2f4a4a3b9f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.1, 89.4],\n",
              " [0.11, 89.4],\n",
              " [0.12000000000000001, 89.4],\n",
              " [0.13, 89.4],\n",
              " [0.14, 89.4],\n",
              " [0.15000000000000002, 89.4],\n",
              " [0.16, 89.4],\n",
              " [0.17, 89.4],\n",
              " [0.18, 89.4],\n",
              " [0.19, 89.4],\n",
              " [0.2, 89.4],\n",
              " [0.21000000000000002, 89.4],\n",
              " [0.22, 89.4],\n",
              " [0.23, 89.4],\n",
              " [0.24000000000000002, 89.4],\n",
              " [0.25, 89.4],\n",
              " [0.26, 89.4],\n",
              " [0.27, 89.4],\n",
              " [0.28, 89.4],\n",
              " [0.29000000000000004, 89.4],\n",
              " [0.30000000000000004, 89.4],\n",
              " [0.31, 89.4],\n",
              " [0.32, 89.4],\n",
              " [0.33, 89.4],\n",
              " [0.33999999999999997, 89.33333333333333],\n",
              " [0.35, 89.33333333333333],\n",
              " [0.36, 89.33333333333333],\n",
              " [0.37, 89.33333333333333],\n",
              " [0.38, 89.33333333333333],\n",
              " [0.39, 89.26666666666667],\n",
              " [0.4, 89.26666666666667],\n",
              " [0.41000000000000003, 89.26666666666667],\n",
              " [0.42000000000000004, 89.2],\n",
              " [0.43000000000000005, 89.2],\n",
              " [0.44000000000000006, 89.13333333333333],\n",
              " [0.45000000000000007, 89.0],\n",
              " [0.45999999999999996, 88.93333333333334],\n",
              " [0.47, 88.93333333333334],\n",
              " [0.48, 88.8],\n",
              " [0.49, 88.66666666666667],\n",
              " [0.5, 88.46666666666667],\n",
              " [0.51, 88.13333333333333],\n",
              " [0.52, 88.06666666666668],\n",
              " [0.53, 87.86666666666667],\n",
              " [0.54, 87.73333333333333],\n",
              " [0.55, 87.6],\n",
              " [0.56, 87.4],\n",
              " [0.5700000000000001, 87.13333333333333],\n",
              " [0.58, 87.06666666666666],\n",
              " [0.59, 86.93333333333332],\n",
              " [0.6, 86.53333333333333],\n",
              " [0.61, 86.46666666666667],\n",
              " [0.62, 86.46666666666667],\n",
              " [0.63, 86.33333333333333],\n",
              " [0.64, 86.26666666666667],\n",
              " [0.65, 86.2],\n",
              " [0.66, 86.06666666666666],\n",
              " [0.67, 85.86666666666667],\n",
              " [0.6799999999999999, 85.8],\n",
              " [0.69, 85.73333333333333],\n",
              " [0.7, 85.46666666666667],\n",
              " [0.71, 85.39999999999999],\n",
              " [0.72, 85.06666666666666],\n",
              " [0.73, 84.8],\n",
              " [0.74, 84.39999999999999],\n",
              " [0.75, 84.2],\n",
              " [0.76, 84.06666666666666],\n",
              " [0.77, 83.93333333333334],\n",
              " [0.78, 83.8],\n",
              " [0.79, 83.33333333333334],\n",
              " [0.8, 82.8],\n",
              " [0.8099999999999999, 82.19999999999999],\n",
              " [0.82, 81.66666666666667],\n",
              " [0.83, 81.53333333333333],\n",
              " [0.84, 81.26666666666667],\n",
              " [0.85, 81.0],\n",
              " [0.86, 80.66666666666666],\n",
              " [0.87, 80.13333333333334],\n",
              " [0.88, 79.86666666666666],\n",
              " [0.89, 79.13333333333334],\n",
              " [0.9, 78.60000000000001],\n",
              " [0.91, 77.8],\n",
              " [0.92, 77.33333333333333],\n",
              " [0.93, 76.06666666666668],\n",
              " [0.94, 75.13333333333333],\n",
              " [0.95, 74.13333333333333],\n",
              " [0.96, 73.06666666666666],\n",
              " [0.97, 71.13333333333334],\n",
              " [0.98, 68.33333333333333],\n",
              " [0.99, 62.133333333333326],\n",
              " [0.99, 62.133333333333326],\n",
              " [0.991, 60.53333333333333],\n",
              " [0.992, 59.46666666666667],\n",
              " [0.993, 58.666666666666664],\n",
              " [0.994, 57.733333333333334],\n",
              " [0.995, 56.53333333333334],\n",
              " [0.996, 55.333333333333336],\n",
              " [0.997, 52.46666666666666],\n",
              " [0.998, 49.06666666666666],\n",
              " [0.999, 43.46666666666666]]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CorrectThresholding = []\n",
        "for i in range(90):\n",
        "  CorrectThresholding.append([0.1 + 0.01*i,100*thresholding(0.1 + 0.01*i)])\n",
        "for i in range(10):\n",
        "  CorrectThresholding.append([0.99 + 0.001*i,100*thresholding(0.99 + 0.001*i)])\n",
        "CorrectThresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn2lRWxDM4G4"
      },
      "outputs": [],
      "source": [
        "def false_identify(threshold):\n",
        "  false_identify = 0\n",
        "  for i in range(prediction_known.shape[0]):\n",
        "    if prediction_known.argmax(axis=1)[i] != Known_data_X_test_label_int[i] and max(prediction_known[i]) > threshold:\n",
        "      false_identify += 1\n",
        "  return false_identify/(prediction_known.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M753dWrlM4Km",
        "outputId": "8e76a9ae-94c1-4851-cc4e-19470ea23aee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.1, 10.6],\n",
              " [0.11, 10.6],\n",
              " [0.12000000000000001, 10.6],\n",
              " [0.13, 10.6],\n",
              " [0.14, 10.6],\n",
              " [0.15000000000000002, 10.6],\n",
              " [0.16, 10.6],\n",
              " [0.17, 10.6],\n",
              " [0.18, 10.6],\n",
              " [0.19, 10.6],\n",
              " [0.2, 10.6],\n",
              " [0.21000000000000002, 10.6],\n",
              " [0.22, 10.6],\n",
              " [0.23, 10.6],\n",
              " [0.24000000000000002, 10.533333333333333],\n",
              " [0.25, 10.533333333333333],\n",
              " [0.26, 10.533333333333333],\n",
              " [0.27, 10.533333333333333],\n",
              " [0.28, 10.533333333333333],\n",
              " [0.29000000000000004, 10.533333333333333],\n",
              " [0.30000000000000004, 10.466666666666667],\n",
              " [0.31, 10.466666666666667],\n",
              " [0.32, 10.466666666666667],\n",
              " [0.33, 10.466666666666667],\n",
              " [0.33999999999999997, 10.466666666666667],\n",
              " [0.35, 10.466666666666667],\n",
              " [0.36, 10.466666666666667],\n",
              " [0.37, 10.466666666666667],\n",
              " [0.38, 10.466666666666667],\n",
              " [0.39, 10.333333333333334],\n",
              " [0.4, 10.266666666666667],\n",
              " [0.41000000000000003, 10.266666666666667],\n",
              " [0.42000000000000004, 10.133333333333333],\n",
              " [0.43000000000000005, 10.066666666666666],\n",
              " [0.44000000000000006, 10.066666666666666],\n",
              " [0.45000000000000007, 9.8],\n",
              " [0.45999999999999996, 9.8],\n",
              " [0.47, 9.8],\n",
              " [0.48, 9.733333333333333],\n",
              " [0.49, 9.6],\n",
              " [0.5, 9.333333333333334],\n",
              " [0.51, 9.333333333333334],\n",
              " [0.52, 9.2],\n",
              " [0.53, 9.133333333333333],\n",
              " [0.54, 9.0],\n",
              " [0.55, 8.866666666666667],\n",
              " [0.56, 8.666666666666668],\n",
              " [0.5700000000000001, 8.533333333333333],\n",
              " [0.58, 8.333333333333332],\n",
              " [0.59, 8.133333333333333],\n",
              " [0.6, 7.8],\n",
              " [0.61, 7.733333333333333],\n",
              " [0.62, 7.533333333333333],\n",
              " [0.63, 7.199999999999999],\n",
              " [0.64, 7.066666666666667],\n",
              " [0.65, 6.7333333333333325],\n",
              " [0.66, 6.533333333333332],\n",
              " [0.67, 6.466666666666667],\n",
              " [0.6799999999999999, 6.333333333333334],\n",
              " [0.69, 6.066666666666666],\n",
              " [0.7, 6.0],\n",
              " [0.71, 5.866666666666666],\n",
              " [0.72, 5.733333333333333],\n",
              " [0.73, 5.733333333333333],\n",
              " [0.74, 5.6000000000000005],\n",
              " [0.75, 5.133333333333334],\n",
              " [0.76, 4.8],\n",
              " [0.77, 4.533333333333333],\n",
              " [0.78, 4.3999999999999995],\n",
              " [0.79, 4.266666666666667],\n",
              " [0.8, 4.2],\n",
              " [0.8099999999999999, 4.0],\n",
              " [0.82, 3.8],\n",
              " [0.83, 3.733333333333334],\n",
              " [0.84, 3.6666666666666665],\n",
              " [0.85, 3.5999999999999996],\n",
              " [0.86, 3.4666666666666663],\n",
              " [0.87, 3.3333333333333335],\n",
              " [0.88, 3.3333333333333335],\n",
              " [0.89, 3.1333333333333333],\n",
              " [0.9, 2.4],\n",
              " [0.91, 2.3333333333333335],\n",
              " [0.92, 2.1333333333333333],\n",
              " [0.93, 2.0],\n",
              " [0.94, 1.9333333333333333],\n",
              " [0.95, 1.6666666666666667],\n",
              " [0.96, 1.4666666666666666],\n",
              " [0.97, 1.2666666666666666],\n",
              " [0.98, 1.0666666666666667],\n",
              " [0.99, 0.5333333333333333],\n",
              " [0.99, 0.5333333333333333],\n",
              " [0.991, 0.46666666666666673],\n",
              " [0.992, 0.4],\n",
              " [0.993, 0.33333333333333337],\n",
              " [0.994, 0.26666666666666666],\n",
              " [0.995, 0.2],\n",
              " [0.996, 0.2],\n",
              " [0.997, 0.13333333333333333],\n",
              " [0.998, 0.13333333333333333],\n",
              " [0.999, 0.13333333333333333]]"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FalseIdentify = []\n",
        "for i in range(90):\n",
        "  FalseIdentify.append([0.1 + 0.01*i,100*false_identify(0.1 + 0.01*i)])\n",
        "for i in range(10):\n",
        "  FalseIdentify.append([0.99 + 0.001*i,100*false_identify(0.99 + 0.001*i)])\n",
        "FalseIdentify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InSYYnV1M4Of"
      },
      "outputs": [],
      "source": [
        "def misclassifyunknown(threshold):\n",
        "  misclassifyunknown = 0\n",
        "  for i in range(prediction_unknown.shape[0]):\n",
        "    if max(prediction_unknown[i]) > threshold:\n",
        "      misclassifyunknown += 1\n",
        "  return misclassifyunknown/(prediction_unknown.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKdfbnzZM4ST",
        "outputId": "c88e65ac-94e8-48a9-d28c-9b345e0fc4b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.1, 100.0],\n",
              " [0.11, 100.0],\n",
              " [0.12000000000000001, 100.0],\n",
              " [0.13, 100.0],\n",
              " [0.14, 100.0],\n",
              " [0.15000000000000002, 100.0],\n",
              " [0.16, 100.0],\n",
              " [0.17, 100.0],\n",
              " [0.18, 100.0],\n",
              " [0.19, 100.0],\n",
              " [0.2, 100.0],\n",
              " [0.21000000000000002, 100.0],\n",
              " [0.22, 100.0],\n",
              " [0.23, 100.0],\n",
              " [0.24000000000000002, 100.0],\n",
              " [0.25, 100.0],\n",
              " [0.26, 100.0],\n",
              " [0.27, 100.0],\n",
              " [0.28, 100.0],\n",
              " [0.29000000000000004, 99.9],\n",
              " [0.30000000000000004, 99.8],\n",
              " [0.31, 99.7],\n",
              " [0.32, 99.7],\n",
              " [0.33, 99.7],\n",
              " [0.33999999999999997, 99.6],\n",
              " [0.35, 99.4],\n",
              " [0.36, 99.2],\n",
              " [0.37, 99.1],\n",
              " [0.38, 99.0],\n",
              " [0.39, 98.8],\n",
              " [0.4, 98.7],\n",
              " [0.41000000000000003, 98.5],\n",
              " [0.42000000000000004, 98.3],\n",
              " [0.43000000000000005, 98.1],\n",
              " [0.44000000000000006, 98.1],\n",
              " [0.45000000000000007, 97.7],\n",
              " [0.45999999999999996, 97.6],\n",
              " [0.47, 97.39999999999999],\n",
              " [0.48, 97.2],\n",
              " [0.49, 96.7],\n",
              " [0.5, 96.1],\n",
              " [0.51, 95.1],\n",
              " [0.52, 94.5],\n",
              " [0.53, 93.7],\n",
              " [0.54, 92.80000000000001],\n",
              " [0.55, 91.2],\n",
              " [0.56, 90.3],\n",
              " [0.5700000000000001, 89.60000000000001],\n",
              " [0.58, 88.9],\n",
              " [0.59, 88.4],\n",
              " [0.6, 87.7],\n",
              " [0.61, 87.1],\n",
              " [0.62, 86.1],\n",
              " [0.63, 85.39999999999999],\n",
              " [0.64, 84.7],\n",
              " [0.65, 84.2],\n",
              " [0.66, 83.39999999999999],\n",
              " [0.67, 82.8],\n",
              " [0.6799999999999999, 81.0],\n",
              " [0.69, 79.7],\n",
              " [0.7, 78.7],\n",
              " [0.71, 77.8],\n",
              " [0.72, 76.9],\n",
              " [0.73, 76.2],\n",
              " [0.74, 75.4],\n",
              " [0.75, 74.5],\n",
              " [0.76, 73.4],\n",
              " [0.77, 73.0],\n",
              " [0.78, 72.1],\n",
              " [0.79, 71.0],\n",
              " [0.8, 70.39999999999999],\n",
              " [0.8099999999999999, 69.5],\n",
              " [0.82, 68.30000000000001],\n",
              " [0.83, 67.80000000000001],\n",
              " [0.84, 67.0],\n",
              " [0.85, 66.2],\n",
              " [0.86, 64.9],\n",
              " [0.87, 63.800000000000004],\n",
              " [0.88, 62.0],\n",
              " [0.89, 60.699999999999996],\n",
              " [0.9, 59.099999999999994],\n",
              " [0.91, 57.4],\n",
              " [0.92, 55.50000000000001],\n",
              " [0.93, 53.300000000000004],\n",
              " [0.94, 51.0],\n",
              " [0.95, 47.599999999999994],\n",
              " [0.96, 44.6],\n",
              " [0.97, 40.2],\n",
              " [0.98, 34.0],\n",
              " [0.99, 26.8],\n",
              " [0.99, 26.8],\n",
              " [0.991, 26.200000000000003],\n",
              " [0.992, 24.8],\n",
              " [0.993, 23.3],\n",
              " [0.994, 21.9],\n",
              " [0.995, 19.8],\n",
              " [0.996, 17.7],\n",
              " [0.997, 14.099999999999998],\n",
              " [0.998, 11.600000000000001],\n",
              " [0.999, 7.3999999999999995]]"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UnknownMisclassify = []\n",
        "for i in range(90):\n",
        "  UnknownMisclassify.append([0.1 + 0.01*i,100*misclassifyunknown(0.1 + 0.01*i)])\n",
        "for i in range(10):\n",
        "  UnknownMisclassify.append([0.99 + 0.001*i,100*misclassifyunknown(0.99 + 0.001*i)])\n",
        "UnknownMisclassify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXvKcD4fM4VD"
      },
      "outputs": [],
      "source": [
        "def dontknow(threshold):\n",
        "  notknown = 0\n",
        "  for i in range(prediction_known.shape[0]):\n",
        "    if max(prediction_known[i]) <= threshold:\n",
        "      notknown += 1\n",
        "  return notknown/(prediction_known.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWInVYNk2bml",
        "outputId": "1c9a5cc6-e359-4e41-c786-f4d1b967feca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.1, 0.0],\n",
              " [0.11, 0.0],\n",
              " [0.12000000000000001, 0.0],\n",
              " [0.13, 0.0],\n",
              " [0.14, 0.0],\n",
              " [0.15000000000000002, 0.0],\n",
              " [0.16, 0.0],\n",
              " [0.17, 0.0],\n",
              " [0.18, 0.0],\n",
              " [0.19, 0.0],\n",
              " [0.2, 0.0],\n",
              " [0.21000000000000002, 0.0],\n",
              " [0.22, 0.0],\n",
              " [0.23, 0.0],\n",
              " [0.24000000000000002, 0.06666666666666667],\n",
              " [0.25, 0.06666666666666667],\n",
              " [0.26, 0.06666666666666667],\n",
              " [0.27, 0.06666666666666667],\n",
              " [0.28, 0.06666666666666667],\n",
              " [0.29000000000000004, 0.06666666666666667],\n",
              " [0.30000000000000004, 0.13333333333333333],\n",
              " [0.31, 0.13333333333333333],\n",
              " [0.32, 0.13333333333333333],\n",
              " [0.33, 0.13333333333333333],\n",
              " [0.33999999999999997, 0.2],\n",
              " [0.35, 0.2],\n",
              " [0.36, 0.2],\n",
              " [0.37, 0.2],\n",
              " [0.38, 0.2],\n",
              " [0.39, 0.4],\n",
              " [0.4, 0.46666666666666673],\n",
              " [0.41000000000000003, 0.46666666666666673],\n",
              " [0.42000000000000004, 0.6666666666666667],\n",
              " [0.43000000000000005, 0.7333333333333333],\n",
              " [0.44000000000000006, 0.8],\n",
              " [0.45000000000000007, 1.2],\n",
              " [0.45999999999999996, 1.2666666666666666],\n",
              " [0.47, 1.2666666666666666],\n",
              " [0.48, 1.4666666666666666],\n",
              " [0.49, 1.7333333333333332],\n",
              " [0.5, 2.1999999999999997],\n",
              " [0.51, 2.533333333333333],\n",
              " [0.52, 2.7333333333333334],\n",
              " [0.53, 3.0],\n",
              " [0.54, 3.266666666666666],\n",
              " [0.55, 3.5333333333333337],\n",
              " [0.56, 3.933333333333333],\n",
              " [0.5700000000000001, 4.333333333333334],\n",
              " [0.58, 4.6],\n",
              " [0.59, 4.933333333333334],\n",
              " [0.6, 5.666666666666666],\n",
              " [0.61, 5.800000000000001],\n",
              " [0.62, 6.0],\n",
              " [0.63, 6.466666666666667],\n",
              " [0.64, 6.666666666666667],\n",
              " [0.65, 7.066666666666667],\n",
              " [0.66, 7.3999999999999995],\n",
              " [0.67, 7.666666666666666],\n",
              " [0.6799999999999999, 7.866666666666666],\n",
              " [0.69, 8.200000000000001],\n",
              " [0.7, 8.533333333333333],\n",
              " [0.71, 8.733333333333333],\n",
              " [0.72, 9.2],\n",
              " [0.73, 9.466666666666667],\n",
              " [0.74, 10.0],\n",
              " [0.75, 10.666666666666668],\n",
              " [0.76, 11.133333333333335],\n",
              " [0.77, 11.533333333333333],\n",
              " [0.78, 11.799999999999999],\n",
              " [0.79, 12.4],\n",
              " [0.8, 13.0],\n",
              " [0.8099999999999999, 13.8],\n",
              " [0.82, 14.533333333333335],\n",
              " [0.83, 14.733333333333334],\n",
              " [0.84, 15.066666666666666],\n",
              " [0.85, 15.4],\n",
              " [0.86, 15.866666666666667],\n",
              " [0.87, 16.53333333333333],\n",
              " [0.88, 16.8],\n",
              " [0.89, 17.733333333333334],\n",
              " [0.9, 19.0],\n",
              " [0.91, 19.866666666666667],\n",
              " [0.92, 20.533333333333335],\n",
              " [0.93, 21.933333333333334],\n",
              " [0.94, 22.933333333333334],\n",
              " [0.95, 24.2],\n",
              " [0.96, 25.466666666666665],\n",
              " [0.97, 27.6],\n",
              " [0.98, 30.599999999999998],\n",
              " [0.99, 37.333333333333336],\n",
              " [0.99, 37.333333333333336],\n",
              " [0.991, 39.0],\n",
              " [0.992, 40.13333333333333],\n",
              " [0.993, 41.0],\n",
              " [0.994, 42.0],\n",
              " [0.995, 43.266666666666666],\n",
              " [0.996, 44.46666666666667],\n",
              " [0.997, 47.4],\n",
              " [0.998, 50.8],\n",
              " [0.999, 56.39999999999999]]"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DontKnowThresholding = []\n",
        "for i in range(90):\n",
        "  DontKnowThresholding.append([0.1 + 0.01*i,100*dontknow(0.1 + 0.01*i)])\n",
        "for i in range(10):\n",
        "  DontKnowThresholding.append([0.99 + 0.001*i,100*dontknow(0.99 + 0.001*i)])\n",
        "DontKnowThresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whvlD7tVNOew",
        "outputId": "b208aa06-3483-43d8-978b-ec2278a91a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The maximum value in the aligned array is: 0.999997\n"
          ]
        }
      ],
      "source": [
        "aligned_array = [element for row in prediction_unknown for element in row]\n",
        "max_value = max(aligned_array)\n",
        "\n",
        "print(\"The maximum value in the aligned array is:\", max_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svXBl0RNhe_c"
      },
      "outputs": [],
      "source": [
        "max_value = 0.9999986"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT16bp2f4xV",
        "outputId": "69875478-ace6-4b83-c253-a811f99ecdd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.068"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thresholding(max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb8_5G4-hP8l",
        "outputId": "5a10d3ca-1b88-4daa-d753-fe7ed659585c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9073333333333333"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dontknow(max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjStdK4hhS8K",
        "outputId": "1b45ebf5-90b5-4fb6-caad-7713cb81a0ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "misclassifyunknown(max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ihRU5ZxhbjN",
        "outputId": "baac1de0-f2d8-42f1-de92-15cf9dbed4c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "false_identify(0.9999986)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRKbUDYDNOSp",
        "outputId": "6444c2b7-3a49-4005-b29e-c289c6d9d9f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.0],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.06666666666666667],\n",
              " [89.60000000000001, 0.13333333333333333],\n",
              " [89.60000000000001, 0.13333333333333333],\n",
              " [89.60000000000001, 0.2],\n",
              " [89.46666666666667, 0.33333333333333337],\n",
              " [89.46666666666667, 0.4],\n",
              " [89.46666666666667, 0.4],\n",
              " [89.46666666666667, 0.4],\n",
              " [89.46666666666667, 0.46666666666666673],\n",
              " [89.4, 0.6],\n",
              " [89.33333333333333, 0.6666666666666667],\n",
              " [89.33333333333333, 0.8],\n",
              " [89.33333333333333, 0.8666666666666666],\n",
              " [89.13333333333333, 1.1333333333333333],\n",
              " [89.06666666666668, 1.2666666666666666],\n",
              " [89.0, 1.4000000000000001],\n",
              " [89.0, 1.5333333333333332],\n",
              " [88.8, 1.7999999999999998],\n",
              " [88.73333333333333, 1.866666666666667],\n",
              " [88.73333333333333, 2.0],\n",
              " [88.66666666666667, 2.1333333333333333],\n",
              " [88.33333333333333, 2.6],\n",
              " [88.06666666666668, 2.933333333333333],\n",
              " [87.93333333333334, 3.1333333333333333],\n",
              " [87.53333333333333, 3.733333333333334],\n",
              " [87.33333333333333, 4.133333333333333],\n",
              " [87.06666666666666, 4.466666666666667],\n",
              " [86.93333333333332, 4.866666666666666],\n",
              " [86.93333333333332, 5.0],\n",
              " [86.8, 5.133333333333334],\n",
              " [86.73333333333333, 5.6000000000000005],\n",
              " [86.53333333333333, 6.066666666666666],\n",
              " [86.2, 6.533333333333332],\n",
              " [85.93333333333332, 7.000000000000001],\n",
              " [85.73333333333333, 7.2666666666666675],\n",
              " [85.6, 7.466666666666668],\n",
              " [85.53333333333333, 7.866666666666666],\n",
              " [85.26666666666667, 8.200000000000001],\n",
              " [85.06666666666666, 8.666666666666668],\n",
              " [84.86666666666667, 9.066666666666666],\n",
              " [84.53333333333333, 9.666666666666666],\n",
              " [84.0, 10.333333333333334],\n",
              " [83.73333333333333, 10.866666666666665],\n",
              " [83.66666666666667, 11.133333333333335],\n",
              " [83.53333333333333, 11.333333333333332],\n",
              " [83.26666666666667, 11.733333333333333],\n",
              " [83.13333333333334, 12.2],\n",
              " [82.8, 12.8],\n",
              " [82.6, 13.200000000000001],\n",
              " [82.46666666666667, 13.333333333333334],\n",
              " [81.93333333333334, 14.133333333333335],\n",
              " [81.73333333333333, 14.399999999999999],\n",
              " [81.26666666666667, 15.066666666666666],\n",
              " [81.06666666666666, 15.466666666666667],\n",
              " [80.2, 16.466666666666665],\n",
              " [80.0, 17.0],\n",
              " [79.53333333333333, 17.533333333333335],\n",
              " [79.13333333333334, 17.933333333333334],\n",
              " [78.73333333333333, 18.4],\n",
              " [77.93333333333334, 19.400000000000002],\n",
              " [77.06666666666668, 20.533333333333335],\n",
              " [75.93333333333334, 21.866666666666667],\n",
              " [75.2, 22.866666666666667],\n",
              " [74.13333333333333, 24.133333333333333],\n",
              " [72.8, 25.666666666666664],\n",
              " [71.39999999999999, 27.400000000000002],\n",
              " [69.26666666666667, 29.666666666666668],\n",
              " [65.86666666666666, 33.46666666666667],\n",
              " [60.0, 39.666666666666664],\n",
              " [58.8, 40.93333333333333],\n",
              " [57.333333333333336, 42.46666666666667],\n",
              " [56.46666666666667, 43.333333333333336],\n",
              " [55.2, 44.6],\n",
              " [53.86666666666666, 45.93333333333333],\n",
              " [52.0, 47.8],\n",
              " [48.8, 51.13333333333333],\n",
              " [46.2, 53.733333333333334],\n",
              " [40.0, 59.93333333333334],\n",
              " [40.0, 59.93333333333334],\n",
              " [38.86666666666667, 61.06666666666667],\n",
              " [38.0, 61.93333333333333],\n",
              " [37.13333333333333, 62.8],\n",
              " [35.6, 64.33333333333333],\n",
              " [34.0, 65.93333333333334],\n",
              " [32.6, 67.33333333333333],\n",
              " [30.4, 69.53333333333333],\n",
              " [27.266666666666666, 72.66666666666667],\n",
              " [22.8, 77.13333333333333],\n",
              " [22.8, 77.13333333333333],\n",
              " [22.133333333333333, 77.8],\n",
              " [21.46666666666667, 78.46666666666667],\n",
              " [20.4, 79.53333333333333],\n",
              " [19.53333333333333, 80.4],\n",
              " [18.73333333333333, 81.2],\n",
              " [17.466666666666665, 82.46666666666667],\n",
              " [16.466666666666665, 83.46666666666667],\n",
              " [15.333333333333332, 84.6],\n",
              " [13.066666666666665, 86.86666666666667],\n",
              " [13.066666666666665, 86.86666666666667],\n",
              " [12.733333333333333, 87.2],\n",
              " [12.2, 87.73333333333333],\n",
              " [11.799999999999999, 88.13333333333333],\n",
              " [11.200000000000001, 88.73333333333333],\n",
              " [10.6, 89.33333333333333],\n",
              " [10.266666666666667, 89.66666666666666],\n",
              " [9.333333333333334, 90.60000000000001],\n",
              " [7.8, 92.13333333333334],\n",
              " [5.866666666666666, 94.13333333333334],\n",
              " [5.866666666666666, 94.13333333333334],\n",
              " [5.666666666666666, 94.33333333333334],\n",
              " [5.466666666666667, 94.53333333333333],\n",
              " [5.266666666666667, 94.73333333333333],\n",
              " [5.066666666666666, 94.93333333333334],\n",
              " [4.8, 95.19999999999999],\n",
              " [4.533333333333333, 95.46666666666667],\n",
              " [9.2, 90.73333333333333]]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DontKnowThresholdingANOTHER = []\n",
        "\n",
        "for i in range(89):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.1 + 0.01*i),100*dontknow(0.1 + 0.01*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.99 + 0.001*i),100*dontknow(0.99 + 0.001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.999 + 0.0001*i),100*dontknow(0.999 + 0.0001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.9999 + 0.00001*i),100*dontknow(0.9999 + 0.00001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.99999 + 0.000001*i),100*dontknow(0.99999 + 0.000001*i),])\n",
        "\n",
        "for i in range(7):\n",
        "  DontKnowThresholdingANOTHER.append([100*thresholding(0.999999 + 0.0000001*i),100*dontknow(0.999999 + 0.0000001*i),])\n",
        "\n",
        "DontKnowThresholdingANOTHER.append([100*thresholding(max_value),100*dontknow(max_value)])\n",
        "\n",
        "DontKnowThresholdingANOTHER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkK7Pwx6b9vA",
        "outputId": "df739dba-5ddd-45da-f451-e34c6e363457"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 100.0],\n",
              " [89.60000000000001, 99.9],\n",
              " [89.60000000000001, 99.9],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.60000000000001, 99.7],\n",
              " [89.46666666666667, 99.6],\n",
              " [89.46666666666667, 99.4],\n",
              " [89.46666666666667, 99.2],\n",
              " [89.46666666666667, 98.9],\n",
              " [89.46666666666667, 98.6],\n",
              " [89.4, 98.4],\n",
              " [89.33333333333333, 98.3],\n",
              " [89.33333333333333, 98.2],\n",
              " [89.33333333333333, 97.89999999999999],\n",
              " [89.13333333333333, 97.7],\n",
              " [89.06666666666668, 97.5],\n",
              " [89.0, 97.1],\n",
              " [89.0, 96.8],\n",
              " [88.8, 96.5],\n",
              " [88.73333333333333, 96.0],\n",
              " [88.73333333333333, 95.19999999999999],\n",
              " [88.66666666666667, 94.39999999999999],\n",
              " [88.33333333333333, 93.89999999999999],\n",
              " [88.06666666666668, 93.2],\n",
              " [87.93333333333334, 92.5],\n",
              " [87.53333333333333, 91.7],\n",
              " [87.33333333333333, 90.9],\n",
              " [87.06666666666666, 90.2],\n",
              " [86.93333333333332, 89.9],\n",
              " [86.93333333333332, 88.9],\n",
              " [86.8, 87.9],\n",
              " [86.73333333333333, 87.1],\n",
              " [86.53333333333333, 86.5],\n",
              " [86.2, 85.39999999999999],\n",
              " [85.93333333333332, 84.5],\n",
              " [85.73333333333333, 83.6],\n",
              " [85.6, 82.89999999999999],\n",
              " [85.53333333333333, 82.3],\n",
              " [85.26666666666667, 80.5],\n",
              " [85.06666666666666, 79.80000000000001],\n",
              " [84.86666666666667, 79.0],\n",
              " [84.53333333333333, 77.9],\n",
              " [84.0, 76.9],\n",
              " [83.73333333333333, 76.2],\n",
              " [83.66666666666667, 75.5],\n",
              " [83.53333333333333, 74.8],\n",
              " [83.26666666666667, 73.5],\n",
              " [83.13333333333334, 72.1],\n",
              " [82.8, 71.39999999999999],\n",
              " [82.6, 70.5],\n",
              " [82.46666666666667, 69.8],\n",
              " [81.93333333333334, 68.8],\n",
              " [81.73333333333333, 67.2],\n",
              " [81.26666666666667, 66.10000000000001],\n",
              " [81.06666666666666, 65.10000000000001],\n",
              " [80.2, 63.5],\n",
              " [80.0, 62.1],\n",
              " [79.53333333333333, 61.199999999999996],\n",
              " [79.13333333333334, 60.3],\n",
              " [78.73333333333333, 59.4],\n",
              " [77.93333333333334, 57.8],\n",
              " [77.06666666666668, 56.39999999999999],\n",
              " [75.93333333333334, 54.1],\n",
              " [75.2, 51.6],\n",
              " [74.13333333333333, 49.5],\n",
              " [72.8, 47.099999999999994],\n",
              " [71.39999999999999, 44.800000000000004],\n",
              " [69.26666666666667, 40.5],\n",
              " [65.86666666666666, 34.8],\n",
              " [60.0, 27.0],\n",
              " [58.8, 26.0],\n",
              " [57.333333333333336, 25.0],\n",
              " [56.46666666666667, 23.599999999999998],\n",
              " [55.2, 21.9],\n",
              " [53.86666666666666, 19.8],\n",
              " [52.0, 18.0],\n",
              " [48.8, 15.2],\n",
              " [46.2, 11.700000000000001],\n",
              " [40.0, 7.9],\n",
              " [40.0, 7.9],\n",
              " [38.86666666666667, 7.9],\n",
              " [38.0, 7.3],\n",
              " [37.13333333333333, 6.800000000000001],\n",
              " [35.6, 6.0],\n",
              " [34.0, 5.4],\n",
              " [32.6, 5.1],\n",
              " [30.4, 3.9],\n",
              " [27.266666666666666, 3.5000000000000004],\n",
              " [22.8, 2.3],\n",
              " [22.8, 2.3],\n",
              " [22.133333333333333, 2.0],\n",
              " [21.46666666666667, 2.0],\n",
              " [20.4, 1.7000000000000002],\n",
              " [19.53333333333333, 1.3],\n",
              " [18.73333333333333, 1.2],\n",
              " [17.466666666666665, 1.0999999999999999],\n",
              " [16.466666666666665, 1.0],\n",
              " [15.333333333333332, 0.8999999999999999],\n",
              " [13.066666666666665, 0.5],\n",
              " [13.066666666666665, 0.5],\n",
              " [12.733333333333333, 0.5],\n",
              " [12.2, 0.4],\n",
              " [11.799999999999999, 0.3],\n",
              " [11.200000000000001, 0.3],\n",
              " [10.6, 0.2],\n",
              " [10.266666666666667, 0.1],\n",
              " [9.333333333333334, 0.1],\n",
              " [7.8, 0.0],\n",
              " [5.866666666666666, 0.0],\n",
              " [5.866666666666666, 0.0],\n",
              " [5.666666666666666, 0.0],\n",
              " [5.466666666666667, 0.0],\n",
              " [5.266666666666667, 0.0],\n",
              " [5.066666666666666, 0.0],\n",
              " [4.8, 0.0],\n",
              " [4.533333333333333, 0.0],\n",
              " [9.2, 0.0]]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UnknownMisclassifyANOTHER = []\n",
        "for i in range(89):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.1 + 0.01*i),100*misclassifyunknown(0.1 + 0.01*i),])\n",
        "for i in range(10):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.99 + 0.001*i),100*misclassifyunknown(0.99 + 0.001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.999 + 0.0001*i),100*misclassifyunknown(0.999 + 0.0001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.9999 + 0.00001*i),100*misclassifyunknown(0.9999 + 0.00001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.99999 + 0.000001*i),100*misclassifyunknown(0.99999 + 0.000001*i),])\n",
        "\n",
        "for i in range(7):\n",
        "  UnknownMisclassifyANOTHER.append([100*thresholding(0.999999 + 0.0000001*i),100*misclassifyunknown(0.999999 + 0.0000001*i),])\n",
        "\n",
        "UnknownMisclassifyANOTHER.append([100*thresholding(max_value),100*misclassifyunknown(max_value),])\n",
        "\n",
        "UnknownMisclassifyANOTHER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe9DmTAKNOXR",
        "outputId": "47b2686d-ff71-4127-cc7d-210bfff77090"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.4],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.333333333333334],\n",
              " [89.60000000000001, 10.266666666666667],\n",
              " [89.60000000000001, 10.266666666666667],\n",
              " [89.60000000000001, 10.2],\n",
              " [89.46666666666667, 10.2],\n",
              " [89.46666666666667, 10.133333333333333],\n",
              " [89.46666666666667, 10.133333333333333],\n",
              " [89.46666666666667, 10.133333333333333],\n",
              " [89.46666666666667, 10.066666666666666],\n",
              " [89.4, 10.0],\n",
              " [89.33333333333333, 10.0],\n",
              " [89.33333333333333, 9.866666666666667],\n",
              " [89.33333333333333, 9.8],\n",
              " [89.13333333333333, 9.733333333333333],\n",
              " [89.06666666666668, 9.666666666666666],\n",
              " [89.0, 9.6],\n",
              " [89.0, 9.466666666666667],\n",
              " [88.8, 9.4],\n",
              " [88.73333333333333, 9.4],\n",
              " [88.73333333333333, 9.266666666666666],\n",
              " [88.66666666666667, 9.2],\n",
              " [88.33333333333333, 9.066666666666666],\n",
              " [88.06666666666668, 9.0],\n",
              " [87.93333333333334, 8.933333333333334],\n",
              " [87.53333333333333, 8.733333333333333],\n",
              " [87.33333333333333, 8.533333333333333],\n",
              " [87.06666666666666, 8.466666666666667],\n",
              " [86.93333333333332, 8.200000000000001],\n",
              " [86.93333333333332, 8.066666666666666],\n",
              " [86.8, 8.066666666666666],\n",
              " [86.73333333333333, 7.666666666666666],\n",
              " [86.53333333333333, 7.3999999999999995],\n",
              " [86.2, 7.2666666666666675],\n",
              " [85.93333333333332, 7.066666666666667],\n",
              " [85.73333333333333, 7.000000000000001],\n",
              " [85.6, 6.933333333333333],\n",
              " [85.53333333333333, 6.6000000000000005],\n",
              " [85.26666666666667, 6.533333333333332],\n",
              " [85.06666666666666, 6.266666666666667],\n",
              " [84.86666666666667, 6.066666666666666],\n",
              " [84.53333333333333, 5.800000000000001],\n",
              " [84.0, 5.666666666666666],\n",
              " [83.73333333333333, 5.4],\n",
              " [83.66666666666667, 5.2],\n",
              " [83.53333333333333, 5.133333333333334],\n",
              " [83.26666666666667, 5.0],\n",
              " [83.13333333333334, 4.666666666666667],\n",
              " [82.8, 4.3999999999999995],\n",
              " [82.6, 4.2],\n",
              " [82.46666666666667, 4.2],\n",
              " [81.93333333333334, 3.933333333333333],\n",
              " [81.73333333333333, 3.8666666666666667],\n",
              " [81.26666666666667, 3.6666666666666665],\n",
              " [81.06666666666666, 3.4666666666666663],\n",
              " [80.2, 3.3333333333333335],\n",
              " [80.0, 3.0],\n",
              " [79.53333333333333, 2.933333333333333],\n",
              " [79.13333333333334, 2.933333333333333],\n",
              " [78.73333333333333, 2.8666666666666667],\n",
              " [77.93333333333334, 2.666666666666667],\n",
              " [77.06666666666668, 2.4],\n",
              " [75.93333333333334, 2.1999999999999997],\n",
              " [75.2, 1.9333333333333333],\n",
              " [74.13333333333333, 1.7333333333333332],\n",
              " [72.8, 1.5333333333333332],\n",
              " [71.39999999999999, 1.2],\n",
              " [69.26666666666667, 1.0666666666666667],\n",
              " [65.86666666666666, 0.6666666666666667],\n",
              " [60.0, 0.33333333333333337],\n",
              " [58.8, 0.26666666666666666],\n",
              " [57.333333333333336, 0.2],\n",
              " [56.46666666666667, 0.2],\n",
              " [55.2, 0.2],\n",
              " [53.86666666666666, 0.2],\n",
              " [52.0, 0.2],\n",
              " [48.8, 0.06666666666666667],\n",
              " [46.2, 0.06666666666666667],\n",
              " [40.0, 0.06666666666666667],\n",
              " [40.0, 0.06666666666666667],\n",
              " [38.86666666666667, 0.06666666666666667],\n",
              " [38.0, 0.06666666666666667],\n",
              " [37.13333333333333, 0.06666666666666667],\n",
              " [35.6, 0.06666666666666667],\n",
              " [34.0, 0.06666666666666667],\n",
              " [32.6, 0.06666666666666667],\n",
              " [30.4, 0.06666666666666667],\n",
              " [27.266666666666666, 0.06666666666666667],\n",
              " [22.8, 0.06666666666666667],\n",
              " [22.8, 0.06666666666666667],\n",
              " [22.133333333333333, 0.06666666666666667],\n",
              " [21.46666666666667, 0.06666666666666667],\n",
              " [20.4, 0.06666666666666667],\n",
              " [19.53333333333333, 0.06666666666666667],\n",
              " [18.73333333333333, 0.06666666666666667],\n",
              " [17.466666666666665, 0.06666666666666667],\n",
              " [16.466666666666665, 0.06666666666666667],\n",
              " [15.333333333333332, 0.06666666666666667],\n",
              " [13.066666666666665, 0.06666666666666667],\n",
              " [13.066666666666665, 0.06666666666666667],\n",
              " [12.733333333333333, 0.06666666666666667],\n",
              " [12.2, 0.06666666666666667],\n",
              " [11.799999999999999, 0.06666666666666667],\n",
              " [11.200000000000001, 0.06666666666666667],\n",
              " [10.6, 0.06666666666666667],\n",
              " [10.266666666666667, 0.06666666666666667],\n",
              " [9.333333333333334, 0.06666666666666667],\n",
              " [7.8, 0.06666666666666667],\n",
              " [5.866666666666666, 0.0],\n",
              " [5.866666666666666, 0.0],\n",
              " [5.666666666666666, 0.0],\n",
              " [5.466666666666667, 0.0],\n",
              " [5.266666666666667, 0.0],\n",
              " [5.066666666666666, 0.0],\n",
              " [4.8, 0.0],\n",
              " [4.533333333333333, 0.0],\n",
              " [9.2, 0.06666666666666667]]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FalseIdentifyANOTHER = []\n",
        "for i in range(89):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.1 + 0.01*i),100*false_identify(0.1 + 0.01*i),])\n",
        "for i in range(10):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.99 + 0.001*i),100*false_identify(0.99 + 0.001*i)])\n",
        "\n",
        "for i in range(10):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.999 + 0.0001*i),100*false_identify(0.999 + 0.0001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.9999 + 0.00001*i),100*false_identify(0.9999 + 0.00001*i),])\n",
        "\n",
        "for i in range(10):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.99999 + 0.000001*i),100*false_identify(0.99999 + 0.000001*i),])\n",
        "\n",
        "for i in range(7):\n",
        "  FalseIdentifyANOTHER.append([100*thresholding(0.999999 + 0.0000001*i),100*false_identify(0.999999 + 0.0000001*i),])\n",
        "\n",
        "\n",
        "FalseIdentifyANOTHER.append([100*thresholding(max_value),100*false_identify(max_value),])\n",
        "FalseIdentifyANOTHER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Vf4U_EQDh6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "astEHCwEWNFf"
      },
      "source": [
        "# Class-adaptive threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgZEtA56usFB"
      },
      "source": [
        "# Run #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEg9x6-vurgQ"
      },
      "outputs": [],
      "source": [
        "prediction_known = prediction_known_ensemble_1\n",
        "prediction_unknown = prediction_unknown_ensemble_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tyc8FzcWUk-"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores = []\n",
        "for i in range(15):\n",
        "  ThresholdNeverSeenBefores.append(np.max(prediction_unknown[:,i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp_hvYS9WP6J",
        "outputId": "1c8b9f81-bf26-4779-fe75-114b0c39f066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count0 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i,0] > ThresholdNeverSeenBefores[0] and np.argmax(prediction_known[i]) == 0:\n",
        "    C_count0 += 1\n",
        "  else:\n",
        "    C_count0 += 0\n",
        "C_count0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAu3K9nEWP8P",
        "outputId": "34350f39-d0bb-42d4-da57-8f74968e6ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake0 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[i]) != 0 and max(prediction_known[i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[i])]:\n",
        "    mistake0 += 1\n",
        "    print(i)\n",
        "  else:\n",
        "    mistake0 += 0\n",
        "mistake0\n",
        "print(\"Number of mistakes in this class:\",mistake0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHzot9WdXkC7",
        "outputId": "794d6fe4-e56b-409c-afc4-750abdff6391"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count1 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[100 + i,1] > ThresholdNeverSeenBefores[1] and np.argmax(prediction_known[100 + i]) == 1:\n",
        "    C_count1 += 1\n",
        "  else:\n",
        "    C_count1 += 0\n",
        "C_count1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4RVufLtXkAR",
        "outputId": "ddf47385-c2d5-43aa-baf9-daad2c5e149c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake1 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[100 + i]) != 1 and max(prediction_known[100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[100 + i])]:\n",
        "    mistake1 += 1\n",
        "    print(100 + i)\n",
        "  else:\n",
        "    mistake1 += 0\n",
        "mistake1\n",
        "print(\"Number of mistakes in this class:\",mistake1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xZ6sVoHNNI0"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[144])] = max(prediction_known[144])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG9PHsHdNQdK",
        "outputId": "837369da-7cc4-4bd8-97c7-fbe886f98ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake1 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[100 + i]) != 1 and max(prediction_known[100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[100 + i])]:\n",
        "    mistake1 += 1\n",
        "    print(100 + i)\n",
        "  else:\n",
        "    mistake1 += 0\n",
        "mistake1\n",
        "print(\"Number of mistakes in this class:\",mistake1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmmpoficmsIo",
        "outputId": "c3c85d35-602e-4de3-fd65-1b71e6ca58b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count2 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[200 + i,2] > ThresholdNeverSeenBefores[2] and np.argmax(prediction_known[200 + i]) == 2:\n",
        "    C_count2 += 1\n",
        "  else:\n",
        "    C_count2 += 0\n",
        "C_count2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTiQq-WKmsMB",
        "outputId": "e18f3969-eaed-4b9a-d064-88a02e11fa7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake2 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[200 + i]) != 2 and max(prediction_known[200 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[200 + i])]:\n",
        "    mistake2 += 1\n",
        "    print(200 + i)\n",
        "  else:\n",
        "    mistake2 += 0\n",
        "mistake2\n",
        "print(\"Number of mistakes in this class:\",mistake2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsNwlMBAmsN3",
        "outputId": "fc325a10-3057-4024-9b17-71b2bfed5b48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count3 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[300 + i,3] > ThresholdNeverSeenBefores[3] and np.argmax(prediction_known[300 + i]) == 3:\n",
        "    C_count3 += 1\n",
        "  else:\n",
        "    C_count3 += 0\n",
        "C_count3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP9oRkpvmsPv",
        "outputId": "31b7c6b8-a4f2-449e-aa5f-79e9159a4232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake3 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[300 + i]) != 3 and max(prediction_known[300 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[300 + i])]:\n",
        "    mistake3 += 1\n",
        "    print(300 + i)\n",
        "  else:\n",
        "    mistake3 += 0\n",
        "mistake3\n",
        "print(\"Number of mistakes in this class:\",mistake3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGdmlHmUmsSi",
        "outputId": "de26067b-5b8d-40c9-d64e-21fb9eede3a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count4 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[400 + i,4] > ThresholdNeverSeenBefores[4] and np.argmax(prediction_known[400 + i]) == 4:\n",
        "    C_count4 += 1\n",
        "  else:\n",
        "    C_count4 += 0\n",
        "C_count4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTXAEphBmsU_",
        "outputId": "6fdd7c37-238b-4283-8180-df39f7939fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake4 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[400 + i]) != 4 and max(prediction_known[400 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[400 + i])]:\n",
        "    mistake4 += 1\n",
        "    print(400 + i)\n",
        "  else:\n",
        "    mistake4 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwuWW0SkqIB_",
        "outputId": "2a1410ea-fb80-4164-b18b-a96dab81eabd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 177,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count5 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[500 + i,5] > ThresholdNeverSeenBefores[5] and np.argmax(prediction_known[500 + i]) == 5:\n",
        "    C_count5 += 1\n",
        "  else:\n",
        "    C_count5 += 0\n",
        "C_count5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMRqQVgIqKWx",
        "outputId": "6a2aaf94-0c91-49ad-eb00-a865f69d6b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "501\n",
            "515\n",
            "518\n",
            "524\n",
            "526\n",
            "532\n",
            "533\n",
            "535\n",
            "540\n",
            "553\n",
            "559\n",
            "560\n",
            "561\n",
            "565\n",
            "570\n",
            "572\n",
            "579\n",
            "Number of mistakes in this class: 17\n"
          ]
        }
      ],
      "source": [
        "mistake5 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[500 + i]) != 5 and max(prediction_known[500 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[500 + i])]:\n",
        "    mistake5 += 1\n",
        "    print(500 + i)\n",
        "  else:\n",
        "    mistake5 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrUFhh6YDNe9"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[570])] = max(prediction_known[570])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbjVSXkUDMy8",
        "outputId": "e5e8494f-8e67-4aff-953f-92280be1d30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake5 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[500 + i]) != 5 and max(prediction_known[500 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[500 + i])]:\n",
        "    mistake5 += 1\n",
        "    print(500 + i)\n",
        "  else:\n",
        "    mistake5 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJt2-_RjqhZM",
        "outputId": "9fe55996-865c-4f65-bb9b-da1257befea7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count6 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[600 + i,6] > ThresholdNeverSeenBefores[6] and np.argmax(prediction_known[600 + i]) == 6:\n",
        "    C_count6 += 1\n",
        "  else:\n",
        "    C_count6 += 0\n",
        "C_count6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ0sklVOqhbU",
        "outputId": "e30bfae1-b28c-4a18-a094-7c20edbe485d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "605\n",
            "607\n",
            "609\n",
            "610\n",
            "639\n",
            "658\n",
            "666\n",
            "670\n",
            "688\n",
            "690\n",
            "696\n",
            "697\n",
            "Number of mistakes in this class: 12\n"
          ]
        }
      ],
      "source": [
        "mistake6 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[600 + i]) != 6 and max(prediction_known[600 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[600 + i])]:\n",
        "    mistake6 += 1\n",
        "    print(600 + i)\n",
        "  else:\n",
        "    mistake6 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46R_9FV_Nv0u"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[610])] = max(prediction_known[610])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRmK5EjkN0bY",
        "outputId": "bb7da245-dcc9-4023-f622-675013c38329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake6 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[600 + i]) != 6 and max(prediction_known[600 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[600 + i])]:\n",
        "    mistake6 += 1\n",
        "    print(600 + i)\n",
        "  else:\n",
        "    mistake6 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMsrIQt1qhe2",
        "outputId": "50890793-c8e2-43e8-b770-faa5a993792c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count7 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[700 + i,7] > ThresholdNeverSeenBefores[7]  and np.argmax(prediction_known[700 + i]) == 7:\n",
        "    C_count7 += 1\n",
        "  else:\n",
        "    C_count7 += 0\n",
        "C_count7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gau4TuhUqhiG",
        "outputId": "8061d2b5-f32c-468a-8547-a1721342b0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "703\n",
            "706\n",
            "795\n",
            "Number of mistakes in this class: 3\n"
          ]
        }
      ],
      "source": [
        "mistake7 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[700 + i]) != 7 and max(prediction_known[700 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[700 + i])]:\n",
        "    mistake7 += 1\n",
        "    print(700 + i)\n",
        "  else:\n",
        "    mistake7 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5m4X1FoDkTq"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[795])] = max(prediction_known[795])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-FPdzebDpOr",
        "outputId": "7bae8eec-bd06-4982-ff9f-e196a5ab039b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake7 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[700 + i]) != 7 and max(prediction_known[700 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[700 + i])]:\n",
        "    mistake7 += 1\n",
        "    print(700 + i)\n",
        "  else:\n",
        "    mistake7 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyufo5sUqhkP",
        "outputId": "9023d4ef-674b-4465-8e27-28332d693729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count8 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[800 + i,8] > ThresholdNeverSeenBefores[8] and np.argmax(prediction_known[800 + i]) == 8:\n",
        "    C_count8 += 1\n",
        "  else:\n",
        "    C_count8 += 0\n",
        "C_count8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqBpRvSWqhnj",
        "outputId": "35e11884-71cf-4ae6-9b26-29d3b2244eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "891\n",
            "Number of mistakes in this class: 1\n"
          ]
        }
      ],
      "source": [
        "mistake8 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[800 + i]) != 8 and max(prediction_known[800 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[800 + i])]:\n",
        "    mistake8 += 1\n",
        "    print(800 + i)\n",
        "  else:\n",
        "    mistake8 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79kQww80Dt9S"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[891])] = max(prediction_known[891])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK_1ke3cDwz-",
        "outputId": "36ac87de-b055-4112-c21e-65e1d23160ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake8 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[800 + i]) != 8 and max(prediction_known[800 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[800 + i])]:\n",
        "    mistake8 += 1\n",
        "    print(800 + i)\n",
        "  else:\n",
        "    mistake8 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbQcIVdBqhpv",
        "outputId": "5d6c6f03-a818-457a-af7a-c697e43860b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count9 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[900 + i,9] > ThresholdNeverSeenBefores[9] and np.argmax(prediction_known[900 + i]) == 9:\n",
        "    C_count9 += 1\n",
        "  else:\n",
        "    C_count9 += 0\n",
        "C_count9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6TeE6FjrOKT",
        "outputId": "1a66fa71-1296-45eb-daec-ee5df62ad305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake9 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[900 + i]) != 9 and max(prediction_known[900 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[900 + i])]:\n",
        "    mistake9 += 1\n",
        "    print(900 + i)\n",
        "  else:\n",
        "    mistake9 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfyOeyVNONgV",
        "outputId": "b56155d7-9228-488e-ae24-b877348807af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count10 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i + 10*100,10] > ThresholdNeverSeenBefores[10] and np.argmax(prediction_known[i + 10*100]) == 10:\n",
        "    C_count10 += 1\n",
        "  else:\n",
        "    C_count10 += 0\n",
        "C_count10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekt_bBxvONcA",
        "outputId": "5d012697-eb1c-4859-afa4-e5a8114a30ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake10 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1000 + i]) != 10 and max(prediction_known[1000 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1000 + i])]:\n",
        "    mistake10 += 1\n",
        "    print(1000 + i)\n",
        "  else:\n",
        "    mistake10 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6byw4L6ONjQ",
        "outputId": "08e35171-d319-45d3-d774-5690647b6f09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count11 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i + 11*100,11] > ThresholdNeverSeenBefores[11] and np.argmax(prediction_known[i + 11*100]) == 11:\n",
        "    C_count11 += 1\n",
        "  else:\n",
        "    C_count11 += 0\n",
        "C_count11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTo9fjq0ONn7",
        "outputId": "08404273-ec78-4538-c78f-603eda8afa79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1184\n",
            "Number of mistakes in this class: 1\n"
          ]
        }
      ],
      "source": [
        "mistake11 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1100 + i]) != 11 and max(prediction_known[1100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1100 + i])]:\n",
        "    mistake11 += 1\n",
        "    print(1100 + i)\n",
        "  else:\n",
        "    mistake11 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAlMQydpD55o"
      },
      "outputs": [],
      "source": [
        "ThresholdNeverSeenBefores[np.argmax(prediction_known[1184])] = max(prediction_known[1184])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsoxMk22D5_j",
        "outputId": "be6a1928-e1d8-4165-a745-66fe68767f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake11 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1100 + i]) != 11 and max(prediction_known[1100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1100 + i])]:\n",
        "    mistake11 += 1\n",
        "    print(1100 + i)\n",
        "  else:\n",
        "    mistake11 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZEkgZKTQHnK",
        "outputId": "5129ece7-a8d6-4470-e01b-6d1019930786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count12 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i + 12*100,12] > ThresholdNeverSeenBefores[12] and np.argmax(prediction_known[i + 12*100]) == 12:\n",
        "    C_count12 += 1\n",
        "  else:\n",
        "    C_count12 += 0\n",
        "C_count12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B0SAhUMQHpb",
        "outputId": "493420f6-858b-4a4d-c7e3-9ed8698fc85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake12 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1200 + i]) != 12 and max(prediction_known[1200 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1200 + i])]:\n",
        "    mistake12 += 1\n",
        "    print(1200 + i)\n",
        "  else:\n",
        "    mistake12 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIpPnc7NQHtA",
        "outputId": "cdbf1145-dac4-4fe6-a335-75e140c6bb98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count13 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i + 13*100,13] > ThresholdNeverSeenBefores[13] and np.argmax(prediction_known[i + 13*100]) == 13:\n",
        "    C_count13 += 1\n",
        "  else:\n",
        "    C_count13 += 0\n",
        "C_count13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-6dJoSlQHvl",
        "outputId": "e0edb37d-3070-4fac-f7c8-66b017d38cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake13 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1300 + i]) != 13 and max(prediction_known[1300 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1300 + i])]:\n",
        "    mistake13 += 1\n",
        "    print(1300 + i)\n",
        "  else:\n",
        "    mistake13 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulMey8s2QHy6",
        "outputId": "0003b370-fb5c-4b27-9319-c5f5b94dc774"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "69"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "C_count14 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i + 14*100,14] > ThresholdNeverSeenBefores[14] and np.argmax(prediction_known[i + 14*100]) == 14:\n",
        "    C_count14 += 1\n",
        "  else:\n",
        "    C_count14 += 0\n",
        "C_count14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3rfjenwQH37",
        "outputId": "87063695-03f5-4da4-fb32-e6afb8201dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of mistakes in this class: 0\n"
          ]
        }
      ],
      "source": [
        "mistake14 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1400 + i]) != 14 and max(prediction_known[1400 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1400 + i])]:\n",
        "    mistake14 += 1\n",
        "    print(1400 + i)\n",
        "  else:\n",
        "    mistake14 += 0\n",
        "print(\"Number of mistakes in this class:\",mistake14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFTqb2dlQH-K",
        "outputId": "3d598d7c-3337-4b0d-ea66-887b6e73d35a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mistake0 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[i]) != 0 and max(prediction_known[i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[i])]:\n",
        "    mistake0 += 1\n",
        "    print(i)\n",
        "  else:\n",
        "    mistake0 += 0\n",
        "\n",
        "mistake1 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[100 + i]) != 1 and max(prediction_known[100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[100 + i])]:\n",
        "    mistake1 += 1\n",
        "    print(100 + i)\n",
        "  else:\n",
        "    mistake1 += 0\n",
        "\n",
        "mistake2 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[200 + i]) != 2 and max(prediction_known[200 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[200 + i])]:\n",
        "    mistake2 += 1\n",
        "    print(200 + i)\n",
        "  else:\n",
        "    mistake2 += 0\n",
        "\n",
        "mistake3 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[300 + i]) != 3 and max(prediction_known[300 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[300 + i])]:\n",
        "    mistake3 += 1\n",
        "    print(300 + i)\n",
        "  else:\n",
        "    mistake3 += 0\n",
        "\n",
        "mistake4 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[400 + i]) != 4 and max(prediction_known[400 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[400 + i])]:\n",
        "    mistake4 += 1\n",
        "    print(400 + i)\n",
        "  else:\n",
        "    mistake4 += 0\n",
        "\n",
        "mistake5 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[500 + i]) != 5 and max(prediction_known[500 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[500 + i])]:\n",
        "    mistake5 += 1\n",
        "    print(500 + i)\n",
        "  else:\n",
        "    mistake5 += 0\n",
        "\n",
        "mistake6 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[600 + i]) != 6 and max(prediction_known[600 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[600 + i])]:\n",
        "    mistake6 += 1\n",
        "    print(600 + i)\n",
        "  else:\n",
        "    mistake6 += 0\n",
        "\n",
        "mistake7 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[700 + i]) != 7 and max(prediction_known[700 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[700 + i])]:\n",
        "    mistake7 += 1\n",
        "    print(700 + i)\n",
        "  else:\n",
        "    mistake7 += 0\n",
        "\n",
        "mistake8 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[800 + i]) != 8 and max(prediction_known[800 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[800 + i])]:\n",
        "    mistake8 += 1\n",
        "    print(800 + i)\n",
        "  else:\n",
        "    mistake8 += 0\n",
        "\n",
        "mistake9 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[900 + i]) != 9 and max(prediction_known[900 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[900 + i])]:\n",
        "    mistake9 += 1\n",
        "    print(900 + i)\n",
        "  else:\n",
        "    mistake9 += 0\n",
        "\n",
        "mistake10 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1000 + i]) != 10 and max(prediction_known[1000 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1000 + i])]:\n",
        "    mistake10 += 1\n",
        "    print(1000 + i)\n",
        "  else:\n",
        "    mistake10 += 0\n",
        "\n",
        "mistake11 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1100 + i]) != 11 and max(prediction_known[1100 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1100 + i])]:\n",
        "    mistake11 += 1\n",
        "    print(1100 + i)\n",
        "  else:\n",
        "    mistake11 += 0\n",
        "\n",
        "mistake12 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1200 + i]) != 12 and max(prediction_known[1200 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1200 + i])]:\n",
        "    mistake12 += 1\n",
        "    print(1200 + i)\n",
        "  else:\n",
        "    mistake12 += 0\n",
        "\n",
        "\n",
        "mistake13 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1300 + i]) != 13 and max(prediction_known[1300 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1300 + i])]:\n",
        "    mistake13 += 1\n",
        "    print(1300 + i)\n",
        "  else:\n",
        "    mistake13 += 0\n",
        "\n",
        "\n",
        "mistake14 = 0\n",
        "for i in range(100):\n",
        "  if np.argmax(prediction_known[1400 + i]) != 14 and max(prediction_known[1400 + i]) > ThresholdNeverSeenBefores[np.argmax(prediction_known[1400 + i])]:\n",
        "    mistake14 += 1\n",
        "    print(1400 + i)\n",
        "  else:\n",
        "    mistake14 += 0\n",
        "\n",
        "NumberOfMistakesAfterThrAdj = [mistake0, mistake1, mistake2, mistake3, mistake4, mistake5, mistake6, mistake7,mistake8,mistake9,mistake10,mistake11,mistake12,mistake13,mistake14]\n",
        "NumberOfMistakesAfterThrAdj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXF3VkIGQIA1"
      },
      "outputs": [],
      "source": [
        "C_count0 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[i,0] > ThresholdNeverSeenBefores[0] and np.argmax(prediction_known[i]) == 0:\n",
        "    C_count0 += 1\n",
        "  else:\n",
        "    C_count0 += 0\n",
        "\n",
        "C_count1 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[100 + i,1] > ThresholdNeverSeenBefores[1] and np.argmax(prediction_known[100 + i]) == 1:\n",
        "    C_count1 += 1\n",
        "  else:\n",
        "    C_count1 += 0\n",
        "\n",
        "C_count2 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[200 + i,2] > ThresholdNeverSeenBefores[2] and np.argmax(prediction_known[200 + i]) == 2:\n",
        "    C_count2 += 1\n",
        "  else:\n",
        "    C_count2 += 0\n",
        "\n",
        "C_count3 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[300 + i,3] > ThresholdNeverSeenBefores[3] and np.argmax(prediction_known[300 + i]) == 3:\n",
        "    C_count3 += 1\n",
        "  else:\n",
        "    C_count3 += 0\n",
        "\n",
        "C_count4 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[400 + i,4] > ThresholdNeverSeenBefores[4] and np.argmax(prediction_known[400 + i]) == 4:\n",
        "    C_count4 += 1\n",
        "  else:\n",
        "    C_count4 += 0\n",
        "\n",
        "C_count5 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[500 + i,5] > ThresholdNeverSeenBefores[5] and np.argmax(prediction_known[500 + i]) == 5:\n",
        "    C_count5 += 1\n",
        "  else:\n",
        "    C_count5 += 0\n",
        "\n",
        "C_count6 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[600 + i,6] > ThresholdNeverSeenBefores[6] and np.argmax(prediction_known[600 + i]) == 6:\n",
        "    C_count6 += 1\n",
        "  else:\n",
        "    C_count6 += 0\n",
        "\n",
        "C_count7 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[700 + i,7] > ThresholdNeverSeenBefores[7]  and np.argmax(prediction_known[700 + i]) == 7:\n",
        "    C_count7 += 1\n",
        "  else:\n",
        "    C_count7 += 0\n",
        "\n",
        "C_count8 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[800 + i,8] > ThresholdNeverSeenBefores[8] and np.argmax(prediction_known[800 + i]) == 8:\n",
        "    C_count8 += 1\n",
        "  else:\n",
        "    C_count8 += 0\n",
        "\n",
        "C_count9 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[900 + i,9] > ThresholdNeverSeenBefores[9] and np.argmax(prediction_known[900 + i]) == 9:\n",
        "    C_count9 += 1\n",
        "  else:\n",
        "    C_count9 += 0\n",
        "\n",
        "C_count10 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[1000 + i,10] > ThresholdNeverSeenBefores[10] and np.argmax(prediction_known[1000 + i]) == 10:\n",
        "    C_count10 += 1\n",
        "  else:\n",
        "    C_count10 += 0\n",
        "\n",
        "C_count11 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[1100 + i,11] > ThresholdNeverSeenBefores[11] and np.argmax(prediction_known[1100 + i]) == 11:\n",
        "    C_count11 += 1\n",
        "  else:\n",
        "    C_count11 += 0\n",
        "\n",
        "\n",
        "C_count12 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[1200 + i,12] > ThresholdNeverSeenBefores[12] and np.argmax(prediction_known[1200 + i]) == 12:\n",
        "    C_count12 += 1\n",
        "  else:\n",
        "    C_count12 += 0\n",
        "\n",
        "\n",
        "C_count13 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[1300 + i,13] > ThresholdNeverSeenBefores[13] and np.argmax(prediction_known[1300 + i]) == 13:\n",
        "    C_count13 += 1\n",
        "  else:\n",
        "    C_count13 += 0\n",
        "\n",
        "\n",
        "C_count14 = 0\n",
        "for i in range(100):\n",
        "  if prediction_known[1400 + i,14] > ThresholdNeverSeenBefores[14] and np.argmax(prediction_known[1400 + i]) == 14:\n",
        "    C_count14 += 1\n",
        "  else:\n",
        "    C_count14 += 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tCq14c7KIKZ",
        "outputId": "f66ff1be-cf78-457b-8069-95f4eb85bc50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 16],\n",
              " [2, 51],\n",
              " [3, 12],\n",
              " [4, 73],\n",
              " [5, 53],\n",
              " [6, 37],\n",
              " [7, 45],\n",
              " [8, 82],\n",
              " [9, 90],\n",
              " [10, 9]]"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FinalDistributionOver10ClassesRun1 = [[1,C_count0],[2,C_count1],[3,C_count2],[4,C_count3],[5,C_count4],[6,C_count5],[7,C_count6],[8,C_count7],[9,C_count8],[10,C_count9]]\n",
        "FinalDistributionOver10ClassesRun1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDX1bRdiKKii",
        "outputId": "7912708d-1d71-4772-b64d-11ccdc322fa9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 16],\n",
              " [2, 51],\n",
              " [3, 12],\n",
              " [4, 73],\n",
              " [5, 53],\n",
              " [6, 37],\n",
              " [7, 45],\n",
              " [8, 82],\n",
              " [9, 90],\n",
              " [10, 9],\n",
              " [11, 46],\n",
              " [12, 0],\n",
              " [13, 0],\n",
              " [14, 99],\n",
              " [15, 69]]"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FinalDistributionOver15ClassesRun1 = [[1,C_count0],[2,C_count1],[3,C_count2],[4,C_count3],[5,C_count4],[6,C_count5],[7,C_count6],[8,C_count7],[9,C_count8],[10,C_count9],[11,C_count10],[12,C_count11],[13,C_count12],[14,C_count13],[15,C_count14]]\n",
        "FinalDistributionOver15ClassesRun1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8xTrLFWLVPX",
        "outputId": "b83ec6ec-ac9c-49d2-a10d-dba25f20bd02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.468"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(C_count0 + C_count1 + C_count2 + C_count3 + C_count4 + C_count5 + C_count6 + C_count7 + C_count8 + C_count9)/1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t0rgKFRLij_",
        "outputId": "0090d4c3-30a6-41fc-b377-911de093138d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.45466666666666666"
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(C_count0 + C_count1 + C_count2 + C_count3 + C_count4 + C_count5 + C_count6 + C_count7 + C_count8 + C_count9 + C_count10 + C_count11 + C_count12 + C_count13 + C_count14)/1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n-IjdrQxXFX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LxQwEOhUQCd",
        "outputId": "be291072-5774-49a3-df01-169c6270831f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 11.25, 3.112474899497183]\n",
            "[2, 46.25, 4.763139720814412]\n",
            "[3, 11.5, 0.8660254037844386]\n",
            "[4, 75.75, 2.947456530637899]\n",
            "[5, 51.75, 11.519006033508273]\n",
            "[6, 35.25, 2.0463381929681126]\n",
            "[7, 54.25, 6.098155458825234]\n",
            "[8, 82.0, 0.0]\n",
            "[9, 90.0, 0.0]\n",
            "[10, 8.25, 2.5860201081971503]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "all_datasets = [FinalDistributionOver10ClassesRun1, FinalDistributionOver10ClassesRun2, FinalDistributionOver10ClassesRun3, FinalDistributionOver10ClassesRun4]\n",
        "\n",
        "new_dataset = []\n",
        "\n",
        "for x_value in range(1, 11):\n",
        "    y_values = []\n",
        "\n",
        "    for dataset in all_datasets:\n",
        "        for data_point in dataset:\n",
        "            if data_point[0] == x_value:\n",
        "                y_values.append(data_point[1])\n",
        "\n",
        "    avg_y = np.mean(y_values)\n",
        "    std_y = np.std(y_values)\n",
        "\n",
        "    new_dataset.append([x_value, avg_y, std_y])\n",
        "\n",
        "for data_point in new_dataset:\n",
        "    print(data_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiOAa6olqnym",
        "outputId": "669044e7-39ab-447e-94af-dd476aaec616"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 11.25, 3.112474899497183],\n",
              " [2, 46.25, 4.763139720814412],\n",
              " [3, 11.5, 0.8660254037844386],\n",
              " [4, 75.75, 2.947456530637899],\n",
              " [5, 51.75, 11.519006033508273],\n",
              " [6, 35.25, 2.0463381929681126],\n",
              " [7, 54.25, 6.098155458825234],\n",
              " [8, 82.0, 0.0],\n",
              " [9, 90.0, 0.0],\n",
              " [10, 8.25, 2.5860201081971503]]"
            ]
          },
          "execution_count": 409,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WihmOq4ts158",
        "outputId": "bcc3f91e-5405-458a-c7d6-a3d5754cb551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 11.25, 3.112474899497183]\n",
            "[2, 46.25, 4.763139720814412]\n",
            "[3, 11.5, 0.8660254037844386]\n",
            "[4, 75.75, 2.947456530637899]\n",
            "[5, 51.75, 11.519006033508273]\n",
            "[6, 35.25, 2.0463381929681126]\n",
            "[7, 54.25, 6.098155458825234]\n",
            "[8, 82.0, 0.0]\n",
            "[9, 90.0, 0.0]\n",
            "[10, 8.25, 2.5860201081971503]\n",
            "[11, 38.25, 6.339361166552983]\n",
            "[12, 0.25, 0.4330127018922193]\n",
            "[13, 1.25, 1.299038105676658]\n",
            "[14, 99.0, 0.0]\n",
            "[15, 72.0, 2.1213203435596424]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "all_datasets = [FinalDistributionOver15ClassesRun1, FinalDistributionOver15ClassesRun2, FinalDistributionOver15ClassesRun3, FinalDistributionOver15ClassesRun4]\n",
        "\n",
        "new_dataset = []\n",
        "\n",
        "for x_value in range(1, 16):\n",
        "    y_values = []\n",
        "\n",
        "    for dataset in all_datasets:\n",
        "        for data_point in dataset:\n",
        "            if data_point[0] == x_value:\n",
        "                y_values.append(data_point[1])\n",
        "\n",
        "    avg_y = np.mean(y_values)\n",
        "    std_y = np.std(y_values)\n",
        "\n",
        "    new_dataset.append([x_value, avg_y, std_y])\n",
        "\n",
        "for data_point in new_dataset:\n",
        "    print(data_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH760zIIs9m-",
        "outputId": "74ad0500-e435-4303-f77c-4e48933a9484"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 11.25, 3.112474899497183],\n",
              " [2, 46.25, 4.763139720814412],\n",
              " [3, 11.5, 0.8660254037844386],\n",
              " [4, 75.75, 2.947456530637899],\n",
              " [5, 51.75, 11.519006033508273],\n",
              " [6, 35.25, 2.0463381929681126],\n",
              " [7, 54.25, 6.098155458825234],\n",
              " [8, 82.0, 0.0],\n",
              " [9, 90.0, 0.0],\n",
              " [10, 8.25, 2.5860201081971503],\n",
              " [11, 38.25, 6.339361166552983],\n",
              " [12, 0.25, 0.4330127018922193],\n",
              " [13, 1.25, 1.299038105676658],\n",
              " [14, 99.0, 0.0],\n",
              " [15, 72.0, 2.1213203435596424]]"
            ]
          },
          "execution_count": 411,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNuHgt9LyYj9zSQ+FXeFUSK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}